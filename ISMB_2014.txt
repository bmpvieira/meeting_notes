ISMB 2014

7/12/14

RegGenSIG
(Thanks to @ramialison_lab, whose tweets have been incorporated into these notes)



Jason Ernst - Opening remarks



QUALITY CONTROL


Marco A. Mendoza-Parra - A quality controls system for enrichment-based NGS data sets

Works on retinoic acid.
Want to integrate multiple sources of info for a systems bio perspective. This talk is on how the integration is preformed.

Polyphemus - peak normalization for comparison

Now the actual QC bit. NAR paper 2013.

Same antibody, same number of reads, but different labs produce different peak intensities, background.

Why? There's technical variability. Ab batches can make a difference, so does depth (though my understanding is that those don't explain the above points).

Cited differences in peak callers as another concern.

Their approach is different - 
Remove a fraction of the reads, randomly, and use those to re-construct the profile. Ideally, this will reconstitute the peaks, but with a lower amplitude. Gives you an idea of the robustness.

Measure the dispersion from the ideal

They use quartiles to grade the enrichment. They aim for at least BBB score or better.

Available at http://www.ngs-qc.org/. Integrated into Galaxy.

Q: If you sample background, you'll get background peaks.
A: Input shows up a DDD.

Q: How does antibody validation work?
A. They just look to see if they're CHiP-seq grade via automated platform.



CHROMATIN/ENHANCERS


Xiaole Shirley Liu - Using ChIP-seq and DNase-seq to identify TF binding and their target genes

Started by mentioning various tools:
ChiLin is command-line tool for peak calling
CistromeMap: has someone done chip seq of factor X in cell Y?
CistrtomeRadar- who else's chip Seq looks like mine?
(Many other things listed, too)

Currently running everything on Grc38

Notch-r reg in T-LL: gamma-secretase inhibitor prevents notch from going in the nucleus. However, only 10% of the notch1 binding sites change upon treatment with GSI. Those sites tend to be be in enhancers, enriched for a paired motif.

Dynamic NOTCH1 binding are enriched in super-enhancers (h3k27-defined) [AKA stretch enhancers].

Summary I:
Static TF ChIP-seq might not identify correct target genes
TF and h3k27ac under perturbations might be better.

Chromatin accessible profiling - DNAse treatment + seq finds regulatory sequences.

DNAse-seq optimization - the short fragments tend to be the ones in the DHS peaks, so focusing on them is more cost-effective. Better signal, too.

TF binding from DNAse footprinting: Footprint occupancy score (FOS).
L and R flanks are 6-25 bp. Lower FOS score is better, predicts better TF binding.

DNAse doesn't cut straight through both strands - strand-specific cutting. 
Most likely vs. least likely cut can be 300-fold different based just on the 6mer sequences.
bp-resolution cleavage pattern hurts TF binding prediction when it's close to what DNAseI wold be cutting anyway.

Alternative to TF footprint - overall read counts + motif match. 

So - DNAse data valuable, but you have to be very careful for TF site prediction.




Guo-cheng yuan - Computational analysis of chromatin-state plasticity provides insights into cell identity

Old paper from another group - chromatin states at promoters tend to be invariant, those at enhancers variant.

Quantifying cross cell-type plasticity - compare lots of cells to quantify variability. But DON'T use the variance to do so, since it's not very informative. Instead, you need to do variance divided by the mean. That's the coefficient of variation, but they call it the plasticity score.

High plasticity regions (HPRs) are associated with regulatory regions.

Plasticity is related to DNA sequence.

Pinello PNAS 2014 - pipeline to idientify regulatory TFs.

Haystack almost available - plasticity analysis based on collections of ChIP BAMs

Integrating multiple marks: can use mutual information to block together multiple regions.

Take-home: have to look at multiple cell types



Christina Leslie - Enhancer poising and regulatory locus complexity shape transcriptional programs in hematopoietic differentiation

Enhancer poising

Models transcription programs governing cell state transition, specifically in hematopoietic differentiation.

The Foxp3 sites in T-reg cells were already accessible in normal CD4+ T-cells, with something else opening them up. Foxp3 was taking advantage of a pre-existing landscape.

Divided genes into "complexity classes" - high complexity tend to have longer transcription units, higher fraction intronic (which tend to have enhancers), greater dynamic range of expression, and larger fold changes in cell-state transition.

SeqGL - captures multiple seq signals from ChIP-seq, DNAse, ATAC-seq (Shirley had pointed out the lsat two have problems). A sparse group lasso model to extract K-mers.
Want to differentiate what's in the peak from what's next to it. Sets of co-occuring k-mers suggest a motif. Setty and Leslie, under review.
The point is that using this to compare peaks from different factors can be used to determine what peaks "explain" the peaks in a different cell type.

They initially assign to nearest gene, but then iteratively attempt to improve it by masking the nearby silent genes. Doing so improves performance.



Tarjei Mikkelsen - Massively parallel reporter assays

Put sequence tags into gene of interest, so you can then do this in multiplex and de-convolute relative (semi-quantitative) activity.
Useful for promoters, enhancers, silencers, etc.

Interferon-B example: each subunit has much more promiscuous activity than the complex as a whole, which is very specific to viral infection.

Saturation mutagenesis of the hIFNB enhancers. Turns out a single nucleotide in the WT enhancer explains the specificity, and it's not something you'd guess based on the structure.

Can also use this for detection of new regulatory elements.

MPRA can also be used for transcription factor recruitment, see which things actually interact.
In his example - PPARG - you have a better ability to discern the signal in MPRA vs. ChIP-seq.

(Also mentioned Eric Mendenhall, UAH, and his genomic integration assay)



MOTIFS


Gary Stormo - Improved computational and experimental methods for motif determination

HT-SELEX - like ChIP-seq but with a binding protein instead of an antibody.
Analyze with non-linear regression.

He says some of the published stuff is over-specified (Taipale lab), since they focused on the strongest peaks.
So Stormo wanted an alternative.

New approach starts with a PWM (published, MEME, whatever. It's mostly for speed)
Optimize parameters using Broyden–Fletcher–Goldfarb–Shanno algorithm.
Machine learning.

(Inverse of hessian matrix is the covairance matrix. Hessian matrix is second derivative of the objective function)

Experimental - rather than sequencing input and bound, if you sequence bound and unbound and compare to a reference, you get the energy directly. That's because you only care about relative affinities.

Specificity of the Lac repressor - involved randomizing various parts of the motif, both the sequence and the spacing. 
By doing so, they detect 5% variance in affinity, with 0.1 kT variance in energy - so it's very reproducible.

The high-affinity version is only SLIGHTLY better than the wild type - and the WT is best among those with 3 bp spacers. That matches up with with the structure - as WT, lac repressor acts as a heterodimer despite being a homodimer, binding asymmetrically. Visualized the motifs by plotting the energy with both positive and negative values.



Yaron Orenstein - The use of HT-SELEX to infer TF binding models: comparison to PBM and an improved algorithm

Models very accurate in predicting PBM (protein-binding microarray) binding, but worse in predicting in vivo binding.
So, were those models over-fit? Yeah. Enter Selex.

NAR 2014 Orenstein and Shier - PBM vs. HT-SELEX
Looked at correlation between the top 100 8-mers.

HT-SELEX derived models predict in vivo ChIP-seq biding better than PBM. Why? SELEX has longer motifs, and that flanking sequence gives it the edge.

There's a strong sequence bias, too, though, so watch out.

Systematic biases in HT-SELEX:
Sticky K-mers
The "false" oligos that don't specify a binding site can go as high as 32,000. He suggests removing them prior to the analysis.

HTS-IBIS - unsupervised algorithm to predict in vivo binding
* Choose first cycle for which KL-divergence between current and original cycle is > 0.1
* Correct k-mer count by removing those that differ greatly from their reverse complement

He notes supervised algorithms are better, of course.

HT-SELEX better for in vivo, PBM better for k-mer ranking.



Raluca Gordân - Improved TF-DNA binding models challenge current hypotheses about genomic recruitment of TFs

In E2F family, at least, PWM are insufficient to describe binding.

The consensus motif often isn't there where the actual binding occurs.

Are PWMs any better? Sort of. Higher AUC, but not much higher than random. ROC curves show that, at least in the case of HeLa S3 E2F1, you can't distinguish at low false-postive rates (only can do about 35%).
So cofactors must be involved. Work from other groups supports this.

So, they're not looking genome-wise. Genomic context PBMs (probes centered on predicted sites, which eliminates positional bias).

A lot of probes with high gcPBM binding signal do not contain matches to the PWM.

Regression models using 3-mers good at capturing specificity for these proteins (better than PWM at predicting E2F binding).

It turns out that cofactors may NOT be required, as the proteins bind quite well on their own. Poor PWM models just made it look like that wasn't the case.



Remo Rohs (@RemoRohs) - Quantitative modeling of transcription factor bidning specificities using DNA shape

TF binding using crystal structures

Method he developed: DNAshape (Zhou et al, NAR 2013). Assumes DNA shape at nt resolution is determined by identity of the base and the sequence context.

Mentions SELEX-seq that found DNA shape preferences of Hox TFs. The N-terminal tail is used to read the shape of the DNA.

Yang et al. NAR 2014 - TFBSshape, a motif database for DNA shape features, such as groove width.

GBshape - genome browser for DNA shapefeatures.  rohslab.cmb.usc.edu/GBshape. Not yet published. Also includes electrostatic potential, but I believe they still need to validate that bit.

Looking at DREAM5 dataset, DNA shape improves binding specificity predictions based on benchmark for 69 TFs. Also gcPBM and HT-SELEX.



Marc Santolini (@msantolini) - Deciphering gene regulatory networks using DNA sequence

Limitations of CRM prediction:
* Model accuracy
* Model exhaustivity (since it doesn't catch everything)

Studies three types of models: PWM, Pairwise interaction model (PIM), and mixture model.
PIM, which incorporates correlations between nucleotides, looks to come out on top.

Model exhaustivity - Imogene
Motif-based. Takes CRMs that you KNOW are there, rank motifs based on overrepresentation therein.

Imogene UI is mobyle.pasteur.fr (source is on github)



VARIATION


Stephem Montgomery - Transcriptome sequencing of large human family identifies the impact of rare non-coding variants.
(Missed the first part of the talk due to poster session)

Mentioned using the Platinum Genomes data (Illumina) to identify rare noncoding variant impact within a large family.
They did RNA-seq on all family members.

Since it's in a family, they do eQTLs through linkage, not through association.

More large effect family sQTLs than eQTLs

Are rare variants enriched near large effect family eQTLs? Yes. Enrichment increases with confidence that the effect is larger than in the population.

large effect family eQTL genes tend to be more conserved based on dN/dS. Rare variants influence more conserved genes.
Network approach: yup, large effect family eQTLs tend to be hub genes (degree > 10)

Rare splice variance enriched for large effect family splicing-QTL genes.

Large effect QTLS influenced GAW genes where expression is already likely to have a role

Turns out using the family is a good way to test annotation-guided genome interpretation methods. (e.g., predicting the effects of rare alleles). Rare variant effects are apparently easier to predict than those of common variants.

They filter out de novos, by the way. Only looking at mendelian variants.



Gerald Quon - Investigating the functional role of varience-eQTL in transcriptional variation

Decanalization: Stressful environment tends to increase phenotypic variance of a population.

So if you're looking at, say, increased BMI, you can either increase the mean of the distribution or you can increase the variance. They did the latter.

v-eQTLs show no enrichment for cell type specific enhancers, unlike mean eQTLs.

v-eQTLs are more constitutively expressed genes compared to e-QTLs.

vQTLs overlap QTTs. 
Different mechanism of action than mean-eQTLs. 
V-eQTLs associated with multiple genes, and their targets are involved in core processes
Still don't know mode of action or contribution to heritability, though.



Hunter Fraser - An atlas of human genomic imprinting reveals global patterns of epigenetic regulation

For imprinted genes, expression depends on which parent it came from.

Why? Most popular hypothesis is that it's conflict between the genomes of mother and father in non-monogamous mammals.

ASE - allele-specific expression. Use reciprocal cross (switch parents' gender) to make the evaluation as to if the parent that gave the allele makes the difference.

Imprinting scored using biological replicates as calibration

They now have an atlas that shows mouse genes that are paternally, maternally imprinted, or not imprinted at all.

2x more paternally imprinted than maternal.
Usually not tissue-specific (95% of mouse imprinted genes aren't).

Also working with humans, though obviously can't do reciprocal crosses. 

Imprinting in a pedigree: can look at allelic flipping, such as when it goes from grandfather -> mother -> son.

ASE validation: mmPCR-seq (Zhang et al. 2014)

Human insights:
* Most human imprinted genes probably already discovered
* 17 strong novel candidates
* Involved in growth, insulin response, glucagon signaling, feeding behavior.

Mat/Pat co-imprinting and accelerated expr divergence is consistent with the "arms race" model.



TFs/NETWORKS


Matthew Slattery - Context-specificity in Drosophila developmental gene regulatory networks

TF binding doesn't always match up with the in vitro motifs.

Scalloped as an example - TF with ubiquitous and specialized functions.
Its specialized function (in the wing disc) is more associated with the "canonical motif". But - was that really a tissue-specific enhancer?

HOT regions may have an elevated mutation rate - certainly they're centered on less-conserved sequence.

HOT binding is associated with enhancers in open DNA, while COLD binding is associated with "closed" (epigenetically-regulated) enhancers.

HOT regions are functionally conserved. More likely to drive 'ubiquitous' or 'broad' expression, while non-HOT are roughly context-specific, patterned enhancers.



Shaun Mahoney -

Context-dependent transcription factor activity

Venn diagram won't detect quantitative differences in peaks called in both conditions. He wants something more nuanced.

He focuses on detecting CONSISTENT binding events across experiments.

Mixture model of ChIP-seq binding events: regions compete with each other to take responsibility for the reads. Gives you better resolution than just looking at peaks.

MultiGPS - Mahoney et al., PLOS comp bio 2014

Introduced priors to encourage, but not force, binding even alignment across conditions.
Uses motifs as positional priors.

How does a TF FIND its binding site? 
Looking at Cdx2 as an example. Can lead to three different cell types.
Many binding sites are context-dependent, but not all.
Cdx2 acts as a pioneer at some sites. It looks like sequence information is what discriminates between sites at which it is a pioneer vs. those at which it is not.

The pioneer sites have the stronger motifs; the other sites are more opprotunistic.



Michael Brent, Wash U -
Mapping the transcriptional network of a specific biological process. Developed NetProphet to do so.

NetProphet ranks all TF -> target pairs by regression coefficient and log odds that the target is differently expressed when the TF is out of the picture.
This maps just functional, direct regulation.

Applied to C. neoformans. Studied capsule size, since enlargement is a critical part of maintaining the infection.

Incorporated idea of "phenologs" - genes that work together in one organism tend to work together in another as orthologs, even if the function they work for has diverged.

PhenoProphet did identify relevant TFs.

His overall point is that when you drive this sort of thing by biology, you get biological insights.



Mirana Ramialison (@ramialison_lab) - In silico prediction of mutant transcription factor function

Looking at cardiac TF targets, specifically those of NKX2-5.

DamID-chip technology: When your factor binds, you digest the nearby methylated sites to see where that binding took place. Control is the same thing, but without the factor, just the methylase.

Did that with WT and control, and three different mutants.

Increase in severity of mutation in the binding domain decreases the TF target overlap. (But doesn't remove ALL interactions, so there may be more domains there)

Then did motif discovery on the WT and various mutants. Most interesting one are the new motifs from when the homeodomain were removed, but they also needed to filter out targets of the known cofactors.



EVOLUTION


Katja Nowick - Evolution of a transcription co-expression network active in the primate prefrontal cortex

Brain size increased rapidly over the last 6 million years.

So what are the molecular changes underlying the change in cognition that's associated with that vs., say, chimpanzees? 
They looked at expression and TFs in prefrontal cortex.

"specific" genes in her talk means genes differentially expressed between a species the other two species (among human, chimp, and macaque), but not diff. expressed between those other two.

The most links have been gained on the lineage from H-C ancestor to human (92.7). Due largely to zinc fingers.

Function of the network: GO enrichment of nervous system development and synaptic transmission. This is all in Berto and Norwick, MBE, in revision.

If you look just at the species-specific expression changed networks, you see highest connectivity in human (also mitochondrial function and epigenetic regulation).

There's been rewiring of the TF network involved in brain development, impacting both human cognition and disorders.



Anna Lyubetskaya - Evolution of regulation in Actinomycetes: Benefits of ChIP-Seq for evolutionary studies

Lots of ChIP-seq on all TFs in tuberculosis genome.

Does sequence conservation actually differentiate experimental sites and computational motifs? Does conserved motif = conserved binding site?

Sequence conservation: Does NOT imply site conservation.

The TF-target regulation is more robust than the exact site itself.

Both genic and intergenic sites are conserved better than random motifs. Not just a consequence of ChIP-Seq process.

What processes drive regulon evolution in bacteria? 
Regulon represented as a sequence of states, and they estimate transition rates fro each branch of the tree (of which there are three).
TFBS loss is strongly associated with gene loss.



DISEASE


Carl Herrmann - Transcriptional (dis)regulation in cancer

Opened by distinguishing between "genetic" contribution (binding of TFs, chrom binding proteins), and "epigenetic" contribution (DNA methylation, histone mods).
Both can go wrong in cancer.

First, geneic. Looking at malignant B-cell lymphoma and medulloblastoma WGS.

Non-coding mutations are not randomly distributed in these datasets. This is confirmed by FANTOM data.
Can therefore look for evidence of selective pressure.

Used a background model of randomization of local heterogeneities, which lets you look for positive & negative pressure.

K-mer creation/disruption patterns: those that are created or disrupted in lymphoma are "oncomers", since they might be relevant for malignancies. For example, hi creation, low disruption is oncogene TFBS, while low creation, high disruption would be tumor suppressor TFBS.
High creation AND disruption can find motifs for protein families with dual roles.

Non-coding SNVs a sign for extensive rewiring of reg. network.


Epigenetic - neuroblastoma dataset.

Highly variable CpGs with intermediate methylation are enriched for enhancer elements. 

By finding distal CpGs, can find positive and correlated CpG gene pairs (by correlating those with nearby genes within a certain window)

The distal CpGs can explain changes in gene expression - a sign of possible enhancer methylation. 
There's a Positive/neg correlation depending on the chromatin context.



Hee-Woong Lim - Genome-wide analysis of enhancer-RNA transcription reveals regulatory mechanisms by and antidiabetic drug in adipocyte.

Looking at anti-diabetic drug, rosi

GRO-seq - Nascent RNA sequencing. Why? Steady-state sequencing isn't a direct measure of transcription rate, since it's influenced by degredation.

Gene transcriptional regulation is accompanied by eRNA regulation

Squelching proposed as a regulatory mechanism as rosi.



7/13/14


KN1 - Michal Linial - Good Things Come in Small Packages - Replicators and Innovators

Where are we going? Main theme is tracing the footsteps of evolution. Dobzhansky quote at the bottom of slide.
Particulars:
* creating MAP for proteins
* Challenge the MAP
* Devleop NAVIGATION
* Hidden functions
* Created functions
* The lesson

Or: A treasure hunt for hidden functions.
Some guidelines:
* Listen carefully to the "big data". Be very careful w/ regard to outliers, exceptions, etc.
* But also listen carefully to the biology, since in that context, the "exceptions" may be the interesting part, rather than something misleading.

Example of a lot of data: LOTS in UniProt, but we only understand a very small fraction of them.

Often forgotten/swept under the carpet:
Gene/DNA is essentially written in stone.
Sequence/transcript is static, but you may have a handful of them.
Protein variants are dynamic, and you could have 25-100.
For protein function…context dependent, and who KNOWS how many of these ther are to find. Defining protein function is hard to do.

Protein space is tough b/c it's high-dimensional, metrics poorly defined, a lot of "dark matter".

The ideal is automatic assignment of sequences -> function.

ProtoNet:
All seq in UniProtKB included, do all-against-all blast, look at things even with a very weak E value of 100 (so dealing with even very remote distances).

Bottom up: Agglomerative clustering
Use the above for clustering.
The more the merrier - adding in MORE proteins (even the weak distances) makes signals clearer. Increasing the sample size actually increased the signal per noise in this case.

Merging rules make a difference. 
Changes the number of singletons you have. You have to pick your clustering method based on the biology.

Correspondence score for similarity

Tree built, tested with respect to external families (various databases)
Evo relatedness is captured

A problem: false transitivity. A is similar to B, B similar to C, but A isn't necessarily similar to C (since you may be dealing with different domains). A and C definitely aren't homologous.

So, how to deal with that?
In the next merge, you use average clustering rather than direct clustering (meaning you take all the out-facing links from all the elements in the current clusters into account)

MC-UPGMA outperforms other clustering methods for both families and domains.

Globin example:
The evolutionary duplications are visible on their tree - which is impressive, since they're VERY different sequence-wise (E score beyond 100)

Another use of this protein map: looking for hidden connections.
Can look at upstream nodes to look for functional connection BETWEEN families (looks like looking at most recent common ancestor of the families)

Visualizing the data lets the data tell you what to do. It showed them to focus on a specific subset of the data with respect to Pfam clans.

Small packages: They found a LOT (like, upper 50% of A-B pairs) of similarity between viral proteins - despite having essentially evolved in isolation.

We're getting a LOT of insect genomes. Why? They're extremely diverse.
Can use those for gain and loss of families. ProtoBug, poster C04.

Switching gears to look at things with NO similarity: have to rely on the biology, here.
Hypothesis: maybe short peptides are an unexplored regulatory mode. 

Turns out the longer the protein, the more likely it is that we understand it.
Except toxins - those are short, but we know a lot about those.

ICI (ion channel inhibitor) - have cases in which many folds hit one target, AND in which one fold hits many targets. So there's no simple relation between the folds and the targets, can't really use that for inferring function.

So what's common among these? 
Compact elements, with structure shaped by cysteine bridges that hold them together.

For studying this, design features that capture your intuition.

TOLIPs at the origin of Metazoa -
Cnidaria are "a factory of TOLIPs" (toxin-like proteins).

Can we find TOLIPs of new function in the human brain?
ANLP3 modulates nicotine ACh receptors.

Evolution duplicates and modifies: what starts as just a toxin becomes a clan with cell signaling receptors and others.

Real toxins have irreversible action. The hit channel can't recover. But the non-toxins TOLIPs ARE reversible - why whiskey doesn't kill you.

Toxin-like sequences have therapeutic applications as pain relievers, etc.



LBR01 - Steven E. Brenner - Transcritome targets of nonsense-mediated mRNA decay offer clues to RNA surveillance rules in human, fish, and fly

NMD originally though to be an RNA surveillance system to protect against dominant negative mutations (truncated proteins blocking out full proteins, etc.)

Sox10 used as an example - mutations near N-term are bad, but individuals live to adulthood. By C-term, you die early, in childhood (except by the VERY end of the protein, which isn't as bad). It's because NMD clears out the mutations near the start.

The 50-nucleotide rule: model for NMD in mammals (Nagy and Maquat, 1998). Used to be the predominant model, but…

Hogg and Goff 2010 - length of the 3' UTR may make a difference. A long 3' UTR may define a premature termination codon.

Premature termination codons appear to be defined differently across species - what counts as a "long" UTR differs.

Alternative splicing can ALOS introduce PTCs.

Did RNA-seq to identify NMD targets genome-wide. used HeLa cells, some of which had NMD machinery knocked down (UPF1 knockdown). Looked via Cufflinks and JuncBASE to see novel isoforms that were degraded but are now measurable.

HeLa has about 11,000 genes expressed. 3,932 had early stop codons according to the 50 nt rule in at least one isoform.

2,116 genes with at least one isoform that is expressed robustly (1.5x increased) AND is being degraded by NMD.

In fish" 3,945 genes of 9,492 genes had at least one transcript with >1 FPKM. 416 of those are NMD targets.

50 nt rule IS a strong predictor of NMD in human. If you go more than 50 nt, suddenly you have much higher difference in expression. 
Fish and fly are similar, but less data, so not as strong. More subtle effect, too. Fly and fish have much greater scatter beyond that 50 nt boundary, while it's sharp in humans.

3' UTR length has little effect on NMD degradation. You have to remove the introns in the UTR to see that, though, since introns would cause the 50 nt rule to kick in again.
Even less signal in fly and fish. Human is signif but modest, but fly and fish aren't even signif.

So why did they think it was UTR-mediated in fly before this? They were using microarray, which had a much better means of capturing those short UTRs.

So why make these proteins if you're just going to destroy them?
Because NMD + splicing can be used for gene regulation, such as autoregulation. Srsf2 is one such splicing factor that does this.

NMD is important in the dense regulatory network of splicing factors

10-30% of alt spliced genes produce transcripts affected by UPF1 in fly and zebrafish.



LBR03 - Daniel Himmelstein - Heterogeneous Network Link Prediction Prioritizes Disease-Associated Genes

Prioritization increases GWAS power by increasing prior probability.

Others have used homogenous networks (one kind of node, one kind of edge).
So he's doing heterogeneous networks (many nodes and edge types).

Network includes genes, protein-protein interactions, disease, pathophysiology, tissues, genomic positions, transcription signatures of perturbations, pathways, microRNA targets, TFBS, cancer neighborhoods, GO terms, oncogenic and immunologic signatures.

Looking at the paths you can take from a gene to get to a disease. Each one gets a metric that measures it's prevalence of that path. Metric is mainly degree-weighted path count (since high degree-nodes are less specific, and thus less informative)

Machine learning methodology - observations were gene-disease pairs for 29 diseases with 10+ associations, and a bunch of other stuff. 70% training set.
Used ridge and lasso for regularization. 

83% chance that a positive was ranked higher than a negative. ROC looks pretty okay to me.

Perturbation signatures capture most of the useful gene set information.

Degree-preserving permutation test: AUROC didn't decrease much, which was weird. Node degree accounts for the majority of performance for many network-based approaches. However, edge specificity is important for the top predictors.

Predicting a future GWAS: When looking at given region, this method might help pick the predicted causal gene.

het.io - decomposes predictions into components.



TT02 - Enoch S. Huang - Computational biology careers at Pfizer R&D

AKA Computational Biology at Pfizer: Overview and examples
Simon Xi, Daniel Ziemek, Eric Fauman & Cristoph Brockel

They'll have a booth tomorrow all day long.

Brockel - overview
Precision medicine is about lower attrition through better decision. Picking better targets, essentially.
Also want to select the right patients, since drugs tend to not work for everyone. 


Simon Xi - Alternative Splicing: New Opportunities for Old Genes

He's the Neuroscience bioinformatics lead (been at Pfizer 13 years, 6 years since the beginning of the group)

Since they're interested in working on a given PROTEIN, it's important to know which splice variant of a given gene is the one you want.
RNA-seq helps with that. A lot of reads cross splice junctions helps determine representation of different isoforms in a given pool.

Looking at GTEx dataset: >185 donors, 46 tissues including 13 brain regions. 
Looked at all junctions to quantify alt splicing.

Alt splicing is universal in all tissues. 

MAG Lipase example: if exon 6 is skipped, the tether is gone, so it disperses into the cytosol. 

So, the appeal of this in the Pfizer setting is that they can tell you what version is going to be in what tissue, and what's the dominant isoform.

Splicing patterns also help interpret GWAS findings. For them, they need to know what's the target, do they up- or down-regulate, etc., before they can start a program. 

CD33 example - likely that rs12459419 is the variant affecting splicing. Suggests LOF of CD33 receptor could be protective for Alzheimer's disease. 
They've now found about 100 splicing QTLs in GWAS catalog for a variety of diseases and traits.


Daniel Ziemek - Interpreting Genetic and Transcriptomic data using the causal reasoning engine

Started from a CS background

Can prior knowledge help with transcriptomic and genomic data?
Many omics studies are under-powered for small effects.

Basics of CRE: mechanistic explanations are causal, which implies direction. Delineation of pathways are often fuzzy.
So, can you construct a large-scale network of casual "increase/decrease" relationships? They use curated data to do so. 

Substantial causal knowledgebase based on Ingenuity pathways analysis, Thompson Reuters and Selventa data.

CRE: Input is set of up-and down regulated transcript, and output is set of potential upstream regulators that caused those changes.

Pfizer does encourage external publication, too.

CRE analysis of Pancreatic induction stage (8- to 11-day transition):
Standard pathway analysis helped to validate experimental protocol. Want to identify molecular drivers of differentiation. 
So your diff. expr. genes get reduced to a smaller list of potential upstream regulators. 

Pain sensitivity in the normal population: an exome sequencing study. Looked at how long subjects could keep their finger in hot water, wanted to see if there was a genetic cause for those who did for long/short time.
There's some evidence for Angiotensin 2, which is genome-wide significant.

Bayes CRE: a non-frequentist approach. Tries to model biological context. 
A building block for a biomarker detection method (PP76 on Tuesday)


Eric Fauman - How genotype influences phenotype: a study of human metabolic variation
(Has a biology background)

Measured 500 metabolites in plasma from 8,000 subjects, and then associated those with 2 million SNPs. That's a billion statistical tests.
Can look at which genes contribute to individual variation in metabolism, affect levels of metabolites

An atlas of genetic influences on human blood metabolites (nature genetics)

In the case of the FADS1 locus, the genetic variant impacts the transcription of the gene.

Sometimes these things aren't direct - for PHGDC, the genetic variant affects a key intermediate in the metabolic pathway (but you're kind of three steps removed)

Interesting example of CCBL1 - KEGG missed an annotation of what it was doing within a metabolic pathway.



KN2 - Eugene (Gene) Meyers (@TheGeneMyers)- DNA Assembly: Past, Present, and Future

Early career - '78-'98. Algorithm guy.

"What's behind blast" - paper on the theoretical underpinnings of BLAST. If you google the term with Yandex, you'll get a video of him giving a talk on it.

'03-now: Current career, imaging, etc.

Mid-career: '84-'05. PE whole-genome shotgun seq & assembly

Points out the first shotgun project was by Fred Sanger, back in the early 80s. Did the lambda phage (48 kb). Restriction enzymes -> DNAse I -> Sonication

Roger Staden jumped off from that with the first assembly algorithm. It was greedy, not dynamic programming, but it was used to find the overlaps.

'85-'90 - Esko Ukkonen et al. put out SEQAID in '84. First overlap-layout-consensus (OLC) assembler
SCS was 89-90, saw that problem was analogous to shortest-common-superstring problem (NP-hard)
91 - OLC + B&B optimal

The bi-directional overlap graph - if you ignore all reads contained in others, you're left with reads that properly overlap. Treat each 3' end as nodes, and the edges point away from those nodes.
A path is that in which in-degree equals out-degree, which equals 1. Sounds like no bubbles to me.

Genesis of string grpahs: '94.
Waterman & Idury were the ones who first came up with DeBruin graph assembly.
At the same time, Myers pointed out that shortest-common-superstring isn't the right way to do it, as you compress the reads too much. Brought up a de bruijn graph approach there, too.

He calls them epsilon-string graphs. You want every read in your set to match some arc of that graph (the epsilon takes care of error).

You can use a graph like that to infer copy number by looking at in- and out-node.

A debruijn graph is a 0-string graph (no errors)
Pro is can build in linear time
Con is that, if you DO have errors, you start getting bulges all over the place. The way that used to be dealt with is removing all the kmers that didn't show up very much.
If the error rate is high (PacBio), this approach won't work at all.

Unitig graph - can perform transitive edge removal on a chordal graph to get to this.
Pro: mostly resolved at the level of reads
Con: repeats in the sequence makes it take quadratic time to build

Shotgun era ('95-'02) -
Statistics of coverage, inspired by Fred Blattner '95: Showing that a bac at 10x isn't that much different from a bac-sized region of the human genome when the whole genome is at 10x.

Weber and Myers 96 - IF you can detect the unique regions at 97% id, then you can assemble the genome with mate pairs (used an information theoretic approach)

PE WGS protocol at Celera meant they only needed a factory for shotgun sequencing, rather than shotgun AND physical mapping. 

98 was the advent of capillary gel sequencers. Big gains in throughput.
At the same time, got 64-bit architecture.

WGA in a nutshell - look at the graphs to get all the unique regions, with very low error rate. Then link those together by using the mate pairs that link them. That's how you get the scaffolds. Then can take the reads with one end in unique and one end in the repetitive region, and can then tell how far into that repetitive region the other end goes.

After "the" genome: consequences.
Cost became the priority, driving us to short reads, which hampers de novo genome assembly. He REALLY doesn't like assembly using short reads.

Points out that, while the melanogaster genome is good, some of the others have much too low an N50 to understand evolution. Can compare genes, sure, but not structural variation.

So he got excited about PacBio. High noise/error, but it's RANDOM, and read sampling of the genome is also NEARLY random. Very close to Poisson.

The perfect assembly possible iff (and he tweeted this Feb 22, 2014)
1. sampling is poisson
2. errors random
3.reads long enough to solve repeats.
Error rate itself isn't actually needed.

He's started a blog called the Dazzler Blog. I believe it's his work on PacBio.

The problem with string graphs:
His assumptions worked on sim data, but not the real ones. His assumptions:
1. Reads are all contiguous stretch of the genome
2. Some maximum error rate

But in reality:
1. Chimeric reads
2. contaminant reads
3. unclipped primer sequence
excessively (…something. He switched slides.)

So data scrubbing is important.

In conclusion, high error rate is only a problem with respect to compute efficiency and coverage, not quality.

During questions:
With advent of CRISPR, you should be able to do biology in whatever model organism you want, but you'll need a genome.



WK02 - Bioinformatics Core Facilities

http://bioinfo-core.org/

Core on a Budget vs. Enterprise-Level Bioinformatics

First: Alastair Kerr (@alastair_kerr)

Teaching is key - gets rid of trivial requests

Very small core, just him and Shaun Webb, with some help from advanced users.

So they do a lot of teaching, collaborations, and infrastructure (manage their own servers). Biggest part of infrastructure is software, but hardware and data management play a role, too.

Open source software & data are a big part of how they work. Many mature projects, well supported.
Also, have had problems with commercial software - some were dropped, some started free and went commercial.

The key thing about open source software, for them, is reproducible research. Slide shows GitHub, Galaxy tool shed, ENA, GEO, etc. 
Also makes reference to dodgy perl scripts, but at least they're reproducible dodgy perl scripts.

Data sharing and visualization -
R Shiny
Shiny Tables - no need for excel!
Shiny Graph - data exploration

Hardware:
500 TB disk split over backup and primary sotrage. Uses ZFS on newer severs.
COmpute is 3 key servers
64 GB RAM, 24 cores
256GB RAM, 64 cores
(and one more, but the slide changed)


Michael Poidinger - He's at SIgN, Singapore. Focus on human immunology.

He has 5 postdocs, soon to be 9. 2 have pharma experience.
Uses commercial-grade hardware: Multi-node 64 big clusters, many terabytes of storage, and of course money for cloud space.

The staff works closely with 25 PIs and 180 researchers in the institute.
Every analysis is special - biologists ask them what they can do with the data; projects often extended due to findings.

He puts a big emphasis on the BIO of bioinformatics - they're here to do the science, not algorithm design, etc.

Baby food maker is a collaborator - really cares about difference in microbiome in breast-fed vs. bottle-fed babies, and what they can do to improve their formula.

Many projects:
clinical survey
clinical tests (like allergy skin prick)
cell based assays
HT data
…and more

In THEIR environment, they don't push the analysis back toward the biologist - they have enough money that they don't HAVE to.

(Complicated slide of "their solution" involving Spotfire)

Software/tools -
Commercial licenses for various stuff

Reporting via Moin Moin Wiki (which lets you specify who has write access)

Uses pipeline pilot, too - sort of seems like the pipelining in galaxy, but I'm sure it has things that are more fancy. It's certainly very human-readable.
Can also pipe perl into python into R, etc.

Spotfire used for RNAseq visualization. Like an interactive cummerbund, without the messiness of setting cummerbund up.

Makes the point that commercial software is much less expensive than the machines that it's designed to analyze, so it's certainly worth it if it makes things run on time.



Budget vs. Enterprise Open session

Q: Which commercial bioinformatics packages have no good open source equivalent? Why? Are they worth the money?

MP - This is an open version of Spotfire, though with fewer features. Spotfire does have it's own web publishing element, for one thing.

AK - R shiny and all that don't cover everything Spotfire does, and takes a little more time to set up, but does sound like it has a useful GUI component


Q: For new cores - what do you suggest? Esp. for low budget.

AK - have an idea of the kind of data you'll be working with. You COULD go straight to AWS, but depending on how much you're dealing with, that might not be financially viable. 

MP - first thing he did in Singapore is to carve out his niche and make it clear what resources he'd need to run a competent core.


Q. Stephen Turner (@genetics_blog) - Suggests putting the above answers on the wiki page. Says his shop is very similar's to AK, under cost-recovery restraint. A lot of his people want the gene list, sure, but then they want pathway analysis. Ingenuity PA is expensive to get others to use - is there an alternative? He's a biologist, but will never be as good as the expert asking the question of him.

AK - Talk to the scientist about what they ACTUALLY want to get out of this - they might think they want the pathway, but maybe it's not really what they're going for.

ST - Yeah, people gravitate to the stuff they already know

Audience member 1 - There's DAVID for enrichment analysis, Gene Mania to find papers with similar gene lists

MP - Remember DAVID hasn't been updated since 2010

Audience member 2 - Maybe have the library buy the Ingenuity license (guy from Tufts agrees)


Q. Audience member 3 - does commercial software make you less agile?

MP - They use freeware, too. The real goal isn't to buy everything, it's to build something that'll still work six months down the road. 
CuffDiff gives you FIFTEEN different outputs. He wrote something to whittle it down, and they haven't looked at the full output since.
His software improves the efficiency of his staff 20-30%.
Why getting 4 new postdocs? 4 grants were funded that all said they needed a bioinformatician, but those PIs were smart enough to know it'd be better to not have them sit by themselves, so they're moved into the core.

Audience member 4 - It makes sense for them to buy commercial software for their core.



Open Session

Pipelines

* Commercial - accelrys, knime
* Open Source - galaxy, taverna
* Home grown - cluster flow, gnu

Who does what? How did you choose?

Audience member 5 - they did build one, since they couldn't find one that supported the systems that they were interested in. Has to do with the very specific requirements of your own local computing infrastructure.

Audience member 6 - Needed to track parameters, needed logs for reports, consistency across analysis, needs to be easy to use/learn, operate across different compute environments, and robust/adaptable to changing of component software.
So, they use the lowest common denominator: shell and perl.

Simon Andrews: Sure - but that doesn't scale very well.

Audience member 2 - her core is for a clinic, so the MDs want essentially a web interface. She want to python - now it's not a pipeline, it's a SYSTEM of pipelines (some of which are hard-coded, because the MDs like them that way). Open source is great, but you need to be really careful about design. Even though she built it modularly, it's now at the point that you can't change one thing without changing the whole thing. One of the pressures on her is the combination of accuracy and speed required by the clinic for, say, in vitro fertilization.


Managing workloads -

Slide on the various things you always need (run analysis, manage compute infrastructure, manage data pipelines/LIMs), jobs you need, but not all the time (training, papers, data submission), and longer term goals (develop training, develop tools, testing new tools)

How do you balance day-to-day vs. longer term planning?

Moderator says they spent about 10% of their time making fastQC.

How do you guide users with unrealistic or unreasonable requests? (e.g., "can you make all the same figures as this paper", "can you download and reprocess these ENCODE samples", "I want all my work done by person X", repeated speculative analyses, "Quick" jobs)

Get your management to back you up in saying "no"

Ask them what they're going to DO with the analysis. If they can back it up, okay - but don't let them "walk through the park".

Train your users. The more they understand, the more they at least get what they can and can't ask.

Maybe have a ticketing system - it's split about half and half. Looks like the bigger cores are the ones that have formal ticketing.

Charge for the time it takes to make the figures, and give them an estimate. They've adopted that one part of Agile, it works really well.


Funding your core -

Slide - time spent in his core is mostly advising consultancy and analysis.

Cost recovery is used by a lot of cores. Can be based on analysis, fraction of staff salary, etc.

But there are things done that aren't charged for, and why?

Training (since they want people to come to said training); initial design work

MP - you CAN charge for training, and people will come in droves. They're thirsty.
Also…there's a perception that if the training is free, then it can't be good. So even a small fee can incentivize people.

What about the time spent reading the paper that someone wants you to reproduce the figure from?

Sounds like people tend to eat that time right now, and don't charge for it. Again, using what you read to figure out how hard the task will be, and using that to provide an estimate to the client, can help.



7/14/14


KN3 - Isaac (Zak) Kohane - Biomedical Quants of the World Unite! We only have our disease burden to lose

Says his job in the next 50 minutes is to get off your butt to get useful things done for society

There's no national health identifier, there's no national payer system (here), electronic health records only really got adopted well within the last 7 years, but it's out-of-date.
New alternative models of healthcare industry; threat/promise of accountable care (paying doctors only for the result)

So doctors are seeking new finding models for research. Example: Cf.Regeneron/Geisinger.

Afferent and efferent arms of healthcare information: afferent is where we get data from patients, etc., and the efferent is where we actually use that to do things.

First up, the afferent axis. 
There's a lot more than just the healthcare data (medications, radiology, examinations, etc.). There's also the social web, genomics, environmental modeling, etc. He says they're all relevant to understanding current/future state of patient. They've published this in JAMA.

I2B2 - free, but requires a big investment for data extraction, but still well-adopted.

Discarded samples could be used inexpensively to get information

The closer two hospitals are to each other, the more they hate each other. Still, he made a distributed query system so that investigators could take advantage of data from the WHOLE system. Makes it easier (possible) to find rare patients.

Brownstein, PLoS ONE, 2007 - Rofecoxib caused a huge - 18% - increase in heart attacks, but you wouldn't really know that without having done the analysis.

If you're healthy at age 80, you're EXTREMELY abnormal. Tells you something about the controls you'd want to use.

Naive Bayes predictor developed that detects incidence of domestic abuse based on diagnosis at various visits

There's about 5,000 various co-morbidities of autism.
Used a 5,000-element bit vector in 6 month chunks, and then clustered those.
Able to find things like sub-divisions within autism - some with higher incidence of seizures, some with psychiatric disorders, some with what looks like innate immune diseases. His point: You'd want to study those three groups separately.

Healthmap - map of all infectious disease in all locations at all times. Mines "a variety of data sources" to do so. Looks like a lot of text mining.

Environment-wide association study - Chirag Patel. Mass spec of various patients from different , looking for heavy metals, etc. Accounts for effects genomics can't.


Efferent arm - making things happen.

While trying to make clinical bioinformatics happen, argument over standards arose. So they made a challenge (CLARITY challenge). So instead, they gave them the data of a really sick kid that was going to kill them, and teams came to work together to get the diagnosis.

Arithmetic: If you have a disease with 1:1000 prevalence and a 5% false positive rate (but with a test that misses nothing), what's the probability that someone diagnosed with the disease actually has it? 2%. There's one person with it, and 50 false positives, so 1/51 is ABOUT 2%.

Incidentalome - if you have a large N, even with a really high specificity, you'd still get like 60% false positive rate across the US population. Need to know the positive predictive value.
Gave various examples where such false positives had disastrous results (BRCA1 leading to unnecessary mastectomy/hysterectomy; PSA; heart disease in African Americans)

EHR - why can't electronic health records be more like the iPhone? Some EHR companies are coming together to use their software. 

Questions he poses:
* Who, if not the healthcare system, is actually going to do this?
* Are we willing to slow down in the interest if worrying about patient safety?
* How will we fix the effector arm (with a very regressive, entrenched healthcare establishment)?
* What's the quant alternative? Disrupting from the outside?
* Should this not be an ISMB priority?



PP23 - Zhaojun Zhang - RNA-Skim: A Rapid Method For RNA-Seq Quantification

Related work -
Alignment-dependent methods: reads aligned back to original transcripts (Cufflinks, RSEM, eXpress, etc.). Takes hours to finish (millions of reads, so huge search space). Estimate the abundance of transcripts.

Second group is alignment-free. So far, that's only Sailfish.
Uses k-mers instead of alignments to quantify. Does not depend on an alignment step, so it's 10x faster, but still supplies comparable results.

How can RNA-Skim be faster? Showing a graph of number of reads compared to location. Can we aproximate the area under this curve? That'll give you abundance.
Considers abundance levels of transcripts to be random variables that follow the same distribution. If variance introduced is an order of magnitude or smaller than transcript variance, he says it should still work.

Sig-mers: Given a partition of the transcription, a sig-mer is a k-mer that occurs in only one cluster, and therefore uniquely identifies the cluster in which it is contained

So you sample the sigmers instead of the full reads.

Probability of sequencing a sig-mer from a transcript: want to maximize the likelihood of observed sig-mers.
(align-dependent maximizes the likelihood of observed alignments, and sailfish maximizes k-mers)
So it's the same underlying idea.

Optimization I: Finding all sig-mers.
Straightforward way is to store all k-mers in memory, which would take 50G. Instead, they use bloom filters to reduce the memory usage down to 100M. Probabilistic data structure, so you may miss some sig-mers.

Detecting sig-mers in RNA-seq data:
Multiple pattern search problem. Use Robin-Karp (rolling hash) algorithm to do so in O(N) time. Means the algorithm is invariant to the size of sig-mers.
It reuses the overlapped values when looking at k-mers, which is how it gets down to constant time. Seems like a sliding window to me.

Quantify the reads through some EM approach.

Results on simulated data:
RNA-skim has the best Pearson correlation, but the other metrics (rank correlation, signif false positive rate, signif false neg rate) are all better in other methods. No single method is best in more than one category, though.

Testing on real data:
Used 56 inbred or F1 mice. Microarray data is NOT ground truth, so can't determine false positive or false neg. All five methods are pretty similar, though RNA-skim isn't better than the other methods.

It is, however, faster. About 10x faster than Sailfish.

Conclusion:
Redundancy in RNA-Seq should be considered in statistical modeling and algorithm design for RNA-Seq

csbio.unc.edu/rs. Five small and independent componenet



PP28 - Pankaj Agarwal - Computational Biology in Medicine: Novel Targets and Drug Repositioning Use Cases

Nature Reviews Drug Discovery, 2013 Aug; 12(8):575-6 - http://www.ncbi.nlm.nih.gov/pubmed/23903214

Cancer drug targets: the march of the lemmings - are drug companies just all trying to hit the same targets?

Active projects from Informa Pipeline/Pharmaprojects
247 proven targets
712 novel targets (no approved selective drug)
These were the ones that they looked at.

42% of targets had only one target looking at it (out of 1,027 targets). 26% are being looked at by FIVE OR MORE companies.
The proven targets are 64% looked at by 5+ companies (since everyone knows it works, but could be improved)
For novel targets, that value is only 13%. 54% of novel targets are only being looked at by a single entity.

More validation leads to more competition. As you progress through the phases of clinical trials, the percentage of targets looked at by a single entity shrinks, and the 5+ value grows.

Centre for therapeutic target validation - Ewan Birney is in charge of it

The kind of validation he (Pankaj) is interested in is drug repositioning.
Given a compound/drug, what new disease applications is it relevant for?

The data that goes in: Molecular assays, phenotype assays, in vivo models, clinical trials, prescriptions. Hurle and Ararwal, Clin Pharmacol Ther 2013. - http://www.ncbi.nlm.nih.gov/pubmed/23443757

First method: connectivity map. Lets you look at a drug and visualize a whole bunch of phenotypes associated with it, and compare to the phenotypes associated with the disease (expected changes). Lamb, Golub 2006, Science. http://www.ncbi.nlm.nih.gov/pubmed/17008526
Cheng, Agrawal PSB 2013. - http://www.ncbi.nlm.nih.gov/pubmed/23424107

Want to build a standard dataset.
Example: Does Cmap work for repurposing?
AUROC isn't that useful - want high precision, not a million things to go test.

GWAS - can insert SNP data into the connectivity map, via ENCODE, FANTOM5, nearest gene, eQTL.
Looked at overlap of GWAS and known drug targets. Sanseau, Agarwal. Nat. Biotech 2012.

Combining screening data - if you have a target for on disease, and it turns out it affects a phenotype for another disease, maybe the clinical compound that binds to the target, designed for the first disease, could be relevant for the second.

Clinical asset repurposing triage - use experimental validation to review and refine evidence you're culling from the data.

Progressing the hypothesis - likeliest decision is to STOP. Need to know if the drug itself is progressable, and incorporate learnings into priors (patent position, target, disease, drug history). Constrained by time, too.

They welcome collaborations, interns, new colleagues. Contact is Parkaj.Agarwal@gsk.com.



PP31 - Michael K. K. Leung - Deep Learning of the Tissue-Regulated Splicing Code

Cassette splicing - alt splicing in which a given exon is included (or not)
PSI - percent of transcripts with alternative exon spliced in.
Goal is to predict alt splicing patterns.

Neural networks - 
feed-forward: inputs go to outputs, passing through hidden nodes. You have a cost function

For a given hidden state, you sum the inputs and pass them through a non-linear function.

Deep learning an neural networks - exploiting many lays of those hidden, non-linear layer. 

GPUs important to machine learning.
Neural networks have been successful in various domains (computer vision, audio processing)

But is it applicable to the relatively small datasets used in biology?


Alternative splicing dataset - RNA-Seq data from mouse in five tissues, 11,019 cassette exons studies. Want to look at PSI and change in PSI between tissues.

DNN for splicing prediction predicts two things: low, med, or high PSI, and tissue difference.

Training:
Stochastic gradient descent with momentum, balanced mini-batches, dropout regularization
Hyperparameter search, GPU acceleration (15x speedup to actually accomplish said search)
5-fold cross-validation. 3 for training, 1 for validation, 1 held out for testing.

Interpreting the DNN -
Can use class saliency to do so. Forward propagate your data to get the output, then perturb the data and back propagate.

Large model with sufficient capacity.

Splicing code with improved predictive performance, and yes, deep architectures can help with small datasets.



TT14 - Raymond Tecotzky (Dir. of Illumina informatics marketing) - Sequencing and Genomic Analysis Onsite and in the Cloud

Focus areas:
Cancer
Enterprise informatics (where he's from)
Life scinces
New oproutities
Reproductive and genomic health

Tip of the iceberg - challenges & opportunity for discovery. The large amount of data, basically.

Last year, Google formed a company called Calico - entering life science realm.

A LARGE percentage of the money spent is spent on bioinformatics.

As throughput scales up, so does compression. Remember when we used to save the IMAGES from the illumina machines? Now we save BAMs, but soon we'll likely only save the VCFs.

BaseSpace - cloud-based genomic sequencing.
Wants to be an "app" based interface, and anyone can develop for them.

"cloud in a box" - basespace onsite. Which…obviously isn't a cloud anymore at that point.

Role of basespace for bioinformaticians - sounds like he's pushing the fact you can push the analysis to the biologists by making the analysis software user-friendly. Says it'll remove our role in "mundane analysis".

45,000 apps launched (with 8,000 logins every week)

FDA uses uses MiSeq and BaseSpace to stream data and share it to cloud, which is sent to NCBI. Sounds like it isn't being analyzed in the cloud, though.

Or, wait - now he says there are over 30 apps in basespace. 

Sharing and collaboration is the most popular app, of course.

How is data stored an analyzed in basespace?
.bcl, the raw reads, are sent to basespace. Then demultiplexed to fastq, then stored as projects. The apps operate on the projects.

Native apps - execution-friendly environment.

e-commerce infrastructure supported, handled by illumina, in case you want to make a pay app. Uses "iCredits", which is basically US dollars.

Can publish software on basespace, so any researcher is one click away from being able to run the program there.

To learn more as a developer, see developer.basespace.com

Questions:

Q: Are there plans to allow uploading of different programs, genomes?
A. Yeah, they're working on it. Right now, basespace only takes date from the instrument.

Q. What kind of motivation is there for someone to develop apps for basespace?
A. Some apps are private, so you can just share it with your colleagues if you want. 

Q. As the API evolves, what happens to apps that break?
A. They notify the community what the changes will be implemented, so developers know what to change. In the future, the native app environment will help with that, as it doesn't rely on a single API.

You can also include the reference data in the app itself (in response to another question), as a workaround to get the data up there.


