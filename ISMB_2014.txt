ISMB 2014

7/12/14

RegGenSIG
(Thanks to @ramialison_lab, whose tweets have been incorporated into these notes)



Jason Ernst - Opening remarks



QUALITY CONTROL


Marco A. Mendoza-Parra - A quality controls system for enrichment-based NGS data sets

Works on retinoic acid.
Want to integrate multiple sources of info for a systems bio perspective. This talk is on how the integration is preformed.

Polyphemus - peak normalization for comparison

Now the actual QC bit. NAR paper 2013.

Same antibody, same number of reads, but different labs produce different peak intensities, background.

Why? There's technical variability. Ab batches can make a difference, so does depth (though my understanding is that those don't explain the above points).

Cited differences in peak callers as another concern.

Their approach is different - 
Remove a fraction of the reads, randomly, and use those to re-construct the profile. Ideally, this will reconstitute the peaks, but with a lower amplitude. Gives you an idea of the robustness.

Measure the dispersion from the ideal

They use quartiles to grade the enrichment. They aim for at least BBB score or better.

Available at http://www.ngs-qc.org/. Integrated into Galaxy.

Q: If you sample background, you'll get background peaks.
A: Input shows up a DDD.

Q: How does antibody validation work?
A. They just look to see if they're CHiP-seq grade via automated platform.



CHROMATIN/ENHANCERS


Xiaole Shirley Liu - Using ChIP-seq and DNase-seq to identify TF binding and their target genes

Started by mentioning various tools:
ChiLin is command-line tool for peak calling
CistromeMap: has someone done chip seq of factor X in cell Y?
CistrtomeRadar- who else's chip Seq looks like mine?
(Many other things listed, too)

Currently running everything on Grc38

Notch-r reg in T-LL: gamma-secretase inhibitor prevents notch from going in the nucleus. However, only 10% of the notch1 binding sites change upon treatment with GSI. Those sites tend to be be in enhancers, enriched for a paired motif.

Dynamic NOTCH1 binding are enriched in super-enhancers (h3k27-defined) [AKA stretch enhancers].

Summary I:
Static TF ChIP-seq might not identify correct target genes
TF and h3k27ac under perturbations might be better.

Chromatin accessible profiling - DNAse treatment + seq finds regulatory sequences.

DNAse-seq optimization - the short fragments tend to be the ones in the DHS peaks, so focusing on them is more cost-effective. Better signal, too.

TF binding from DNAse footprinting: Footprint occupancy score (FOS).
L and R flanks are 6-25 bp. Lower FOS score is better, predicts better TF binding.

DNAse doesn't cut straight through both strands - strand-specific cutting. 
Most likely vs. least likely cut can be 300-fold different based just on the 6mer sequences.
bp-resolution cleavage pattern hurts TF binding prediction when it's close to what DNAseI wold be cutting anyway.

Alternative to TF footprint - overall read counts + motif match. 

So - DNAse data valuable, but you have to be very careful for TF site prediction.




Guo-cheng yuan - Computational analysis of chromatin-state plasticity provides insights into cell identity

Old paper from another group - chromatin states at promoters tend to be invariant, those at enhancers variant.

Quantifying cross cell-type plasticity - compare lots of cells to quantify variability. But DON'T use the variance to do so, since it's not very informative. Instead, you need to do variance divided by the mean. That's the coefficient of variation, but they call it the plasticity score.

High plasticity regions (HPRs) are associated with regulatory regions.

Plasticity is related to DNA sequence.

Pinello PNAS 2014 - pipeline to idientify regulatory TFs.

Haystack almost available - plasticity analysis based on collections of ChIP BAMs

Integrating multiple marks: can use mutual information to block together multiple regions.

Take-home: have to look at multiple cell types



Christina Leslie - Enhancer poising and regulatory locus complexity shape transcriptional programs in hematopoietic differentiation

Enhancer poising

Models transcription programs governing cell state transition, specifically in hematopoietic differentiation.

The Foxp3 sites in T-reg cells were already accessible in normal CD4+ T-cells, with something else opening them up. Foxp3 was taking advantage of a pre-existing landscape.

Divided genes into "complexity classes" - high complexity tend to have longer transcription units, higher fraction intronic (which tend to have enhancers), greater dynamic range of expression, and larger fold changes in cell-state transition.

SeqGL - captures multiple seq signals from ChIP-seq, DNAse, ATAC-seq (Shirley had pointed out the lsat two have problems). A sparse group lasso model to extract K-mers.
Want to differentiate what's in the peak from what's next to it. Sets of co-occuring k-mers suggest a motif. Setty and Leslie, under review.
The point is that using this to compare peaks from different factors can be used to determine what peaks "explain" the peaks in a different cell type.

They initially assign to nearest gene, but then iteratively attempt to improve it by masking the nearby silent genes. Doing so improves performance.



Tarjei Mikkelsen - Massively parallel reporter assays

Put sequence tags into gene of interest, so you can then do this in multiplex and de-convolute relative (semi-quantitative) activity.
Useful for promoters, enhancers, silencers, etc.

Interferon-B example: each subunit has much more promiscuous activity than the complex as a whole, which is very specific to viral infection.

Saturation mutagenesis of the hIFNB enhancers. Turns out a single nucleotide in the WT enhancer explains the specificity, and it's not something you'd guess based on the structure.

Can also use this for detection of new regulatory elements.

MPRA can also be used for transcription factor recruitment, see which things actually interact.
In his example - PPARG - you have a better ability to discern the signal in MPRA vs. ChIP-seq.

(Also mentioned Eric Mendenhall, UAH, and his genomic integration assay)



MOTIFS


Gary Stormo - Improved computational and experimental methods for motif determination

HT-SELEX - like ChIP-seq but with a binding protein instead of an antibody.
Analyze with non-linear regression.

He says some of the published stuff is over-specified (Taipale lab), since they focused on the strongest peaks.
So Stormo wanted an alternative.

New approach starts with a PWM (published, MEME, whatever. It's mostly for speed)
Optimize parameters using Broyden–Fletcher–Goldfarb–Shanno algorithm.
Machine learning.

(Inverse of hessian matrix is the covairance matrix. Hessian matrix is second derivative of the objective function)

Experimental - rather than sequencing input and bound, if you sequence bound and unbound and compare to a reference, you get the energy directly. That's because you only care about relative affinities.

Specificity of the Lac repressor - involved randomizing various parts of the motif, both the sequence and the spacing. 
By doing so, they detect 5% variance in affinity, with 0.1 kT variance in energy - so it's very reproducible.

The high-affinity version is only SLIGHTLY better than the wild type - and the WT is best among those with 3 bp spacers. That matches up with with the structure - as WT, lac repressor acts as a heterodimer despite being a homodimer, binding asymmetrically. Visualized the motifs by plotting the energy with both positive and negative values.



Yaron Orenstein - The use of HT-SELEX to infer TF binding models: comparison to PBM and an improved algorithm

Models very accurate in predicting PBM (protein-binding microarray) binding, but worse in predicting in vivo binding.
So, were those models over-fit? Yeah. Enter Selex.

NAR 2014 Orenstein and Shier - PBM vs. HT-SELEX
Looked at correlation between the top 100 8-mers.

HT-SELEX derived models predict in vivo ChIP-seq biding better than PBM. Why? SELEX has longer motifs, and that flanking sequence gives it the edge.

There's a strong sequence bias, too, though, so watch out.

Systematic biases in HT-SELEX:
Sticky K-mers
The "false" oligos that don't specify a binding site can go as high as 32,000. He suggests removing them prior to the analysis.

HTS-IBIS - unsupervised algorithm to predict in vivo binding
* Choose first cycle for which KL-divergence between current and original cycle is > 0.1
* Correct k-mer count by removing those that differ greatly from their reverse complement

He notes supervised algorithms are better, of course.

HT-SELEX better for in vivo, PBM better for k-mer ranking.



Raluca Gordân - Improved TF-DNA binding models challenge current hypotheses about genomic recruitment of TFs

In E2F family, at least, PWM are insufficient to describe binding.

The consensus motif often isn't there where the actual binding occurs.

Are PWMs any better? Sort of. Higher AUC, but not much higher than random. ROC curves show that, at least in the case of HeLa S3 E2F1, you can't distinguish at low false-postive rates (only can do about 35%).
So cofactors must be involved. Work from other groups supports this.

So, they're not looking genome-wise. Genomic context PBMs (probes centered on predicted sites, which eliminates positional bias).

A lot of probes with high gcPBM binding signal do not contain matches to the PWM.

Regression models using 3-mers good at capturing specificity for these proteins (better than PWM at predicting E2F binding).

It turns out that cofactors may NOT be required, as the proteins bind quite well on their own. Poor PWM models just made it look like that wasn't the case.



Remo Rohs (@RemoRohs) - Quantitative modeling of transcription factor bidning specificities using DNA shape

TF binding using crystal structures

Method he developed: DNAshape (Zhou et al, NAR 2013). Assumes DNA shape at nt resolution is determined by identity of the base and the sequence context.

Mentions SELEX-seq that found DNA shape preferences of Hox TFs. The N-terminal tail is used to read the shape of the DNA.

Yang et al. NAR 2014 - TFBSshape, a motif database for DNA shape features, such as groove width.

GBshape - genome browser for DNA shapefeatures.  rohslab.cmb.usc.edu/GBshape. Not yet published. Also includes electrostatic potential, but I believe they still need to validate that bit.

Looking at DREAM5 dataset, DNA shape improves binding specificity predictions based on benchmark for 69 TFs. Also gcPBM and HT-SELEX.



Marc Santolini (@msantolini) - Deciphering gene regulatory networks using DNA sequence

Limitations of CRM prediction:
* Model accuracy
* Model exhaustivity (since it doesn't catch everything)

Studies three types of models: PWM, Pairwise interaction model (PIM), and mixture model.
PIM, which incorporates correlations between nucleotides, looks to come out on top.

Model exhaustivity - Imogene
Motif-based. Takes CRMs that you KNOW are there, rank motifs based on overrepresentation therein.

Imogene UI is mobyle.pasteur.fr (source is on github)



VARIATION


Stephem Montgomery - Transcriptome sequencing of large human family identifies the impact of rare non-coding variants.
(Missed the first part of the talk due to poster session)

Mentioned using the Platinum Genomes data (Illumina) to identify rare noncoding variant impact within a large family.
They did RNA-seq on all family members.

Since it's in a family, they do eQTLs through linkage, not through association.

More large effect family sQTLs than eQTLs

Are rare variants enriched near large effect family eQTLs? Yes. Enrichment increases with confidence that the effect is larger than in the population.

large effect family eQTL genes tend to be more conserved based on dN/dS. Rare variants influence more conserved genes.
Network approach: yup, large effect family eQTLs tend to be hub genes (degree > 10)

Rare splice variance enriched for large effect family splicing-QTL genes.

Large effect QTLS influenced GAW genes where expression is already likely to have a role

Turns out using the family is a good way to test annotation-guided genome interpretation methods. (e.g., predicting the effects of rare alleles). Rare variant effects are apparently easier to predict than those of common variants.

They filter out de novos, by the way. Only looking at mendelian variants.



Gerald Quon - Investigating the functional role of varience-eQTL in transcriptional variation

Decanalization: Stressful environment tends to increase phenotypic variance of a population.

So if you're looking at, say, increased BMI, you can either increase the mean of the distribution or you can increase the variance. They did the latter.

v-eQTLs show no enrichment for cell type specific enhancers, unlike mean eQTLs.

v-eQTLs are more constitutively expressed genes compared to e-QTLs.

vQTLs overlap QTTs. 
Different mechanism of action than mean-eQTLs. 
V-eQTLs associated with multiple genes, and their targets are involved in core processes
Still don't know mode of action or contribution to heritability, though.



Hunter Fraser - An atlas of human genomic imprinting reveals global patterns of epigenetic regulation

For imprinted genes, expression depends on which parent it came from.

Why? Most popular hypothesis is that it's conflict between the genomes of mother and father in non-monogamous mammals.

ASE - allele-specific expression. Use reciprocal cross (switch parents' gender) to make the evaluation as to if the parent that gave the allele makes the difference.

Imprinting scored using biological replicates as calibration

They now have an atlas that shows mouse genes that are paternally, maternally imprinted, or not imprinted at all.

2x more paternally imprinted than maternal.
Usually not tissue-specific (95% of mouse imprinted genes aren't).

Also working with humans, though obviously can't do reciprocal crosses. 

Imprinting in a pedigree: can look at allelic flipping, such as when it goes from grandfather -> mother -> son.

ASE validation: mmPCR-seq (Zhang et al. 2014)

Human insights:
* Most human imprinted genes probably already discovered
* 17 strong novel candidates
* Involved in growth, insulin response, glucagon signaling, feeding behavior.

Mat/Pat co-imprinting and accelerated expr divergence is consistent with the "arms race" model.



TFs/NETWORKS


Matthew Slattery - Context-specificity in Drosophila developmental gene regulatory networks

TF binding doesn't always match up with the in vitro motifs.

Scalloped as an example - TF with ubiquitous and specialized functions.
Its specialized function (in the wing disc) is more associated with the "canonical motif". But - was that really a tissue-specific enhancer?

HOT regions may have an elevated mutation rate - certainly they're centered on less-conserved sequence.

HOT binding is associated with enhancers in open DNA, while COLD binding is associated with "closed" (epigenetically-regulated) enhancers.

HOT regions are functionally conserved. More likely to drive 'ubiquitous' or 'broad' expression, while non-HOT are roughly context-specific, patterned enhancers.



Shaun Mahoney -

Context-dependent transcription factor activity

Venn diagram won't detect quantitative differences in peaks called in both conditions. He wants something more nuanced.

He focuses on detecting CONSISTENT binding events across experiments.

Mixture model of ChIP-seq binding events: regions compete with each other to take responsibility for the reads. Gives you better resolution than just looking at peaks.

MultiGPS - Mahoney et al., PLOS comp bio 2014

Introduced priors to encourage, but not force, binding even alignment across conditions.
Uses motifs as positional priors.

How does a TF FIND its binding site? 
Looking at Cdx2 as an example. Can lead to three different cell types.
Many binding sites are context-dependent, but not all.
Cdx2 acts as a pioneer at some sites. It looks like sequence information is what discriminates between sites at which it is a pioneer vs. those at which it is not.

The pioneer sites have the stronger motifs; the other sites are more opprotunistic.



Michael Brent, Wash U -
Mapping the transcriptional network of a specific biological process. Developed NetProphet to do so.

NetProphet ranks all TF -> target pairs by regression coefficient and log odds that the target is differently expressed when the TF is out of the picture.
This maps just functional, direct regulation.

Applied to C. neoformans. Studied capsule size, since enlargement is a critical part of maintaining the infection.

Incorporated idea of "phenologs" - genes that work together in one organism tend to work together in another as orthologs, even if the function they work for has diverged.

PhenoProphet did identify relevant TFs.

His overall point is that when you drive this sort of thing by biology, you get biological insights.



Mirana Ramialison (@ramialison_lab) - In silico prediction of mutant transcription factor function

Looking at cardiac TF targets, specifically those of NKX2-5.

DamID-chip technology: When your factor binds, you digest the nearby methylated sites to see where that binding took place. Control is the same thing, but without the factor, just the methylase.

Did that with WT and control, and three different mutants.

Increase in severity of mutation in the binding domain decreases the TF target overlap. (But doesn't remove ALL interactions, so there may be more domains there)

Then did motif discovery on the WT and various mutants. Most interesting one are the new motifs from when the homeodomain were removed, but they also needed to filter out targets of the known cofactors.



EVOLUTION


Katja Nowick - Evolution of a transcription co-expression network active in the primate prefrontal cortex

Brain size increased rapidly over the last 6 million years.

So what are the molecular changes underlying the change in cognition that's associated with that vs., say, chimpanzees? 
They looked at expression and TFs in prefrontal cortex.

"specific" genes in her talk means genes differentially expressed between a species the other two species (among human, chimp, and macaque), but not diff. expressed between those other two.

The most links have been gained on the lineage from H-C ancestor to human (92.7). Due largely to zinc fingers.

Function of the network: GO enrichment of nervous system development and synaptic transmission. This is all in Berto and Norwick, MBE, in revision.

If you look just at the species-specific expression changed networks, you see highest connectivity in human (also mitochondrial function and epigenetic regulation).

There's been rewiring of the TF network involved in brain development, impacting both human cognition and disorders.



Anna Lyubetskaya - Evolution of regulation in Actinomycetes: Benefits of ChIP-Seq for evolutionary studies

Lots of ChIP-seq on all TFs in tuberculosis genome.

Does sequence conservation actually differentiate experimental sites and computational motifs? Does conserved motif = conserved binding site?

Sequence conservation: Does NOT imply site conservation.

The TF-target regulation is more robust than the exact site itself.

Both genic and intergenic sites are conserved better than random motifs. Not just a consequence of ChIP-Seq process.

What processes drive regulon evolution in bacteria? 
Regulon represented as a sequence of states, and they estimate transition rates fro each branch of the tree (of which there are three).
TFBS loss is strongly associated with gene loss.



DISEASE


Carl Herrmann - Transcriptional (dis)regulation in cancer

Opened by distinguishing between "genetic" contribution (binding of TFs, chrom binding proteins), and "epigenetic" contribution (DNA methylation, histone mods).
Both can go wrong in cancer.

First, geneic. Looking at malignant B-cell lymphoma and medulloblastoma WGS.

Non-coding mutations are not randomly distributed in these datasets. This is confirmed by FANTOM data.
Can therefore look for evidence of selective pressure.

Used a background model of randomization of local heterogeneities, which lets you look for positive & negative pressure.

K-mer creation/disruption patterns: those that are created or disrupted in lymphoma are "oncomers", since they might be relevant for malignancies. For example, hi creation, low disruption is oncogene TFBS, while low creation, high disruption would be tumor suppressor TFBS.
High creation AND disruption can find motifs for protein families with dual roles.

Non-coding SNVs a sign for extensive rewiring of reg. network.


Epigenetic - neuroblastoma dataset.

Highly variable CpGs with intermediate methylation are enriched for enhancer elements. 

By finding distal CpGs, can find positive and correlated CpG gene pairs (by correlating those with nearby genes within a certain window)

The distal CpGs can explain changes in gene expression - a sign of possible enhancer methylation. 
There's a Positive/neg correlation depending on the chromatin context.



Hee-Woong Lim - Genome-wide analysis of enhancer-RNA transcription reveals regulatory mechanisms by and antidiabetic drug in adipocyte.

Looking at anti-diabetic drug, rosi

GRO-seq - Nascent RNA sequencing. Why? Steady-state sequencing isn't a direct measure of transcription rate, since it's influenced by degredation.

Gene transcriptional regulation is accompanied by eRNA regulation

Squelching proposed as a regulatory mechanism as rosi.



7/13/14


KN1 - Michal Linial - Good Things Come in Small Packages - Replicators and Innovators

Where are we going? Main theme is tracing the footsteps of evolution. Dobzhansky quote at the bottom of slide.
Particulars:
* creating MAP for proteins
* Challenge the MAP
* Devleop NAVIGATION
* Hidden functions
* Created functions
* The lesson

Or: A treasure hunt for hidden functions.
Some guidelines:
* Listen carefully to the "big data". Be very careful w/ regard to outliers, exceptions, etc.
* But also listen carefully to the biology, since in that context, the "exceptions" may be the interesting part, rather than something misleading.

Example of a lot of data: LOTS in UniProt, but we only understand a very small fraction of them.

Often forgotten/swept under the carpet:
Gene/DNA is essentially written in stone.
Sequence/transcript is static, but you may have a handful of them.
Protein variants are dynamic, and you could have 25-100.
For protein function…context dependent, and who KNOWS how many of these ther are to find. Defining protein function is hard to do.

Protein space is tough b/c it's high-dimensional, metrics poorly defined, a lot of "dark matter".

The ideal is automatic assignment of sequences -> function.

ProtoNet:
All seq in UniProtKB included, do all-against-all blast, look at things even with a very weak E value of 100 (so dealing with even very remote distances).

Bottom up: Agglomerative clustering
Use the above for clustering.
The more the merrier - adding in MORE proteins (even the weak distances) makes signals clearer. Increasing the sample size actually increased the signal per noise in this case.

Merging rules make a difference. 
Changes the number of singletons you have. You have to pick your clustering method based on the biology.

Correspondence score for similarity

Tree built, tested with respect to external families (various databases)
Evo relatedness is captured

A problem: false transitivity. A is similar to B, B similar to C, but A isn't necessarily similar to C (since you may be dealing with different domains). A and C definitely aren't homologous.

So, how to deal with that?
In the next merge, you use average clustering rather than direct clustering (meaning you take all the out-facing links from all the elements in the current clusters into account)

MC-UPGMA outperforms other clustering methods for both families and domains.

Globin example:
The evolutionary duplications are visible on their tree - which is impressive, since they're VERY different sequence-wise (E score beyond 100)

Another use of this protein map: looking for hidden connections.
Can look at upstream nodes to look for functional connection BETWEEN families (looks like looking at most recent common ancestor of the families)

Visualizing the data lets the data tell you what to do. It showed them to focus on a specific subset of the data with respect to Pfam clans.

Small packages: They found a LOT (like, upper 50% of A-B pairs) of similarity between viral proteins - despite having essentially evolved in isolation.

We're getting a LOT of insect genomes. Why? They're extremely diverse.
Can use those for gain and loss of families. ProtoBug, poster C04.

Switching gears to look at things with NO similarity: have to rely on the biology, here.
Hypothesis: maybe short peptides are an unexplored regulatory mode. 

Turns out the longer the protein, the more likely it is that we understand it.
Except toxins - those are short, but we know a lot about those.

ICI (ion channel inhibitor) - have cases in which many folds hit one target, AND in which one fold hits many targets. So there's no simple relation between the folds and the targets, can't really use that for inferring function.

So what's common among these? 
Compact elements, with structure shaped by cysteine bridges that hold them together.

For studying this, design features that capture your intuition.

TOLIPs at the origin of Metazoa -
Cnidaria are "a factory of TOLIPs" (toxin-like proteins).

Can we find TOLIPs of new function in the human brain?
ANLP3 modulates nicotine ACh receptors.

Evolution duplicates and modifies: what starts as just a toxin becomes a clan with cell signaling receptors and others.

Real toxins have irreversible action. The hit channel can't recover. But the non-toxins TOLIPs ARE reversible - why whiskey doesn't kill you.

Toxin-like sequences have therapeutic applications as pain relievers, etc.



LBR01 - Steven E. Brenner - Transcritome targets of nonsense-mediated mRNA decay offer clues to RNA surveillance rules in human, fish, and fly

NMD originally though to be an RNA surveillance system to protect against dominant negative mutations (truncated proteins blocking out full proteins, etc.)

Sox10 used as an example - mutations near N-term are bad, but individuals live to adulthood. By C-term, you die early, in childhood (except by the VERY end of the protein, which isn't as bad). It's because NMD clears out the mutations near the start.

The 50-nucleotide rule: model for NMD in mammals (Nagy and Maquat, 1998). Used to be the predominant model, but…

Hogg and Goff 2010 - length of the 3' UTR may make a difference. A long 3' UTR may define a premature termination codon.

Premature termination codons appear to be defined differently across species - what counts as a "long" UTR differs.

Alternative splicing can ALOS introduce PTCs.

Did RNA-seq to identify NMD targets genome-wide. used HeLa cells, some of which had NMD machinery knocked down (UPF1 knockdown). Looked via Cufflinks and JuncBASE to see novel isoforms that were degraded but are now measurable.

HeLa has about 11,000 genes expressed. 3,932 had early stop codons according to the 50 nt rule in at least one isoform.

2,116 genes with at least one isoform that is expressed robustly (1.5x increased) AND is being degraded by NMD.

In fish" 3,945 genes of 9,492 genes had at least one transcript with >1 FPKM. 416 of those are NMD targets.

50 nt rule IS a strong predictor of NMD in human. If you go more than 50 nt, suddenly you have much higher difference in expression. 
Fish and fly are similar, but less data, so not as strong. More subtle effect, too. Fly and fish have much greater scatter beyond that 50 nt boundary, while it's sharp in humans.

3' UTR length has little effect on NMD degradation. You have to remove the introns in the UTR to see that, though, since introns would cause the 50 nt rule to kick in again.
Even less signal in fly and fish. Human is signif but modest, but fly and fish aren't even signif.

So why did they think it was UTR-mediated in fly before this? They were using microarray, which had a much better means of capturing those short UTRs.

So why make these proteins if you're just going to destroy them?
Because NMD + splicing can be used for gene regulation, such as autoregulation. Srsf2 is one such splicing factor that does this.

NMD is important in the dense regulatory network of splicing factors

10-30% of alt spliced genes produce transcripts affected by UPF1 in fly and zebrafish.



LBR03 - Daniel Himmelstein - Heterogeneous Network Link Prediction Prioritizes Disease-Associated Genes

Prioritization increases GWAS power by increasing prior probability.

Others have used homogenous networks (one kind of node, one kind of edge).
So he's doing heterogeneous networks (many nodes and edge types).

Network includes genes, protein-protein interactions, disease, pathophysiology, tissues, genomic positions, transcription signatures of perturbations, pathways, microRNA targets, TFBS, cancer neighborhoods, GO terms, oncogenic and immunologic signatures.

Looking at the paths you can take from a gene to get to a disease. Each one gets a metric that measures it's prevalence of that path. Metric is mainly degree-weighted path count (since high degree-nodes are less specific, and thus less informative)

Machine learning methodology - observations were gene-disease pairs for 29 diseases with 10+ associations, and a bunch of other stuff. 70% training set.
Used ridge and lasso for regularization. 

83% chance that a positive was ranked higher than a negative. ROC looks pretty okay to me.

Perturbation signatures capture most of the useful gene set information.

Degree-preserving permutation test: AUROC didn't decrease much, which was weird. Node degree accounts for the majority of performance for many network-based approaches. However, edge specificity is important for the top predictors.

Predicting a future GWAS: When looking at given region, this method might help pick the predicted causal gene.

het.io - decomposes predictions into components.



TT02 - Enoch S. Huang - Computational biology careers at Pfizer R&D

AKA Computational Biology at Pfizer: Overview and examples
Simon Xi, Daniel Ziemek, Eric Fauman & Cristoph Brockel

They'll have a booth tomorrow all day long.

Brockel - overview
Precision medicine is about lower attrition through better decision. Picking better targets, essentially.
Also want to select the right patients, since drugs tend to not work for everyone. 


Simon Xi - Alternative Splicing: New Opportunities for Old Genes

He's the Neuroscience bioinformatics lead (been at Pfizer 13 years, 6 years since the beginning of the group)

Since they're interested in working on a given PROTEIN, it's important to know which splice variant of a given gene is the one you want.
RNA-seq helps with that. A lot of reads cross splice junctions helps determine representation of different isoforms in a given pool.

Looking at GTEx dataset: >185 donors, 46 tissues including 13 brain regions. 
Looked at all junctions to quantify alt splicing.

Alt splicing is universal in all tissues. 

MAG Lipase example: if exon 6 is skipped, the tether is gone, so it disperses into the cytosol. 

So, the appeal of this in the Pfizer setting is that they can tell you what version is going to be in what tissue, and what's the dominant isoform.

Splicing patterns also help interpret GWAS findings. For them, they need to know what's the target, do they up- or down-regulate, etc., before they can start a program. 

CD33 example - likely that rs12459419 is the variant affecting splicing. Suggests LOF of CD33 receptor could be protective for Alzheimer's disease. 
They've now found about 100 splicing QTLs in GWAS catalog for a variety of diseases and traits.


Daniel Ziemek - Interpreting Genetic and Transcriptomic data using the causal reasoning engine

Started from a CS background

Can prior knowledge help with transcriptomic and genomic data?
Many omics studies are under-powered for small effects.

Basics of CRE: mechanistic explanations are causal, which implies direction. Delineation of pathways are often fuzzy.
So, can you construct a large-scale network of casual "increase/decrease" relationships? They use curated data to do so. 

Substantial causal knowledgebase based on Ingenuity pathways analysis, Thompson Reuters and Selventa data.

CRE: Input is set of up-and down regulated transcript, and output is set of potential upstream regulators that caused those changes.

Pfizer does encourage external publication, too.

CRE analysis of Pancreatic induction stage (8- to 11-day transition):
Standard pathway analysis helped to validate experimental protocol. Want to identify molecular drivers of differentiation. 
So your diff. expr. genes get reduced to a smaller list of potential upstream regulators. 

Pain sensitivity in the normal population: an exome sequencing study. Looked at how long subjects could keep their finger in hot water, wanted to see if there was a genetic cause for those who did for long/short time.
There's some evidence for Angiotensin 2, which is genome-wide significant.

Bayes CRE: a non-frequentist approach. Tries to model biological context. 
A building block for a biomarker detection method (PP76 on Tuesday)


Eric Fauman - How genotype influences phenotype: a study of human metabolic variation
(Has a biology background)

Measured 500 metabolites in plasma from 8,000 subjects, and then associated those with 2 million SNPs. That's a billion statistical tests.
Can look at which genes contribute to individual variation in metabolism, affect levels of metabolites

An atlas of genetic influences on human blood metabolites (nature genetics)

In the case of the FADS1 locus, the genetic variant impacts the transcription of the gene.

Sometimes these things aren't direct - for PHGDC, the genetic variant affects a key intermediate in the metabolic pathway (but you're kind of three steps removed)

Interesting example of CCBL1 - KEGG missed an annotation of what it was doing within a metabolic pathway.



KN2 - Eugene (Gene) Meyers (@TheGeneMyers)- DNA Assembly: Past, Present, and Future

Early career - '78-'98. Algorithm guy.

"What's behind blast" - paper on the theoretical underpinnings of BLAST. If you google the term with Yandex, you'll get a video of him giving a talk on it.

'03-now: Current career, imaging, etc.

Mid-career: '84-'05. PE whole-genome shotgun seq & assembly

Points out the first shotgun project was by Fred Sanger, back in the early 80s. Did the lambda phage (48 kb). Restriction enzymes -> DNAse I -> Sonication

Roger Staden jumped off from that with the first assembly algorithm. It was greedy, not dynamic programming, but it was used to find the overlaps.

'85-'90 - Esko Ukkonen et al. put out SEQAID in '84. First overlap-layout-consensus (OLC) assembler
SCS was 89-90, saw that problem was analogous to shortest-common-superstring problem (NP-hard)
91 - OLC + B&B optimal

The bi-directional overlap graph - if you ignore all reads contained in others, you're left with reads that properly overlap. Treat each 3' end as nodes, and the edges point away from those nodes.
A path is that in which in-degree equals out-degree, which equals 1. Sounds like no bubbles to me.

Genesis of string grpahs: '94.
Waterman & Idury were the ones who first came up with DeBruin graph assembly.
At the same time, Myers pointed out that shortest-common-superstring isn't the right way to do it, as you compress the reads too much. Brought up a de bruijn graph approach there, too.

He calls them epsilon-string graphs. You want every read in your set to match some arc of that graph (the epsilon takes care of error).

You can use a graph like that to infer copy number by looking at in- and out-node.

A debruijn graph is a 0-string graph (no errors)
Pro is can build in linear time
Con is that, if you DO have errors, you start getting bulges all over the place. The way that used to be dealt with is removing all the kmers that didn't show up very much.
If the error rate is high (PacBio), this approach won't work at all.

Unitig graph - can perform transitive edge removal on a chordal graph to get to this.
Pro: mostly resolved at the level of reads
Con: repeats in the sequence makes it take quadratic time to build

Shotgun era ('95-'02) -
Statistics of coverage, inspired by Fred Blattner '95: Showing that a bac at 10x isn't that much different from a bac-sized region of the human genome when the whole genome is at 10x.

Weber and Myers 96 - IF you can detect the unique regions at 97% id, then you can assemble the genome with mate pairs (used an information theoretic approach)

PE WGS protocol at Celera meant they only needed a factory for shotgun sequencing, rather than shotgun AND physical mapping. 

98 was the advent of capillary gel sequencers. Big gains in throughput.
At the same time, got 64-bit architecture.

WGA in a nutshell - look at the graphs to get all the unique regions, with very low error rate. Then link those together by using the mate pairs that link them. That's how you get the scaffolds. Then can take the reads with one end in unique and one end in the repetitive region, and can then tell how far into that repetitive region the other end goes.

After "the" genome: consequences.
Cost became the priority, driving us to short reads, which hampers de novo genome assembly. He REALLY doesn't like assembly using short reads.

Points out that, while the melanogaster genome is good, some of the others have much too low an N50 to understand evolution. Can compare genes, sure, but not structural variation.

So he got excited about PacBio. High noise/error, but it's RANDOM, and read sampling of the genome is also NEARLY random. Very close to Poisson.

The perfect assembly possible iff (and he tweeted this Feb 22, 2014)
1. sampling is poisson
2. errors random
3.reads long enough to solve repeats.
Error rate itself isn't actually needed.

He's started a blog called the Dazzler Blog. I believe it's his work on PacBio.

The problem with string graphs:
His assumptions worked on sim data, but not the real ones. His assumptions:
1. Reads are all contiguous stretch of the genome
2. Some maximum error rate

But in reality:
1. Chimeric reads
2. contaminant reads
3. unclipped primer sequence
excessively (…something. He switched slides.)

So data scrubbing is important.

In conclusion, high error rate is only a problem with respect to compute efficiency and coverage, not quality.

During questions:
With advent of CRISPR, you should be able to do biology in whatever model organism you want, but you'll need a genome.



WK02 - Bioinformatics Core Facilities

http://bioinfo-core.org/

Core on a Budget vs. Enterprise-Level Bioinformatics

First: Alastair Kerr (@alastair_kerr)

Teaching is key - gets rid of trivial requests

Very small core, just him and Shaun Webb, with some help from advanced users.

So they do a lot of teaching, collaborations, and infrastructure (manage their own servers). Biggest part of infrastructure is software, but hardware and data management play a role, too.

Open source software & data are a big part of how they work. Many mature projects, well supported.
Also, have had problems with commercial software - some were dropped, some started free and went commercial.

The key thing about open source software, for them, is reproducible research. Slide shows GitHub, Galaxy tool shed, ENA, GEO, etc. 
Also makes reference to dodgy perl scripts, but at least they're reproducible dodgy perl scripts.

Data sharing and visualization -
R Shiny
Shiny Tables - no need for excel!
Shiny Graph - data exploration

Hardware:
500 TB disk split over backup and primary sotrage. Uses ZFS on newer severs.
COmpute is 3 key servers
64 GB RAM, 24 cores
256GB RAM, 64 cores
(and one more, but the slide changed)


Michael Poidinger - He's at SIgN, Singapore. Focus on human immunology.

He has 5 postdocs, soon to be 9. 2 have pharma experience.
Uses commercial-grade hardware: Multi-node 64 big clusters, many terabytes of storage, and of course money for cloud space.

The staff works closely with 25 PIs and 180 researchers in the institute.
Every analysis is special - biologists ask them what they can do with the data; projects often extended due to findings.

He puts a big emphasis on the BIO of bioinformatics - they're here to do the science, not algorithm design, etc.

Baby food maker is a collaborator - really cares about difference in microbiome in breast-fed vs. bottle-fed babies, and what they can do to improve their formula.

Many projects:
clinical survey
clinical tests (like allergy skin prick)
cell based assays
HT data
…and more

In THEIR environment, they don't push the analysis back toward the biologist - they have enough money that they don't HAVE to.

(Complicated slide of "their solution" involving Spotfire)

Software/tools -
Commercial licenses for various stuff

Reporting via Moin Moin Wiki (which lets you specify who has write access)

Uses pipeline pilot, too - sort of seems like the pipelining in galaxy, but I'm sure it has things that are more fancy. It's certainly very human-readable.
Can also pipe perl into python into R, etc.

Spotfire used for RNAseq visualization. Like an interactive cummerbund, without the messiness of setting cummerbund up.

Makes the point that commercial software is much less expensive than the machines that it's designed to analyze, so it's certainly worth it if it makes things run on time.



Budget vs. Enterprise Open session

Q: Which commercial bioinformatics packages have no good open source equivalent? Why? Are they worth the money?

MP - This is an open version of Spotfire, though with fewer features. Spotfire does have it's own web publishing element, for one thing.

AK - R shiny and all that don't cover everything Spotfire does, and takes a little more time to set up, but does sound like it has a useful GUI component


Q: For new cores - what do you suggest? Esp. for low budget.

AK - have an idea of the kind of data you'll be working with. You COULD go straight to AWS, but depending on how much you're dealing with, that might not be financially viable. 

MP - first thing he did in Singapore is to carve out his niche and make it clear what resources he'd need to run a competent core.


Q. Stephen Turner (@genetics_blog) - Suggests putting the above answers on the wiki page. Says his shop is very similar's to AK, under cost-recovery restraint. A lot of his people want the gene list, sure, but then they want pathway analysis. Ingenuity PA is expensive to get others to use - is there an alternative? He's a biologist, but will never be as good as the expert asking the question of him.

AK - Talk to the scientist about what they ACTUALLY want to get out of this - they might think they want the pathway, but maybe it's not really what they're going for.

ST - Yeah, people gravitate to the stuff they already know

Audience member 1 - There's DAVID for enrichment analysis, Gene Mania to find papers with similar gene lists

MP - Remember DAVID hasn't been updated since 2010

Audience member 2 - Maybe have the library buy the Ingenuity license (guy from Tufts agrees)


Q. Audience member 3 - does commercial software make you less agile?

MP - They use freeware, too. The real goal isn't to buy everything, it's to build something that'll still work six months down the road. 
CuffDiff gives you FIFTEEN different outputs. He wrote something to whittle it down, and they haven't looked at the full output since.
His software improves the efficiency of his staff 20-30%.
Why getting 4 new postdocs? 4 grants were funded that all said they needed a bioinformatician, but those PIs were smart enough to know it'd be better to not have them sit by themselves, so they're moved into the core.

Audience member 4 - It makes sense for them to buy commercial software for their core.



Open Session

Pipelines

* Commercial - accelrys, knime
* Open Source - galaxy, taverna
* Home grown - cluster flow, gnu

Who does what? How did you choose?

Audience member 5 - they did build one, since they couldn't find one that supported the systems that they were interested in. Has to do with the very specific requirements of your own local computing infrastructure.

Audience member 6 - Needed to track parameters, needed logs for reports, consistency across analysis, needs to be easy to use/learn, operate across different compute environments, and robust/adaptable to changing of component software.
So, they use the lowest common denominator: shell and perl.

Simon Andrews: Sure - but that doesn't scale very well.

Audience member 2 - her core is for a clinic, so the MDs want essentially a web interface. She want to python - now it's not a pipeline, it's a SYSTEM of pipelines (some of which are hard-coded, because the MDs like them that way). Open source is great, but you need to be really careful about design. Even though she built it modularly, it's now at the point that you can't change one thing without changing the whole thing. One of the pressures on her is the combination of accuracy and speed required by the clinic for, say, in vitro fertilization.


Managing workloads -

Slide on the various things you always need (run analysis, manage compute infrastructure, manage data pipelines/LIMs), jobs you need, but not all the time (training, papers, data submission), and longer term goals (develop training, develop tools, testing new tools)

How do you balance day-to-day vs. longer term planning?

Moderator says they spent about 10% of their time making fastQC.

How do you guide users with unrealistic or unreasonable requests? (e.g., "can you make all the same figures as this paper", "can you download and reprocess these ENCODE samples", "I want all my work done by person X", repeated speculative analyses, "Quick" jobs)

Get your management to back you up in saying "no"

Ask them what they're going to DO with the analysis. If they can back it up, okay - but don't let them "walk through the park".

Train your users. The more they understand, the more they at least get what they can and can't ask.

Maybe have a ticketing system - it's split about half and half. Looks like the bigger cores are the ones that have formal ticketing.

Charge for the time it takes to make the figures, and give them an estimate. They've adopted that one part of Agile, it works really well.


Funding your core -

Slide - time spent in his core is mostly advising consultancy and analysis.

Cost recovery is used by a lot of cores. Can be based on analysis, fraction of staff salary, etc.

But there are things done that aren't charged for, and why?

Training (since they want people to come to said training); initial design work

MP - you CAN charge for training, and people will come in droves. They're thirsty.
Also…there's a perception that if the training is free, then it can't be good. So even a small fee can incentivize people.

What about the time spent reading the paper that someone wants you to reproduce the figure from?

Sounds like people tend to eat that time right now, and don't charge for it. Again, using what you read to figure out how hard the task will be, and using that to provide an estimate to the client, can help.



7/14/14


KN3 - Isaac (Zak) Kohane - Biomedical Quants of the World Unite! We only have our disease burden to lose

Says his job in the next 50 minutes is to get off your butt to get useful things done for society

There's no national health identifier, there's no national payer system (here), electronic health records only really got adopted well within the last 7 years, but it's out-of-date.
New alternative models of healthcare industry; threat/promise of accountable care (paying doctors only for the result)

So doctors are seeking new finding models for research. Example: Cf.Regeneron/Geisinger.

Afferent and efferent arms of healthcare information: afferent is where we get data from patients, etc., and the efferent is where we actually use that to do things.

First up, the afferent axis. 
There's a lot more than just the healthcare data (medications, radiology, examinations, etc.). There's also the social web, genomics, environmental modeling, etc. He says they're all relevant to understanding current/future state of patient. They've published this in JAMA.

I2B2 - free, but requires a big investment for data extraction, but still well-adopted.

Discarded samples could be used inexpensively to get information

The closer two hospitals are to each other, the more they hate each other. Still, he made a distributed query system so that investigators could take advantage of data from the WHOLE system. Makes it easier (possible) to find rare patients.

Brownstein, PLoS ONE, 2007 - Rofecoxib caused a huge - 18% - increase in heart attacks, but you wouldn't really know that without having done the analysis.

If you're healthy at age 80, you're EXTREMELY abnormal. Tells you something about the controls you'd want to use.

Naive Bayes predictor developed that detects incidence of domestic abuse based on diagnosis at various visits

There's about 5,000 various co-morbidities of autism.
Used a 5,000-element bit vector in 6 month chunks, and then clustered those.
Able to find things like sub-divisions within autism - some with higher incidence of seizures, some with psychiatric disorders, some with what looks like innate immune diseases. His point: You'd want to study those three groups separately.

Healthmap - map of all infectious disease in all locations at all times. Mines "a variety of data sources" to do so. Looks like a lot of text mining.

Environment-wide association study - Chirag Patel. Mass spec of various patients from different , looking for heavy metals, etc. Accounts for effects genomics can't.


Efferent arm - making things happen.

While trying to make clinical bioinformatics happen, argument over standards arose. So they made a challenge (CLARITY challenge). So instead, they gave them the data of a really sick kid that was going to kill them, and teams came to work together to get the diagnosis.

Arithmetic: If you have a disease with 1:1000 prevalence and a 5% false positive rate (but with a test that misses nothing), what's the probability that someone diagnosed with the disease actually has it? 2%. There's one person with it, and 50 false positives, so 1/51 is ABOUT 2%.

Incidentalome - if you have a large N, even with a really high specificity, you'd still get like 60% false positive rate across the US population. Need to know the positive predictive value.
Gave various examples where such false positives had disastrous results (BRCA1 leading to unnecessary mastectomy/hysterectomy; PSA; heart disease in African Americans)

EHR - why can't electronic health records be more like the iPhone? Some EHR companies are coming together to use their software. 

Questions he poses:
* Who, if not the healthcare system, is actually going to do this?
* Are we willing to slow down in the interest if worrying about patient safety?
* How will we fix the effector arm (with a very regressive, entrenched healthcare establishment)?
* What's the quant alternative? Disrupting from the outside?
* Should this not be an ISMB priority?



PP23 - Zhaojun Zhang - RNA-Skim: A Rapid Method For RNA-Seq Quantification

Related work -
Alignment-dependent methods: reads aligned back to original transcripts (Cufflinks, RSEM, eXpress, etc.). Takes hours to finish (millions of reads, so huge search space). Estimate the abundance of transcripts.

Second group is alignment-free. So far, that's only Sailfish.
Uses k-mers instead of alignments to quantify. Does not depend on an alignment step, so it's 10x faster, but still supplies comparable results.

How can RNA-Skim be faster? Showing a graph of number of reads compared to location. Can we aproximate the area under this curve? That'll give you abundance.
Considers abundance levels of transcripts to be random variables that follow the same distribution. If variance introduced is an order of magnitude or smaller than transcript variance, he says it should still work.

Sig-mers: Given a partition of the transcription, a sig-mer is a k-mer that occurs in only one cluster, and therefore uniquely identifies the cluster in which it is contained

So you sample the sigmers instead of the full reads.

Probability of sequencing a sig-mer from a transcript: want to maximize the likelihood of observed sig-mers.
(align-dependent maximizes the likelihood of observed alignments, and sailfish maximizes k-mers)
So it's the same underlying idea.

Optimization I: Finding all sig-mers.
Straightforward way is to store all k-mers in memory, which would take 50G. Instead, they use bloom filters to reduce the memory usage down to 100M. Probabilistic data structure, so you may miss some sig-mers.

Detecting sig-mers in RNA-seq data:
Multiple pattern search problem. Use Robin-Karp (rolling hash) algorithm to do so in O(N) time. Means the algorithm is invariant to the size of sig-mers.
It reuses the overlapped values when looking at k-mers, which is how it gets down to constant time. Seems like a sliding window to me.

Quantify the reads through some EM approach.

Results on simulated data:
RNA-skim has the best Pearson correlation, but the other metrics (rank correlation, signif false positive rate, signif false neg rate) are all better in other methods. No single method is best in more than one category, though.

Testing on real data:
Used 56 inbred or F1 mice. Microarray data is NOT ground truth, so can't determine false positive or false neg. All five methods are pretty similar, though RNA-skim isn't better than the other methods.

It is, however, faster. About 10x faster than Sailfish.

Conclusion:
Redundancy in RNA-Seq should be considered in statistical modeling and algorithm design for RNA-Seq

csbio.unc.edu/rs. Five small and independent componenet



PP28 - Pankaj Agarwal - Computational Biology in Medicine: Novel Targets and Drug Repositioning Use Cases

Nature Reviews Drug Discovery, 2013 Aug; 12(8):575-6 - http://www.ncbi.nlm.nih.gov/pubmed/23903214

Cancer drug targets: the march of the lemmings - are drug companies just all trying to hit the same targets?

Active projects from Informa Pipeline/Pharmaprojects
247 proven targets
712 novel targets (no approved selective drug)
These were the ones that they looked at.

42% of targets had only one target looking at it (out of 1,027 targets). 26% are being looked at by FIVE OR MORE companies.
The proven targets are 64% looked at by 5+ companies (since everyone knows it works, but could be improved)
For novel targets, that value is only 13%. 54% of novel targets are only being looked at by a single entity.

More validation leads to more competition. As you progress through the phases of clinical trials, the percentage of targets looked at by a single entity shrinks, and the 5+ value grows.

Centre for therapeutic target validation - Ewan Birney is in charge of it

The kind of validation he (Pankaj) is interested in is drug repositioning.
Given a compound/drug, what new disease applications is it relevant for?

The data that goes in: Molecular assays, phenotype assays, in vivo models, clinical trials, prescriptions. Hurle and Ararwal, Clin Pharmacol Ther 2013. - http://www.ncbi.nlm.nih.gov/pubmed/23443757

First method: connectivity map. Lets you look at a drug and visualize a whole bunch of phenotypes associated with it, and compare to the phenotypes associated with the disease (expected changes). Lamb, Golub 2006, Science. http://www.ncbi.nlm.nih.gov/pubmed/17008526
Cheng, Agrawal PSB 2013. - http://www.ncbi.nlm.nih.gov/pubmed/23424107

Want to build a standard dataset.
Example: Does Cmap work for repurposing?
AUROC isn't that useful - want high precision, not a million things to go test.

GWAS - can insert SNP data into the connectivity map, via ENCODE, FANTOM5, nearest gene, eQTL.
Looked at overlap of GWAS and known drug targets. Sanseau, Agarwal. Nat. Biotech 2012.

Combining screening data - if you have a target for on disease, and it turns out it affects a phenotype for another disease, maybe the clinical compound that binds to the target, designed for the first disease, could be relevant for the second.

Clinical asset repurposing triage - use experimental validation to review and refine evidence you're culling from the data.

Progressing the hypothesis - likeliest decision is to STOP. Need to know if the drug itself is progressable, and incorporate learnings into priors (patent position, target, disease, drug history). Constrained by time, too.

They welcome collaborations, interns, new colleagues. Contact is Parkaj.Agarwal@gsk.com.



PP31 - Michael K. K. Leung - Deep Learning of the Tissue-Regulated Splicing Code

Cassette splicing - alt splicing in which a given exon is included (or not)
PSI - percent of transcripts with alternative exon spliced in.
Goal is to predict alt splicing patterns.

Neural networks - 
feed-forward: inputs go to outputs, passing through hidden nodes. You have a cost function

For a given hidden state, you sum the inputs and pass them through a non-linear function.

Deep learning an neural networks - exploiting many lays of those hidden, non-linear layer. 

GPUs important to machine learning.
Neural networks have been successful in various domains (computer vision, audio processing)

But is it applicable to the relatively small datasets used in biology?


Alternative splicing dataset - RNA-Seq data from mouse in five tissues, 11,019 cassette exons studies. Want to look at PSI and change in PSI between tissues.

DNN for splicing prediction predicts two things: low, med, or high PSI, and tissue difference.

Training:
Stochastic gradient descent with momentum, balanced mini-batches, dropout regularization
Hyperparameter search, GPU acceleration (15x speedup to actually accomplish said search)
5-fold cross-validation. 3 for training, 1 for validation, 1 held out for testing.

Interpreting the DNN -
Can use class saliency to do so. Forward propagate your data to get the output, then perturb the data and back propagate.

Large model with sufficient capacity.

Splicing code with improved predictive performance, and yes, deep architectures can help with small datasets.



TT14 - Raymond Tecotzky (Dir. of Illumina informatics marketing) - Sequencing and Genomic Analysis Onsite and in the Cloud

Focus areas:
Cancer
Enterprise informatics (where he's from)
Life scinces
New oproutities
Reproductive and genomic health

Tip of the iceberg - challenges & opportunity for discovery. The large amount of data, basically.

Last year, Google formed a company called Calico - entering life science realm.

A LARGE percentage of the money spent is spent on bioinformatics.

As throughput scales up, so does compression. Remember when we used to save the IMAGES from the illumina machines? Now we save BAMs, but soon we'll likely only save the VCFs.

BaseSpace - cloud-based genomic sequencing.
Wants to be an "app" based interface, and anyone can develop for them.

"cloud in a box" - basespace onsite. Which…obviously isn't a cloud anymore at that point.

Role of basespace for bioinformaticians - sounds like he's pushing the fact you can push the analysis to the biologists by making the analysis software user-friendly. Says it'll remove our role in "mundane analysis".

45,000 apps launched (with 8,000 logins every week)

FDA uses uses MiSeq and BaseSpace to stream data and share it to cloud, which is sent to NCBI. Sounds like it isn't being analyzed in the cloud, though.

Or, wait - now he says there are over 30 apps in basespace. 

Sharing and collaboration is the most popular app, of course.

How is data stored an analyzed in basespace?
.bcl, the raw reads, are sent to basespace. Then demultiplexed to fastq, then stored as projects. The apps operate on the projects.

Native apps - execution-friendly environment.

e-commerce infrastructure supported, handled by illumina, in case you want to make a pay app. Uses "iCredits", which is basically US dollars.

Can publish software on basespace, so any researcher is one click away from being able to run the program there.

To learn more as a developer, see developer.basespace.com

Questions:

Q: Are there plans to allow uploading of different programs, genomes?
A. Yeah, they're working on it. Right now, basespace only takes date from the instrument.

Q. What kind of motivation is there for someone to develop apps for basespace?
A. Some apps are private, so you can just share it with your colleagues if you want. 

Q. As the API evolves, what happens to apps that break?
A. They notify the community what the changes will be implemented, so developers know what to change. In the future, the native app environment will help with that, as it doesn't rely on a single API.

You can also include the reference data in the app itself (in response to another question), as a workaround to get the data up there.



PP40 - Menachem Fromer - Statistical challenges in whole-exome sequencing of 7000 individuals with schizophrenia and controls

(Missed the first part of the talk)

No significantly increased rate of mutations in schizophrena
BUT…were there gene sets enriched?

dnenrich - subset of all genes is genes with sequence coverage -> longer genes (more de novos) -> CG positions. Take this into account when modeling the genome. Need to model because you can't get a large enough set of control trios to estimate the per base mutation rate at EVERY base.
Treat that like a dartboard, with semi-matched "darts". So if it hit GC before, it has to hit there again - but not necessarily the same gene set.
Permute that (bootstrap?) and see what sets are enriched.

Cases/controls matched for ancestry of common variants, and with respect to rare genetic variation.

Need to group variants, since otherwise doing them all via 2x2 contingency tables will never give you the power you need.

Need to focus on singleton nonsense variants.

Burden represents rare mutations across many genes - not localized to a few genes

Plink-seq - flashed URL, but took it down pretty fast



LBR08 - Sara Gosline - Beyond Argonaute: understanding microRNA dysregulation in cancer and its effect on protein interaction and transcriptional regulatory networks

miRNA impose a modest fold change - but are implicated in many biological processes.

Can search TCGA for miRNAs with expression correlated with survival (both at high and low levels)

Can then look at mRNA in those same patients. miRNA effects are observed by measuring expression ANTI-CORRELATION across patient samples

Hypothesis: miRNA function is determined by NETWORK of changes caused by miRNA-medated repression.
Also wants to identify the relevant signaling molecules

Networks involves miRNA, mRNA, protein, transcription factors, and the mRNAs that THOSE produce. There are plenty of interactions in the data for all of these.

Tool they use: SAMNet reduces a large netowrk of putative interactions to the most meaningful. Sh has a 2012 paper in Integrative Biology on this. Finds miRNA-specific edges in a network, and reduces them to the paths that best explains the miRNA-mRNA anti-correlation.
Each edge has a weight and a capacity. Want to minimize based on weights. Capacity is a constraint that forces all mRNAs to share edges.

iRefWeb is used to characterize the edges

Protein-DNA interactions are predicted from DNase I hypersensitive regions

Some edges in network are specific, while others are shared, hitting common cancer pathways.


Using networks to ascribe miRNA function:

Looked at apoptosis, cell-cycle specific stuff


Finding proteins of interest:

Signaling proteins that mediate the effect observed above.

Rank nodes by flow ascribed to them. Compare across networks using a heat map.

Core nodes (very high flow) are shared across all networks (Src, TP53, Myc, Brb2, Hif1a)

Can identify signaling proteins UNIQUE to specific cancer networks - which make good drug targets.
Smarca5 is unique to breast cancer. It's a target of Mir100.
Dazap2 unique to pancreatic cancer.



WK04, part C - Monolis Kellis

(Talk started early, so I missed the beginning - but looked to be discription of ChromHMM)

HepG2 motifs vs. tiling enhancer dissection deconvoluted signal - activator give higher reg potential; a different cell type with a repressor shows the oppose effect

So we know chromatin states, etc. How to interpret disease-association studies?

Focus on SNPs falling into regulatory states, for one thing. HaploReg was made to do that (compbio.mid.edu/HaploReg).

Activity patterns improve GWAS enrichment. Enhancers plus DNase data, you get more enrichment than either alone.

***

Bayesian model for joining weak SNPs in pathways - link the genes most likely to be involved back to their regulators.

Missing heritability partly due to weak variants; regulators lacking association harbor rare variants


Experimental dissection of disease regulatory modules - 

Did some cardiac study in fish using morpholinos. (…Maybe they should rethink that one, given all we know about morpholinos now.)

Summary:
1. Reference epigenomics -> chromatin states (dynamic elements in multiple cell types)
2. Interpreting disease-associated sequence variants (mechanistic predictions)
3. Disease networks: link SNPs -> genes -> phenotypes (link enhancers to targets; this is where the Bayesian model comes in)
4. Genetic/epigenomic variation in health and disease (such as genetic variation <-> brain methylation <-> Alzheimer's disease)
5. Experimental dissection of SNPs/motifs/TFs/targets (AD in mouse, heart in zebrafish [this was the morpholino one, studying SNPs that were below significant], BMI in human and mouse)



LBR10 - Matthew Scotch - Utilizing a Phylogeographic Generalized Linear Model for Identifying Predictors Driving H5N1 Diffusion within Egypt

Public health genetics for disease surveillance

Phylogeography - sub-discipline of biogeography. You look at evolution and divergence, but with a spatial component as well as temporal.
Focus on sequence data, but also metadata, is used in the modeling process. (like location, year, host)

Popular applications - Paup*, BEAST (bayesian phylogenetic inference using Markov chain Monte Carlo; does discrete and continuous phylogeny)
Discrete has spaces in specific bins (example: if only modeling canada and the US, you can't model spread into mexico). Continuous uses latitude and longitude, brownian diffusion.

Early work focused on mtDNA polymorphisms
Mre recent: RNA viruses: seasonal and novel influenza, rabies, WNV. Purpose is to inform public health of how the viruses spread & what factors influence that spread.

KML file can be put in Google Earth to visualize the spread of a virus over time. The spread is synonymous to branches in the tree.

Limitations for surveillance -
Focus on sequence data
Lack framework for integrating epidemiologic variables with evolutionary diffusion (demographic, climate, phenotypic traits)
Spatial epidemiology

Spatiotemporal hypothesis testing: generalized linear model works for this (GLM)

Mentioned this paper: http://www.ncbi.nlm.nih.gov/pubmed/24586153
The found that passenger airflow was a signif predictor for H3N2 diffusion


H5N1 -
70% of all trade of poultry via live bird markets
Egypt has a very diverse climate (desert, mountains, populous and condense Nile delta)
…but didn't know cause of spread.

So looked at a bunch of birds, mostly chickens.

Predictors:
Avian population density (FAO, but only up to 2005, so extrapolated forward)
Human population density
Distance, elevation, and latitude (had to take the centroid of the various regions, though)
Precipitation
Case counts
Motif identification at HA cleavage sites (known to influence transmissibility in experimental studies) using sequence viewer

Used an older version of BEAST (1.8)

Predictor support - used Bayes factor. BF >= 3 means they're 3 times more interested in a predictor after the posterior.

Cross-species transmission: Aimed to identify species most responsible for viral diffusion. Not in GLM due to formatting requirements of the matrices, though

Results of GLM:
Avian counts are the big ones. Followed by all the density predictors of human and individual bird density.
The motif was important, too, as well as precipitation and elevation.
For all of them, they're confident they contribute - but uncertainly means they really don't know how much any of them contribute, as error bars all span zero.

Chickens, which are the more popular birds, don't infect humans as much as the other birds. Humans, on the other hand, are much more likely to give the infection to the various birds.

So factors look to be environmental, demographic, geographic, and genetic predictors.

Limitations include a low number of human sequences (14)

You can generalize this type of framework to studying other RNA viruses. They'd like to develop a platform to integrate this work so it's useable by others.

A problem with genbank - the metadata isn't as specific as you'd like, which limits studies like this (why he had to break Egypt into governoids)



KN4 - Dana Pe'er - A multidimensional Single Cell Approach to Understand Cellular Behavior
AKA "Dimensionality in biological data: the power of single cells"

"Thank you for that very embarrassing introduction; now I can only disappoint."

Thinks of cells as little computers. Need to sense environment, incorporate signals, make decisions.
She uses machine learning to learn the cell networks from data
Perturb and observe: need to sample the cell in many states.
Basic assumption: statistical relationships reflect underlying biology

The genomic challenge for machine learning: relatively small sample size (relative to Amazon shopping cart or Netflix, for example).
Unlike them, we have a LOT of variables d, and very flew samples n (opposite of the above).
So…too much complexity with far too few samples to learn the correct model.

Modularity can increase statistical power - Segal, Regev, Pe'er et al. Nat gen 2003 (http://www.ncbi.nlm.nih.gov/pubmed/12740579)
Modularity has since been used to aid network learning. Gives you an interpretation of the data, as the network is better organized. More samples to learn with.

Data integration (like combining DNA variation and expression) - DNA anchors causality. Modeling is key.

Learning networks from single cells: certainly a way to get around not having enough samples, if you treat each cell as a sample. Idea is to use nature stochasticity within a cell population and treat measurements of individual cells as the input for learning

In her example, even a small sample of single cell data did better than the average data (this was 2005). Need large number of sample and single cell resolution are needed for success

Mass cytometry: a game changer. GIves 45 dimensions simultaneous in MILLIONS of indinvidual cells. Bendall, Simonds et al Science 2011

Data isn't linear nor Gaussian, so you do non-linear dimensionality reduction. Perseveres distances between high dimensional and low dimensional space, persevering the shape both locally and globally.

viSNE Map of healthy immune system: Clusters different cell types together, naively.
It's robust, too.

What if you assume linearity? PCA doesn't work too well, since the data is NOT linear.

How does leukemic bone marrow differ from healthy? Phenotypic space is deformed in cancer, but consistent in healthy samples.
Markers reveal the structure of heterogeneity.

viSNI also detects a tiny relapse population.


Bendall, Davis, Amir et al Cell 2014 (http://www.ncbi.nlm.nih.gov/pubmed/24766814) - How does signaling change and rewire along B-cell development? What's precise order and timing of events during B-cell development?

Idea is to use the trajectory of development as a scaffold, which you can pin other things to.

The challenge is, again, non-linearity. Euclidian distance is a poor measure for chronological distance.

Use "graph walk" to derive trajectory - but very noisy data, many additional tricks needed to avoid the path jumping down shortcuts.

How deal with it? They use randomness to overcome noise.
Use randomization - instead of k nearest neighbors, do a kl nearest neighbors graph. That way, the shortcut only shows up in a few of the random graphs, so it gets averaged out.

Wanderlust - infers "developmental clock"

Refine distances using waypoints (which seem like those white dots in Google Maps when you change the route)

Pinpointing the timing of VDJ recombination - CD24 consistently bisects TdT expressing cells, occurring BEFORE CD10

Rewiring of signaling network - Wanderlust algorithm picked up a signal occurring in only 7/10,000 cells

She wants to do for humans what was done for c. elegans, and do the fate map (or at least do it in human bone marrow).


How does signal processing differ between subtypes?

DREMI captures "edge strength" - the power of having many cells. This is from 2% cells, which would normally be thrown out as outliers - but since those are 2000 cells, you can learn something and be very confident in it.

Now can compare naive and effector memory T-cells. Naive cells have more faithful transmission of the data.


What's next?
Put it all together to study cancer. Wanderlust to study trajectory of cells, look at edges to view rewiring in the network.
What else? Don't want to be limited by antibodies, so RNA-seq is the answer. They're working on a better technology to do this.



7/15/14

KN5 - Robert Langer, Sc.D. - Biomaterials & Biotechnology: From the discovery of the first angiogenesis inhibitors to the development of controlled drug delivery systems and the foundation of tissue engineering

After grad school in 1974, he had 20 job offers from oil companies, but 40 academic positions didn't even write him back.

Needed an assay to test factors that stop blood vessel growth (due to relevance to tumor growth). Decided to use a rabbit eye - it's big, and has no blood vessels in it.

Delayed drug release - originally told it couldn't be done, and his first NINE grants were turned down. He did show that it IS possible, though.

A large molecule working its way through one of these pores is like driving through Boston - high tortuosity, takes a long time to wind its way through.

Starting in 2004, angiogenesis inhibitors started being approved for clinical use.

Getting the patent for delayed release involved showing the patent examiner an article citing their previous paper, showing it was a surprising and useful result. The examiner was impressed, and asked him to get signed affidavits from the five authors of the other paper, stating that they had actually written what they published. True story.

Single compound release, using a chip: electric signal causes the cover to fly off in seconds.
Idea was to activate it remotely, so doctor can dial the dose up or down.
Gates foundation has also funded them to do the same sort of thing on fertility control.

"Something that was designed to be a ladies' girdle may not be the best blood contact material" - things like the artificial heart were designed because of household objects that seemed like that'd be an okay material, not because that's optimal.

Designed a polymer that erodes in water, but not by enzymes - because we all probably have excess water, but different enzymes.
Ratio can be used to change percent degradation.
(This is all taking place in the 80s, by the way)

Laundry list of reasons they were told things won't work:
Polymers can't be synthesized (1981)
Polymers will react with encapsulated drug (83)
polymers are fragile (85)
polymer-drug system would be toxic (86)
The drug will not diffuse far enough to kill remaining tumor (88)
Even if it does, it is a very poor drug (90)
The drug delivery systems cannot be manufactured (1993)

But of course it DOES work. 

Another anecdote - had two bloody slides of implants in brain, which he once showed to chemical engineers for 12 minutes with no warning. They didn't like that, so he went to giving a warning, and only keeping them up briefly. Later, he was invited to give a talk to neurosurgeons. They didn't mind the bloody images - but found the slides with chemical formulas to be excruciating.

Polymer fibers plus liver cells - trying to grow a liver. Still early stage, but they've used it to grow cartilage in mice.
Also using it to grow new ears for army vets.

Also showed an implant for spinal chord injuries, getting rats and monkeys to walk (somewhat correctly) again



WK05 Part A - Levi Waldron - An overview of genomic data analysis in Bioconductor

Bioconductor conference: July 30 - Aug 1, Boston USA (register.bioconductor.org/BioC2014)

Application areas:
* Microarray analysis (expression, copy number, SNPs, methylation, etc.)
* Sequencing
* Annotation
* Epigenetics
* Gene set enrichment
(and more)

Many levels of documentation.
Top level: bioconductor.org/help
Workflows: bioconductor.org/help/workflows (sequence analysis, RNAseq differential expression, oligonucleotide arrays, variants, etc.)
Package vignettes: Working "literate code" demonstrating the use of a package
Function man pages and reference manuals

Additional sources of documentation:
bioconductor.org/help/course-materials - also has the slides from today!
BiocViews - hierarchical controlled vocabulary for easier searching of what's available

Key data structures - as a submitted, expected to use these.
Containers: Expression set (matrix of samples and results, but also containers for metadata), SummerizedExperiment (similar, but for genomic positions), GRanges (coordinates, and additional data)

Microarray analysis - what bioconductor cut its teeth on. 300 packages for this, all kinds of arrays supported. If you have a weird array, see Arrays workflow.

RNA-seq differential expression analysis - 55 such packages

Slide on string-related data structures and tools - useful, because sometimes it's hard to tell where to go to see how to use these things. Most rely on Biostrings.

Annotation resources - you have regions, you want to annotate with genes, pathways, identifiers, ENCODE stuff, etc. That's what this is for. Pre-built packages give you fast local analyses.
Web access packages give access to UCSC and BioMart. Annotationhub is a new addition.

There's an RStudio IDE, and also online courses. Boilerplates for making presentations and web documents are all there, too.

R can be slow or fast, depending on how you use it. Vectorization speeds things up, as does Rcpp (which lets you put in C++ code)
The other things on the slide list additional things that speed it up. library(data.table) can speed reading of large files.
library(sqldf) builds dataframes from SQL queries
knitr can also speed things up via caching.




PP59 - Jeroen de Ridder - Chromatin landscapes and long-range interactions or retroviral and transposon integrations

PLOS genetics - chromatin landscapes of retroviral and transposon integration profiles - dx.doi.org/10.1371/journal.pgen.1004250

Last few slides will explore long-range interactions

Looking at Retroviruses and DNA transposons. Relevant for gene therapy, study of gene regulation (akhtar and de jong et al, cell 2013, using sleeping beauty - http://www.ncbi.nlm.nih.gov/pubmed/23953119)

Mutations can affect genes, either activating nearby genes or inserting within them.

In mice, you can either introduce new elements or activate the ones that are there already, and you'll eventually get a tumor. Some are drivers, selected for in tumor development.
Can they distinguish between the drivers and passengers?

One way to address that is to do the experiment a lot, so you're relying on selective pressure to find common insertion sites (CS).

Ah, here we go. That method assumes a random distribution. If you don't have that, the biases give rise to "spurious" CIS.

Looking at integrations without selective pressure, in the following cells:
SleepingBeauty: mESC
PiggyBac: mESC
MMTV: NMuMG cells

They analyzed about 80 features, and histone mods are apparently among them

Looked at signal compared to signal some distance d away. Get a t-score for each.

SB and PB are similar for some features, opposite for others.

Some features are significant only at larger scales, or smaller.

Do they NEED macrofeatures to explain the distribution, or do microfeatures take care of it all? Used a Markov blanket analysis to do so. Lets them estimate confidence, via bootstrapping, of finding a feature upon which a given node depends. Found macrofeatures are relevant, and often shared between systems.

Target site selection is similar between PB and SB at large scale, but differ at small scales.

So can you use it to detect spurious CIS? Yup.

Co-localized insertion clusters - do you find different hotspots when you take 3D nature of genome into account?

Kool et al cancer research 2010 - http://www.ncbi.nlm.nih.gov/pubmed/20068150
Uren et al Cell 2008 - http://www.ncbi.nlm.nih.gov/pubmed/18485879. Two big MLV integration datasets. Well, they were big at the time, anyway.

Used that data and compared bins under different clusters to the Hi-C clusters of things at approx. the same linear distance.

Dixon et al., Nature 2012 - http://www.ncbi.nlm.nih.gov/pubmed/22495300. Use of Hi-C.

Found 874 CLICs, some of which you wouldn't see without taking this 3D nature into account.

Taking 3D chromatin confo. into account, you can find additional insertion hotspots.

Q. Some guy claimed that AAV and MLV have very strong sequence biases, and asked if they saw any of that (but, of course, the sequence bias isn't THAT strong, at least for MLV).
A. They didn't look at MLV in the assay he was referring to.

I looked for the paper the questioner was referring to - something by Bushman? - but I don't see one where they looked as far out as he said.



PP62 - Serghei Mangul - Accurate viral population assembly from ultra-deep sequencing data

RNA viruses exist as a quasispecies. ~10^4 mutation rate, combined with recombination between different viral genomes, make them…tricky.

Want to assemble a large number of highly similar but distinct viral genomes. Specifically, want to assemble the population.

Of course, you have errors in the data, which makes it look like there are some viral genomes in there that are from seq errors, not actual defining mutations.

Needed for a good assembly:
* Read quality (minimal seq errors)
* Read length (long enough to resolve conserved regions)
* Coverage (enough to cover the rare viral genomes)

Errors result in inaccurate assembly IF you do the normal protocol.

High-fidelity sequencing protocol:
Use barcodes. (Wu, Nicholas C. et al. High-throughput profiling of influenza A virus hemagglutinin gene at single-nucleotide resolution, http://www.ncbi.nlm.nih.gov/pubmed/24820965, or at least a modification thereof)
Amplify every fragment with the barcode, then group by barcodes. Now you know which things came from the same fragment, and you can throw out those that weren't amplified enough.

With that, then go to viral assembler (VGA).
Map reads to a consensus - though you don't care about the reference itself, so SNVs are only among the reads, not relative to the reference.

If two reads overlap, but disagree on the overlap, they're probably from different genomes (remember, we're assuming the above protocol gives us error-free reads)

Conflict graph - reads connect to reads with which they are in conflict.
It's like the color the map problem - you want the colors to not touch itself. Colors here are independent sets of non-conflicting reads. Reads of the same color end up not being in conflict.
You want MINIMUM coloring, though, which is NP-complete. Use heuristics, partitioning, to deal with this.

If you get non-continuous coloring, you give it a second chance. If it still doesn't work, just say it's not enough coverage, and you throw it out.

Expectation maximization, not just mapping, is used to separate the genomes from each other.

What about recombination? There will be common regions between these genomes, after all. If it's longer than the fragment length, that's a problem. They use expression profiles to try to get around this.

genetics.cs.ucla.edu/vga

scales to millions of PE reads
Able to discover ultra-rare viral genomes
suitable for clinical applications



WK05 Part D - Michael Love - RNA-Seq workflows in Bioconductor

As before, these slides will be available. They're at http://mikelove.github.io/biocrnaseq/.

Preparing gene models - can load from GTF files, annotationhub will be able to do this soon, etc.

Super useful - to convert from chrX to X and back: seqlevelsStyle(gr) <- "NCBI" (or "UCSC"). It's in genomeinfodb.

Extracting exons - list structure, every element in the list is a g-range. We'll use these exons to count reads to do differential expression.

yieldSize helps control memory, to tell it how many reads to read at a time

Saving as an SE - a summarized experiment.
First row of col data is first column of assay data (I think)

Exploratory data analysis (EDA) - see preprint for deseq2. A logarithm that's better for stabilizing variants.
The image is an example PCA plot

The first five packages on his list use a GLM to analyze the counts. For each gene and each sample, counts K are distributed as a negative binomial. 2 parameters, mean and dispersion. Separate out the size factor, which is the sequencing depth. You don't care about depth differences, only differences based on condition.
Instead of specifying the size factor, can also just specify the matrix.

Differential expression analysis - wrapper that simplifies things. 

Plot counts - not in the release yet, but soon. The example is pulling out the gene with the lowest P-value and looking at the counts.

Controlling for unknown batch: give counts and condition of interest, and it does a PCA after removing said condition to find other axes on which the counts vary, which could then the batch effects. Returns a matrix.

Reporting tools - really nice software for visualization. Can make boxplots, etc. From Genentech.

browseVignettes("pkg") will give the documentation for the version you're USING, not whatever the current release it (if you're not on the cutting edge)

It's worthwhile to contribute to Bioconductor - they go through your code and improve it.



WK06 Part A - Ciaran O'Neill - Peering into review: innovation, credit & reproducibility

Works at BioMed central

Speaking about open peer-review. Reveals the names of reviewers visible to authors, and to some extent, to reads (along with the reviews themselves)

Biology Direct - Authors can choose reviewers, and ALSO make the decision about going to publication or not - so the editor is effectively removed.
Reviewer pool is editorial board - so since they're primed, it doesn't take long to find reviewers.

PeerJ - reviews are each given a DOI, so you can get credit for doing said reviews.

F1000Research is the one doing post-publication review: publish, THEN review, and then only AFTER that is it sent to pubmed.

(speaker change - Amye Kenall)

How to combat irrepoducibility from the journal side?

Dynamic documents - like knitr and IPython - can be used to make your process more accessible for those trying to reproduce your work

Giga science is journal + database + computational tools

Example: authors submitted in knitr, which allowed reviewers to rerun analysis; also updates things automatically

Why stop at publication? If you could launch the cloud instance from the journal, that could help keep it relevant, and bring discussion back to the journal (which is nice to keep centralized)



WK06 Part B - Alejandra González-Beltrán - What was the plan? A role for data standards models and computational workflows in scholarly data publishing

Metadata - often created at the end of the workflow. They work to create it at the beginning. Intended to help reproducibility.

Experimental plan - life sciences case: need to describe all methodologies used.

For computational case, worked with SOAPdenovo2.
3 x 3 design - 3 algorithms, 3 genomes of different sizes.
Response variables is genome coverage, memory consumption & computational runtime.

In both cases, they've used the isa-infrastructure to describe experimental design. Based on isa-tools.org, isatab, and isacommons.org.

isacommons.org - the community, members of which use the other isa tools to track metadata

isatab - tabular representation of the experimental workflow.

isatools.org - curate, store, analyze, share and publish your bioscience experiment. 

isa-tools.github.io/soapdenovo2

Nanopub - represent in a structured way the main output of the article
ResearchObject - holds the whole workflow, all necessarily to re-run the results.

Nano publications of the three response variables mentioned above. It looks like the nanopubs are labels on edges of the graph of the workflow, but I could be mistaken about that.

This approach - using isatab, for example - can start from the experimental design stage, and then you can build on that with things generated from a galaxy workflow, and label things with nanopublications. 

github.com/ISA-tools - repo
blog: isatools.wordpress.com
@isatools - twitter



PP79 - Peter Robinson - Improved exome prioritization of disease genes through cross-species phenotype comparison

(missed beginning)

Comparing human and mouse - mutations in orthologous genes are often associated with similar phenotypes.
Can use that to prioritize candidate genes (also looking at zebrafish and mouse)

Many ways to compare disease models:
a. inferred by orthology asserted models
b. phenolog mapping
c. direct phenotype mapping with ontologies - what he's talking about today

Human Phenotype Ontology - terms are subclasses of higher terms. about 110,000 annotations for about 7000 mendelian, mostly monogenic, diseases.

http://www.ncbi.nlm.nih.gov/pubmed/24217912

similarity score - For all terms annotated in disease 1, find those closest in disease two and sum them. Do it from both sides, as it's not symmetric. Similarity visualized as clusters.

Semantic web of the human phenome - using an OWL class definition
Gkoutos GV et al 2009
(and other papers, but then he left the slide)

http://www.ncbi.nlm.nih.gov/pubmed/24358873.1 - reasoning over phenotypes.

Interspecies mapping - http://www.ncbi.nlm.nih.gov/pubmed/22293552. Groups homologous anatomical structures. 

Phenodigm - now we have a means of calculating similarity between phenotypic concepts based on OWLSim
http://www.ncbi.nlm.nih.gov/pubmed/23660285

Exomizer uses PhenoDigm semantic analysis to improve candidate gene prioritization in the setting of WES.
You upload a list of HPO terms, filter by allele freq, how to deal with pathogenicity, etc. Can filter on pedigrees, if you have those.
Click submit, VCF is uploaded. Program annotates variants to transcripts, remove things that are common or off-target, then do variant scores based on predicted pathogenicity (from polyphen, mutation taster and sift). Then take those and do phenotypic profile vs. 30,000 mouse models.
Those that have a good phenotypic match AND a good variant match get a good overall score.
Get back a ranked list of candidates.

Pheno + variant > either alone

Collaborates with the UDP - yeah, NIH UDP.

Monarch Initiative - Multiple organism, integration of many phenotypes



WK06 Part D - Kaitlin Thaney (@kaythaney) - Software Review audience participation

(Started before I came in)

One guy saying he has to talk to his university's tech transfer office before he's allowed to publish on GitHub

How many have a code review system in place in your lab? Five hands go up. 

Otherwise, most of the discussion revolved around being unhappy with the current system, such as lack of credit for the data analysis people, and roadblocks to releasing things as open-source.



KN6 - Russ B. Altman (@Rbaltman) - Informatics for Understanding Drug Response at All Scales

www.pharmgkb.org
(Disclosure: co-founder, Personalis Inc.)

Focus of his work is drug action at multiple scales - molecular, cellular, and organism. Nice thing about informatics is that you're not tied to scale.
Toolkit (also at many levels): Population, literature, cellular, molecular.

PharmGkb is to understand phenotypes and molecular stuff with drug response
Has nice pathways, available for download. Can use in publications if you want.

PMID 20435227 - there are 10s to 100s of drugs per person for which human genetic variation can affect response, and thus influence what should be prescribed.

FDA Adverse Events reports - can also be used to find new effects of drugs (or new interactions between drugs)

Synthetic associations confound things. If drug X causes side effect A, and drug Y is usually used with X, Y LOOKS like it causes A, but doesn't.
Even worse, if disease B has side effect C, drug X used to treat B, it LOOKS like X appears to cause side effect C (like diabetes being associated with high glucose - that's not the fault of the drugs)

Removing those associations: Need to pick controls with similar expected correlation structure (similar disease background, similar drug background). FDA database has millions of entries, so doing so is possible.
Matched controls are MUCH better than random. Takes care of age effects (like drugs mostly used in the old, or in the young), and several other things, like sex of the patient.

Two databases from this:
OFFSIDES: off-label side effects. 438 THOUSAND new side effects.
TWOSIDES: drug-drug interactions. Subset validated in electronic medical records.
www.pharmgkb.org/downloads/

Currently no ranking of side-effects, so they're working on that. Gottlieb in his lab using amazon's Mechanical Turk (crowdsourced) to decide which side effect is worse. Also spike in which side effect is worse from a KNOWN set, so you can tell if they're not doing a good job.

How do you get a rank order from pairwise comparisons? Cool algorithm, but he's not going to talk about it today. Uses linear programming, though.

Mild and severe effects were consistently ranked as such.

This project cost only $6000, not counting postdoc labor.


Literature mining

(Perecha et al., under review)

Want to dissect the RELATIONSHIP between gene and drug by looking at pubmed abstracts.
Used Stanford parser, from Manning et al. Creates a dependency graph with the verb as the root of the tree, and the subject and object, etc., descending from that. There's a path connecting the gene and node (though the direction of the edges point away from each other). The edges are all labeled with what type of relationship it is.

Turns out there's not an infinite way to express the relationship between gene and drug, and some way are more common than others. Reduces the search space.

Biclustering of gene/drug space and dependency paths. Clusters mean similar sentences for those across the literature.

He put up a list of about 25 gene-drug interaction types, which essentially constitutes all ways that genes and drugs can interact.

SO they took that and compared it to drugbank. Look within the same cluster to find other drugs that might affect the same gene, but currently aren't listed as such in drugbank.
Can ALSO look for finding completely new effects that aren't within the same cluster for those with potentially new effects.


Effect of drugs on gene expression

Looking at "first responders". Drugs go to the liver first.

DrugMatrix database measures rat liver expression in response to drugs

Trying to predict genes based only on the small molecules that induce them.

Top performing gene classifiers predict PK and other liver proteins


Knowledge-based ID of fragments for drug design
Tang & Altman, PloS comp bio 2014 - http://www.ncbi.nlm.nih.gov/pubmed/24762971

Doesn't use physics.

FragFEATURE - given a query protein with a pocket, represent portions of the pockets as a set of balloons. Each balloon is a microenvironment that is associated with the types of fragments it like to be near.
Use stats to find association of protein neighborhoods with various features.

Each balloon is a vector of 400 numbers summarizing that space.

For every co-crystal in the PDB, they write down all the fragments each environment is next to.

Then, take a new protein, identify pockets, make feature vectors. For each feature vector, look up five nearest neighbors. Take hypergeometric P value (so you don't always predict the common fragments), and also do it from the features near THAT. They each get a vote as to what that piece of the pocket wants to be near.

Validation of performance with various pockets, compounds. Potential to find compounds that bind VERY tightly - if they exist.


Conclusions:
Informatics good for:
* Inferring and triage of side effects
* Gene-drug interaction characterization via text
* Predicting expression from chemical structure
* 3D structure to predict fragment binding


