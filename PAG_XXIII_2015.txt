1/10/15

W786 - C. Titus Brown - How Well Can We Reconstruct Metagenome Content? Thoughts and Data on Metagenome Assembly

Reconstructing metagenomes from shotgun data.

Not talking about 16S. Shotgun is collecting samples, extracting DNA, sequence fragments,  and then you analyze it. "Sequence it all and let the bioinformaticians sort it out." That is, don't try to fiddle too much with the molecular biology - much can be sorted computationally.

To assemble or not? If the goal is to reconstruct phylogenetic content and predict functional potential of ensemble, you can either analyze short reads directly, OR you can assemble and THEN analyze (with weighted contains). So which to do?

Assembly is a bad bioinformatics technique - uses a lot of heuristics, bad software.

But - it can be useful. For stuff like this, you want to assemble. The length of the alignment means that you're passing a better single to blast for homology searches. Specificity is much better than just short reads.

Few assembly benchmarks have been done. 
We usually take unknown data and run it with new software & publish on it - which is a terrible way to do software.

Shakya et al 2013 - built a mock community so they KNEW what it was, then did real sequencing of the mock community. 101 bp PE reads, as well as other comparison sequencing.
What they found:
* Metagenome seq outperformed rRNA primer sets
* Illumina is good if you already know what you're looking for and you're just counting
* 454 with MG-RAST doesn't work very well at all with default parameters. So don't do that. Can change the perimeters.

What about assembly? His lab has been making a tutorial (since otherwise, there aren't really standards). Kalamazoo Metagenome Assembly protocol.

They used that to benchmark that mock community dataset. Tried three different assemblers (Velvet, IDBA, SPAdes). Also applied three different filterings. Since they have a known answer, they can compare with Quast.
Note - if something works on mock communities, it might not work on real ones (and probably won't).

IDBA and SPAdes outperform Velvet for genome fraction recovered (90-91%). Those two are pretty similar overall.

Computational cost - Quality takes a lot of RAM, which is why they do Diginorm and Partition.

Project is still in progress.
What is NOT being assembled, and why? Low coverage? Strain variation? Something else?
The mock community also gets 10 MB of sequence that isn't supposed to be there - probably contamination.
MEGAHIT is a good assembler.

90% recovery is pretty good - relatively few misassembles, too.

* You don't misassemble sequences (not systematically)
* You DO need big computers (for now)
* It's still technically tricky, but there's hope.

Everything they did here is reproducible and open. Search for "khmer protocols"

During questions, again stressed the importance of having better known standards, so we can actually trust our protocols.



W038 - Alexandre R. Caetano - De Novo Genome Assembly of the South American Freshwater Fish Tambaqui (Colossoma macropomum)

(Talk started before I arrived, so I don't have the beginning)

First round of assembly - one lane with shotgun and one lane with mate-pair
Partial assemblies using ~25% of each MP library size class and all 400+800 shotgun data available (if you throw out a lot of mate pair entires, the N50 drops)
That's only the case for the 10 kbp library - it's better than 15 kb, and much better than 5 kb.

Second round - additional reads. One lane with 400 bp SG, one with 800, and one lane with mate pair.
# of N's in the assembly was initially 33% of reads, but after gap filling, it dropped to 10%.

Quality check:
Detection of "universal" genes using CEGMA (run prior to gap filling)
85% of CEGMA core genes found complete
9% found partially
So pretty good.

Automated annotation: used MAKER2. Used the Mexican tetra to serve as the source of ESTs, since they don't have them for the actual species they're sequencing.

MAKER2 found 23,632 gene models found, ~20,000 shared with the tetra.

Variant calling - After filtering for Q20, have 3.6 million high-quality SNPs, and I think about 300,000 indels.

Future:
Data from other assemblers
RNA-seq to improve gene prediction
Detection of markers within and between related species

Used Nextera, so it doesn't go through PCR.



W788 - Mick Watson - The Microbial Proteome Associated with High Methane Emissions in Cattle Determined By Shotgun Metagenomic Sequencing

Food security - Everyone in the world having access to the food they need to survive. We don't do this well - despite all the obese people, there are a LOT of hungry people. Lots of challenges coming down the pipe, too - population increase, changes in consumer demand, globalization, climate change, competition for resources (like water), and Westerners who want to eat "happy chickens".

Three gorges damn - just to show that humans change the environment. It shifted so much mass that the day is now 0.06 microseconds longer.

Similarly, humans have changed our livestock, even over just ~45 years. That's all just through breeding. See van der Steen, Prall and Plastow, 2005 J. Anim Sci.
We need to keep this up, since we need these improvements to feed our species.

He studies gut rumen metagenomics. Why?
* Energy from food - microbiome could perhaps be used to get MORE energy from the food by manipulating microbiome.
* Novel enzyme discovery - Scientifically, it's a novel environment
* Methane emissions - entirely microbial in origin. And the cows burp it out, not from the other end.

No relationship between methane emissions and metaGENOMIC abundance of methanogenic archaea, but there is with metaTRANSCRIPTOMIC abundance. That was in a GR paper, anyway. Mick's data doesn't support this. (The Shi et al. paper looked at sheep, though, and Mick looked at cattle)

Matched steers based on breed and diet, looked at methane emissions. Also separated high and low methane emitters.
High methane emitters DO correlate with high archaeal abundance.

Enzyme abundance - matched for breed and diet, the abundance of several enzymes is associated with methane production.

What's in there? Assembled all 8 metagenomes from the cattle using MetaVelvet, predicted genes with Prokka, annotated using Pfam domains. 1.5 million gene/protein predictions, less than half have any known domain (and that includes domains of unknown function). Even in 8 samples, thousands of novel enzymes to look at.

Figuring out what all those enzymes DO is hard - as biologists, we don't really have a good system for tackling this (at scale?).

Metagenomic assembly - Working on a cattle metagenomics dataset, tried various assemblers

Mick: "There's regular velvet, and there's version specifically for metagenomics."
Titus: "But you're the only person who uses it now."
Mick: "...Yeah."

But you can't do the whole dataset - as Titus warned, you'll run out of RAM. 

So they did a smaller dataset. When you map back to the assembly with BWA, IDBA-UD works reasonably well.

Need to use complete genes in these studies, which is tricky in metagenomics, as orthologs often look like repeats, so a LOT of contigs start or end in the middle of a gene.
IDBA-UD did well relative to the others, BUT it still mostly found fragmented genes

Metagenomic assembly is an unsolved problem, need lots of RAM, and assemblies fail in the middle of genes.
IDBA-UC looks to be the best assembler so far.



W789 - Timothy P.L. Smith - Characterization of the Bovine Nasopharyngeal Microbiome in the Context of Respiratory Disease and Treatment

Dealing with Bovine Respiratory Disease Complex (BRDC) - 25-30% morbidity after feed lot.

Multi-factorial, stress-related disease. Mortality associated with bacterial pleurpneumonia.

Control measures - vaccination against the disease, vaccination against specific pathogens. Limited protective ability, though, and antibiotics not always effective.

First experiment - stress the cattle so they'll get sick (since they need to study sick cattle). Mixing and highway transport are big sterssors, make them "high risk" cattle.

Some were metaphylactically injected, and the rest weren't. Three sets of these treatments, from cattle from separate sources.

Studies nasophyryngial swabs, rectal samples, blood phenotypes (cytokines and immune repertoires)

Profiling the microbial community with 16S. The problem, though, is that most of the DNA you get is cow, not microbial. Bustamante's selection technique gets rid of too much DNA, and it costs them too much to just sequence everything, so they're punting on that and just doing 16S.

Can now also do full-length sequencing of 16S with PacBio.

Analysis: WebMGA vs Greengenes give somewhat different answers. Lots of mycoplasma, though.

Substantial animal-to-animal variation, even within the same sale barn.

All animals that received metaphylactic macrolide treatment had increased mycoplasma (N=7)

Summary:
* Full-length 16S does achieve species-level discrimination
* Nasopharynx is relatively low-diversity compared to, say, rumen.
* (And there were more, but the slide changed)



###################################



Plant Genome Engineering

(Note: this session had a small, low screen in a crowded room, so I was only able to see about the top half of each slide.)



W597 - Dan Voytas & Bob ??? - Strategies for the Targeted Modification of Plant Genomes

Research dedicated to developing SITE-DIRECTED DNA sequence modification metrologies and applications. Modifications often stimulated by DSBs.

Desired outcomes: mutagenesis, gene replacement or edition, targeted gene insertion (like from the transgene world, creating molecular stacks), targeted structural changes.

Example: targeting soybean Dcl1a and 1b with ZFNs (dicer family). Used one ZFN with two paralogous targets, and recovered two T0 plants with multiple mutations. Able to "stack" double mutants.

CRISPRs deleted a 76.7 kb region in Medicago. That's 4 CRISPRs, not just one.

Dan Voytas -

He's going to talk about using a "homologous template" to serve in repair, so you're able to put in your desired modification.

Using Agrobacterium to achieve gene targeting: the bacterial T-DNA goes from the bacterium to the plant nucleus, and has the machinery to make the cut. It ALSO contains, on the same construct, the modified template you'll use for HR repair.

Harnessing plant DNA viruses (geminiviruses) to get efficient targeting. Similar to before, but now the tDNA has the viral replication machinery. That lets it circularize in the plant nuclease, AND replicate so you have more copies to work with.

Replicons enhance gene targeting frequencies 10 to 100-fold. Partially due to increase in donor molecules, AND by pushing the host cell into S-phase, making HR more likely.

Introduced a promoter to drive Ant1. Since this is plants, you can take the part that was modified (explants) and grow it back into a whole plant. 

Questions: larger donor plasmids have less efficient rolling-circle replication, so keep that in mind.

Have they changed GC content of the replicons (lowering it) do geminivirus has an easier time b/c it'll be less methylated? They haven't done that yet.



W598 - Caixia Gao - Developing Genome Editing Technologies for Crop Improvement

Working in rice and wheat. 

SSNs enable efficient genome engineering. Their research focuses on ZFN and CRISPR/Cas9.

4-5 months from TALEN or CRISPR design to produce knock-out mutants.

Showed example of a ~1.3 kb deletion from TALEN

CRISPR: Rice uses OsU3 promoter, wheat uses TaU6 promoter

Homology-directed repair (HDR) in rice: Just inserting some restriction sites, but it works.

Creation of fragrant rice by genome editing. Done by making deletions in Badh2. TALEN-induced mutations in OsBADH2 gene, which were transmitted to T1 and T2 generations. Making these deletions increase 2AP content, the fragrant compound.

Interestingly, new indel mutations were detected in subsequent generations. Looks like 1 bp deletions were expanding in T1 and T2.

They also used CRISPR for OsBADH2, and found similar results.

Powdery mildew - a very destructive disease in wheat. They combated this by targeting three TaML homoeoalleles with TALENs. Knockout of all siz alleles of TaMLO in wheat confers resistance to powdery mildew. (She later noted that this does run into regulatory challenges in the US)

NHEJ-mediated GFP insertion into wheat protoplasts.



W599 - Puchta Holger - The CRISPR/Cas System can be used as Nuclease for in planta Gene Targeting and as Paired Nickases for Directed Mutagenesis in Arabidopsis Resulting in Heritable Progeny

Nuclease-induced genome engineering

Background: years ago we were using I-SceI to introduce DSBs - at least to show what happens when you make that break. You could put T-DNA in the break, and it needn't be homologous for it to integrate (why sometimes you get the CRISPR transgenes integrating therein). Lower frequency than homologous, of course.

In planta gene targeting technique - gene targeting that takes place during the life cycle of the plant. Idea is to do the targeting in all cells, and then going to the seeds to get what you want. The donor is integrated into the donor via transformation. The way they do this still involves I-SceI in his example - your donor has the same homology arms as whatever flanks your I-SceI site of interest, and that's how you get HR.

The idea now is to do the same sort of thing with targeted nucleases (like the CRISPR system). Turns out it works efficiently. Verified with Southern blots.

Used the agrobacterium approach on ADH1, TT4, etc. They work.

Germline transmission in their hands is 15-20%.

Off-target effects: To deal with this, they converted CRISPR/Cas into a nickase. Did that by knocking out one of the nuclease motifs, so it only cuts on one side. Make two nicks and you get the DSB you needed, but with twice the specificity.
(I am surprised that this solution also works in plants, with with multiple genome duplications, etc.)

Mentioned at the end - we now have the tools to make a synthetic plant genome.



W600 - Feng Zhang - Highly Efficient Genome Editing in Crops

From Celectis. Idea is to create better food products through genome editing.

Efficient delivery methods are key to success. There are a couple way to do it: agrobacterium, particle bombardment, PEG or electroporation, and RNA or DNA viruses.

Protoplast transformation: what people did before agrobacterium.

Targeted gene editing in N. benthamiana. They do TALEN transformation, regeneration, and mutant screening. No need to select the plants on selective media because the transformation efficiency is very high.

Multiplex gene knockout in one generation: Trying to knock out four genes at once; 17% of screened plants did.
Results in a significant reduction of N-glycosilation in mutant plants.

Potato: trying protoplast transformation in commercial varieties. 
Cold-induced sweetening reduces potato storage quality, so they're trying to tackle that.
TALENs to knock out the gene responsible for the sweetening. 
Transgene-free mutant plants were identified, and did have reduced reducing sugar (not a typo) and acrylamide levels.
This substantially reduced the darkening of the color after CIS.



W601 - Sandeep Kumar - A Modular Gene Targeting System for Sequential Transgene Stacking in Plants

SmartStax has 8 transgenes stacked into one product. Crops with multiple traits. Typically, the transgenes reside on different chromosomes, so it's tricky to put them together and KEEP them together. 

The solution is the molecular stack since transgenes can be stacked into a single locus.

They're heavily invested in ZFN, so that's what they're using.

They use a modular and sequential gene targeting and stacking strategy. Involves switching back and forth between selectable markers every other generation. Can keep using the same ZFN to do this.

Particle bombardment used to insert donor and ZFN. Done on a T1 homo target (T0 is selfed to get homo). Looks like about 30% mutational efficiency overall.

Targeting confirmed via Southern.



###################################



C01 - Gary Saunders - The European Variation Archive at EMBL-EBI

Many ways to GET data (generate it, get from a paper, the web, colleagues), BUT - it’s not accessioned, no metadata, and no links to core data.

That’s where Eva comes in. Centra repository for VCFs. Any variation is represented as a change against a reference sequence.

www.ebi.ac.uk/eva

Study browser - can filter by free text search, checkboxes for organism, or experiment type. 
There’s only one curation study - maize - and that’s where they picked what parts of the genome to analyze.

Clicking the left box on an entry expands the metadata.

Going to the study view has an FTP link at the bottom.

EVA variant browser - select organism/assembly, and can put the region in which you’re interested in looking.
SNP IDs are from dbSNP. The “view” column links out to various external ways to look at it, like ensembl and dbSPN.

There's a way that parses the VCF genotype info to make it a little more human-readable.

Can submit in any file format, though VCF is preferred. It's all described in their online documentation. 
You can contact them via email to get their various submission templates, etc.
Data submitted to EVA is also brokered to NCBI, so no need to submit again. Once NCBI accessions it, it's sent to Ensembl.



C02 - Nathan A. Dunn (LBNL) - WebApollo 2.0: A Scalable, Distributed Sequence Annotation Editor

Designed for collaborations across large groups

His example: log in, search "Group1.3", which takes you into the annotation editor. Can add annotations locally or via URL.

Can use BLAT to search the whole genome OR just the scaffold you're currently on.

Check boxes on the right can be used to add various other pre-loaded tracks.

Drag any feature up to the yellow area to see the annotation that you can start modifying. When you modify something, you're not REALLY changing the reference sequence, you're adding metadata to show that something has been added or removed.

If you make a change, the ORF is recalculated.

Option-click opens the information editor.

Can add in RNAseq data, and I believe dragging exon boundaries will snap to those supported by RNA-seq.

New UI adds sidebar to help with sharing to specific individuals, among other things.

He'll be at P1154.f



C03 - Jodi L. Humann - GenSAS: A Web-Based Platform for Automated and Manual Curation of Genomic Sequence

Wrapped a bunch of command-line tools into a GUI. It's for non-bioinformaticians. Integrates with JBrowse and WebApollo for visualization and editing.

genesas.bioinfo.wsu.edu

Log in, click "use GenSAS" tab. All steps therein have the directions/instructions in a tab. This is in lieu of external documentation.

All projects have a 120 expiration date. You can always reset the expiration, but if you don't, it'll disappear to free up space.

Uploads must all be in FASTA format. If you put all the sequences in a multi fasta file, then ALL those sequences will have to get the same analysis.

Can also upload GFF3 files, apparently.

After upload, you filter repeats. If you go back and give it a different name, you can run it with different settings and pick the one you want.

After that is masking, then gene prediction (Transcript alignment is via BLAST or BLAT, I believe).

Don't need to stay logged in after your jobs are submitted.

Consensus: Pick the gene finding jobs you want to use, and assign them a weight.

"Refine" and "Functional" aren't available yet, but will be in the future.

Annotate

Publish - GeneSAS exports in GFF3 and FASTA formats (sequence of the features, not the genomic sequence).



C04 - Kim D. Pruitt - Sequence Viewer and Genome Workbench - Sequence Display and Analysis Tools

Sequence Viewer is web-based, Genome Workbench is what you download.
sviewer designed to look at one accession number at a time. Also exports high-quality PDFs.

sviewer - basically a genome browser. For example, when you go to NCBI Gene, sviewer is what appears on that page.

Can display custom data, including BLAST RIDs.

If you select two annotation objects, the system with automatically highlight regions of difference. Good way to compare, say, ensembl and ncbi.

gbench - learning curve, but does have documentation and videos. It's used by RefSeq curators and the GRC.

GCF - refseq; GCA - genbank



C05 - Emily Perry - Analysing SNP Consequences on Genes and Proteins with Ensembl's Variant Effect Predictor

www.ebi.ac.uk/~emily/Workshiops/2015/PAG - everything from this talk, including the demo files.

SIFT scores - based on protein conservation and the chemical change.

Two ways to use the VEP: web interface and standalone perl script.

If they DON'T have a species in Ensembl, you can make a cache.

Her example vcf doesn't have a header, but should still work.

Web-based: click on the VEP link on the main Ensembl page, then Launch Vep. If you have a bunch of variants, probably better to upload file or provide URL, rather than paste.

Output: Summarizes output.
Can then filter by, for example "consequence" and "missense".
Can download the data, or send them to BioMart and get them that way.

Perl script version: Just follow the instructions. Pretty much just a matter of installing and then specifying your vcf on the command line. You can also download a cache of your genome of interest.

You get a local vcf output, and an html file that should give you the summary images.



C06 - Hugues Roest Crollius - Genomicus: Fast and Intuitive Comparative Genomics in Plant and Animal Genome

239 species in their database.

Search for a gene, and the default view pops up, called PhyloView.
Click arrow in upper left to select which genomes it's compared to.

Can change color scheme to view protein similarity by clicking near the top of the screen. The similarity scores come from ensembl.

View -> AlignView lets you view alignments of the other genomes, such that if there isn't a homologous gene in the same locus, a gap appears.

There's also a view (karyotype?) to see the orthologous chromosome segments between two genomes - just mouse over.



C07 - Justin T. Page - BamBam: Genome Sequence Analysis Tools for Biologists

sourceforge.net/projects/bambam. Recently published in BMC Research Notes.

For manipulating and summarizing BAM files. Only going over some of the tools today.

Bam2Consensus: returns a consensus sequence for every contig in the reference. If you give an annotation via a BED file, you can pull just those regions.

Counter - Breaks it down by BED region, counts up how many things were there.

InterSNP - straightforward SNP caller. Just looks at a single basepair and counts up how many A, C, G, and T in one BAM vs. another BAM. Default output is a genotype table.

SubBam - Takes a subset of a sorted BAM file. SAMtools does something similar. Lets you make a smaller BAM if you're not interested in most of what's covered.

HapHunt - uses k-means clustering. Needs continuous coverage for it to work. 

MetHead - summarizes bisulfite sequencing data



###################################


	
Next Generation Genome Annotation and Analysis

W522 - Shin-Han Shiu - Nuances in Plant Genome Annotation: Those Duplicated and Dead Genes

Recent whole genome triplication of Brassica rapa hasn't resulted as three times as many genes, since the rate of pseudogenes goes way, way up.

But are pesudogenes really "dead"? They're expressed in certain conditions, and when they ARE, they're expressed around the level of canonical genes. About 20% are transcribed.

Some likely are translated - about 20%, depending on the dataset. They checked via ribosome pull-down.

Some of the pseudogenes are under surprisingly high evolutionary constraint, and evovle as slow as housekeeping genes.

Comparing the 5' and 3' parts of pseudogenes, before and after the first stop/frameshift. 5' is under slightly stronger constraint than 3' meaning it's maintaining a bit of function.

Though duplicates tend to die, quite a few duplicates remain. That's after 25 million years, which is quite a long time, even evolutionarily.

How did they stick around? Might be sub-functionalization.

What they see if asymmetric partition of function. It's not half and half, but one-or-nothing. 

At the expression level, they found that one of the duplicated genes would be the same as the ancestor, and the other is on but lower. One seems to be decaying.

For example, DDF2 has lost function over time. Doesn't bind as well anymore, either.

Of duplicates that are still around, one copy seems much like the ancestor, and the other tends to decay (rather than becomes neo-functionalized).



W523 - Joshua Puzey - Transposon Density Affects Homeolog Specific Gene Expression in Allopolyploid Monkeyflower (Mimulus)

Adaptation to polyploidy. It CAN have a lot of benefits, but polyploidy doesn't happen that often - many rapidly go extinct.

Major problems:
* Chromosome pairing at meiosis
* Control of gene expression - especially during hybridization, where things from genome A may try to regulate things from genome B.
* Competition with diploid progenitors

Autopolyploidization - within a given species
Allopolyploidization - two species make a hybrid

He's focusing on gene expression.

Number of proximal TEs inversely correlated with gene expression in arabadopsis. Another paper found that when genes had methylated TEs upstream, it's not expressed, but if they're not methylated you're fine.

Homeologs - sister genes from different parental genomes.

He predicted that expression differences in homeologs may be explained by local TE density.
Also, this expression bias should be less in early generations than later.

M. guttatus has undergone multiple polyploidizations, some of which have happened multiple times independently in the field.

Mimulus peregrinus - a hybrid that formed in the UK, as botanists brought it two other plants back from California and Chile.

Gene expression in M. peregrinus - called SNPs to give confident mappings, since they needed to know which parental genome a given transcript came from. Demanded 100% identity.

But first - looking at how TEs affect expression in the parental species. Again, local transposon load is negatively correlated with expression, and that hold for both parents.

Homeologs with more TE bases UPSTREAM are less expressed than the other homeolog.

The skew of the Homeolog Expression Bias (HEB) toward the luteus homeolog increases with more generations. The bias comes from hybridization, not the initial duplication.



W524 - Daniel D. Ence - Transposable Element Islands Facilitate Adaptation to Novel Environments in an Invasive Species

He's from Yandell's lab.

TEs can be effected by effective population size.
Looking at Cardiocondyla obscurior, and invasive ant species. 

The smaller the effective population size, the more impact of genetic drift relative to natural selection. Any new TEs that arise in the "new" population can go to fixation much faster than they otherwise would in the previous, larger population.

C. obscurior - originated in SE asia, and has spread throughout the globe (though looks to be mostly warm climes). 

Made a reference genome with 454 and Illumina, annotated with RNAseq from all castes and developmental stages, and filtered for prokaryotic scaffolds (important, since it has symbiotic bacteria).
It's the smallest genome assembly of any ant.

Repeat annotation - C. obscurior has about the same proportion of TEs as other ants.

TEs are organized into TE islands. 

TEs in this species are organized differently than in other ants. It's more similar to Nasonia (a wasp) than other ants (that's another inbreeding species). 

TE island genes are overexpressed in adult queens compared to queen larvae.

Brazilian vs. Japanese colonies - Japanese larger, same number of workers, more queens. 
B aggressive to B, J, and external ant; J is only aggressive to external ant. Due to different cuticular compounds.

How do they differ genomically? 

TE island regions are enriched for SNVs. Since the individuals were pooled HAPLOID genomes, that means they're polymorphic at those loci. Since odor receptor genes are there, that affects their behavior.

Other points:
Genes important for development are excluded from TE islands.
TE islands preferentially expressed in adult queens



W525 - Ingo Braasch - The Spotted Gar Provides Connectivity Among Vertebrate Genomes and Insights into Evolution By Genome Duplication in Fish

Begins by reminding us of teleost genome duplication (after the split from the branch leading to tetrapods, so we don't have this). This complicates assignments of orthology between, say, zebrafish and humans.
Coelacanth is a lobefin fish, and doesn't have that teleost gene duplication (since it's from after the split, on the same branch as tetrapods).
Tetrapods have lost over 50 genes with known function in zebrafish.
Coelacanth is highly endangered, though, so can't do much work on them.

Duplicate retention and loss after the TGD (teleost genome duplication event): Majority of orthologs lost right after the TGD.

So, we need something on the rayfin branch from BEFORE the TGD. Hence, spotted gar. There are other species that could have been used, but spotted gar has the fewest problems.

GARNOMICS: genomic tools for the spotted gar:
* Fosmid/BAC libraries
* RNA-seq
* miRNA-seq
* High density genetic map (RAD-tags) - shows that it has an unduplicated genome relative to the human genome.

Broad Institute made an ALLPATHS-LG assembly.

Genome is available on ensembl. Repetitive elements only 20% (Zebrafish is 51%, medaka is 16%)
Probably around 21,000 genes.

Reconstructing ancestral vertebrate genome: use synteny database to compare species. You can see rearrangements between fish and humans (though not so much between the gar and chickens - so humans are the weird ones, here).
Further analysis shoes there is a history of microchromosomes in the vertebrate genome.

Resolving unclear gene family relationships: Like Prrx genes. Tetrapods have Prrx1 and 2, zebrafish have prrx1a and 1b. Prxx2 is in coelocanth - AND in the gar. Predates the divergence and was lost by teleosts, probably before the TGD.
Well, not QUITE - it's present in the basal teleosts. One copy lost right away, and then the other copy lost at a later split.

Connecting vertebrate cis-regulatory elements: LastZ -> pairwise -> phastCons to identify conserved noncoding elements. Can find CNEs specific to teleosts this way, for example.
Many CNEs appear to be lineage-specific - can't necessarily align CNEs between fish and humans, for example.
You can't align the things from human and pufferfish, but if you align both to the gar, THEN you can see that things line up.
The evolutionary rate in teleosts is very high, but gar has a much slower rate of evolution - so it's closer to humans than zebrafish.
Because of that, you can align zebrafish to gar and human to gar, and THEN you can see 24,926 CNEs that you wouldn't see otherwise - about 40% of which are orthologs.

How did gene expression evolve after the TGD? 
gpr22 is in brain and heart in gar. In teleosts, one ortholog is in brain, and the other ortholog is in heart. Sub-functionalization.
Neo-functionalization: slc1a3 is in brain in gar. Teleosts have it in brain AND liver.



###################################



1/11/15

W243 - Beat Keller - Lessons Learnt from Transgenic Gene Pyramiding for Classical Resistance Breeding in Wheat

Standing-room only, so I wasn't able to type up notes.



W245 - Jay Shockey - Sanding the Rough Edges: In Search of Enhanced Performance of Engineered Enzymes in Transgenic Systems

Novel oil plants - the oil-producing plants we’ve bred are for eating, not for industrial use. For that, we turn to novel oil-producing plants.

The downside is that those plants have poor agronomic traits, so that’s where they come in.

Putting transgenes from the exotic plant into target plants like arabadopsis doesn’t work too well. Fatty acid production goes from about 80% in exotic plant to around 20% in the target, which is unacceptably low.

Their goal is to find ALL the genes required for the biosynthesis of an oil with high amounts of an exotic fatty acid. This also involves promoter optimization, etc.

Castor oil - makes a very good lubricant that stands up to high temperatures, but you can’t cultivate the host plant at industrial scale because those plants produce ricin.

Diacylglycerol acyltransferase (DGAT) - catalyzes the committed step in TAG formation.

Castor bean DGAT2 strongly prefers dircinolein in vitro.

RcDGAT2 coexpression drives dramatic increases in seed ricinoleate levels.

Tung tree - Oil has conjugated double bonds, and contains trans double bonds. Unusual - and makes it easily oxidized.

FADX involved in biosynthesis of tung oil.

Design of a flexible cloning and expression system - what he's been up to. Hopes to share with the community soon.
Also adapted for CRISPR/Cas9.

Testing it - promoter optimization of tung FADX expression.

DGAT2 prefers eleostearate-containing substrates.

As in yeast, VfDGAT2 outperforms VfDGAT1 in planta.

Summary:
* DGAT, especially DGAT2, improves HFA and ESA production.
* Promoter choice suppression of competing processes, and the use of engineered enzymes all help.



W242 - Jian-Kang Zhu (not really, but someone presenting on his behalf) - Gene Editing in Plants using CRISPR/Cas and TALENs

Begins with explaining CRISPR, etc.

They use two sets of CRISPR vectors. One for arabadopsis, and one with a different promoter for rice.

Testing if CRISPR/Cas9 in Arabadopsis protplasts

Endogenous targets of CRISPR/Cas9 in Arabadopsis - testing three visible phenotypes. RFLP assay.

Successfully targeted two sites in the same gene simultaneously

Mutation is usally a single bp change - and drops off quickly in either direction.

Transmission:
T1: chimeric
T2: homozygous, bi-allelic or hetoerzygous
T3: homozygous or bi-allelic
They find that no new mutations or modifications arise as generations progress.

Did WGS to look for off-targets. No off-targets detected in T1 or T2. I believe he said there's a paper that contains this information (PNAS 2014? Might be http://www.ncbi.nlm.nih.gov/pubmed/24550464)
Ahhh, it's not quite what he said. They did WGS, yes, but they only looked for mutations in PREDICTED off-target sites. There's been other work published that has shown you can get off-target effects in sites you wouldn't predict based on sequence similarity.

CRISPR in rice: Similar RFLP results

Trying to knock out all PYLS in rice (and there are 13 of them).
Multiplex targeting or rice PYLS - one vector contains multiple sgRNAs.



W244 - Yinong Yang - Plant Genome Editing and Precision Breeding with CRISPR-Cas9 System

Nice figure indicating the 20mer is called the protospacer (which is why the PAM is called the protospacer-adjacent motif)

Made vectors for plant genome editing, some of which are already available through Addgene.

For off-target concern, made genome-wide prediction of specific gRNA spacers. Mol Plant 2014, http://www.ncbi.nlm.nih.gov/pubmed/24482433
That's the CRISPR-PLANT website.

Transgeneic rice lines with targeted mutation of OsMPK5. Got single base indels and frame-shifts, and you can cross out the transgene.

Developing advanced mulitples editin tools for functional geneomics and precision breeding. For example, expressing for more than one target for offset nicks, or just trying to hit multiple places with DSBs.

PTG technology - array gRNAs tandemly in a singe polycistronic gene and utilize endogenous tRNA processing system for precise cleaving and production of numerous gRNAs in vivo. Submitted to PNAS.
Didn't quite catch this, but sounds like connecting the guide RNAs to tRNAs.
Because it's a single transcript, you get identical expression of all the guides.

DNA fragment deletion at a frequency of 4-45%. Fragment deletion efficiency is negatively correlated to its size. (This is with multiplex targeting)

PTG:Cas9 has a higher mutational rate than sgRNA:Cas9.

For working in crops, you can do it by providing transient information, or using stable information and then crossing it out.

For example: regeneration of transgene-free potato plants from protoplasts after targeted mutagenesis of AS1.
Hopefully, he says, this sort of thing will alleviate concerns about genetically modified crops.



W246 - Vai Lor - Targeted Mutagenesis of the Tomato PROCERA Gene Using TALENs

Gibberellins (GAs) are phytohormones. Promote pant height, flowering, leaf size

You don't necessarily want TALLER plants - can lead to net yeild loss. Semi-dwarf GA mutants were therefore important during the "green revolution", increasing yeild.

GA relieves DELLA repression of the GA response. Combines with other proteins to ubiquitinate (and therefore degrade) DELLA.

The tomato DELLA gene is PROCERA (PRO), and the mutant is tall.
Two possibilities: Either the mutant isn't really a null, or there's a DELLA-independent function.

Designed TALENS to destroy a diagnostic restriction site to make mutant identification easier.

Seven pro alleles were recovered. proTALEN mutants are parthenocarpic (no seeds). They're male sterile.
So, if you cross WT male to proTALEN female, you do get fruit with seeds.

proTALEN mutants are not responsive to GA and Paclobutrazol (PAC). Suggests that proTALEN mutant is likely null.

GA signaling is fully activated in proTALEN mutants.



###################################



Statistical Genomics



W799 - Qishan Wang - An Empirical Bayes Method for Genome-Wide Association Studies

Most popular model is a linear mixed model. Combo fo fixed and random (random controls for the polygenic background). There are both exact methods (GEMMA, FaST) and approximate (EMMAX and P3D).

Two major issues:
1. Genomic background noise (marker density too high). Increase false positive.
2. Bonferroni correction is too conservative, leading to low power.

For example, if you had 2287 subjects with ~509,000 SNPs, you wouldn't be able to detect FGFR2 in breast cancer, despite actually being associated.

So they came up with Empirical Bayes (EB) method. Reduces noise and allows FGFR2 to show up as significant.

Their model has a different covariance matrix. Random marker effect approach.

Gamma k is the parameter of interest, and phi k squared is just prior variance.

Advantage 1: shrinkage nature.

Advantage 2: can calculate an "effective number of tests", and use THAT to perform Bonferroni correction to increase statistical power.
Note the effective number of tests only applies to the random model approach, not the fixed model approach. Testing in this way has a higher type-1 error (false positives) than using the usual Bonferroni correction.

Advantage 3: improving the computational speed



W800 - Lauren M. McIntyre - Using Path Analysis to Reveal Novel Components of the Sex Determination Hierarchy in Drosophila melanogaster

Example of retinitis pigmentosa - there are a LOT of SNPs in there that can cause it. But since they're all in the same gene with low frequency, they'd be really hard to find with GWAS.

Working on a "known" fly pathway - sex determination.

Building off the work of Sewall Wright. Supervised GRN Reconstruction.

Begin with baseline models, using two datasets (DSPR from King, and F1-Hybrid from Mackay).
DSPR shows no covarnaince, but F1-hybrids have some evidence of full or partial covariance.

Go through and add in all possible paths, asking if they improve model fit.
Found 12 putative new relationships.

Similarly, you can add new genes and see what happens.
Here, the datasets are VERY difference. 0 genes added to the DSPR model, but F1-Hybrids had 1,390 genes were added to the model.

Why? The number of alleles matters (allelic combinations). DSRP has fewer alleles, so the model becomes saturated sooner.

Of those 1,390 genes, they're enriched for sex-based splicing effects.

Now, corn.

Idea is to zoom to the relevant part of KEGG pathway, see what fits, and then see what else there is to improve the model.

Metabolomics - challenging and new. They have money for it that must be given to people outside of UFL.



W801 - Zhiwu Zhang - Getting Power Back from Population Structure and Kinship in Genome-Wide Association Studies

If you have linkage equilibrium, random mating will let you distinguish between the causative mutation and others that are just there not doing anything. 
If it's linkage disequalibrium - like if they're geographically isolated - then you can't necessarily do that.

Y = SNP + Q (or PCs) + e. Q or PCs captures population structure, and the other two terms is phenotype of individuals.

MLM for GWAS adds in kinship, to capture unequal relatedness. (Yu et al 2005, nature genetics)

GWAS does not have power for traits associated with structure.

PLINK is a GLM, but has 75% of the publications. Why don't human geneticists use MLM instead of PLINK? Well, in humans, you don't gain much power. You do in arabidopsis, though.

FarmCPU outperforms both GLM and MLM in terms of power, and that's for Arabidopsis AND human.

FarmCPU: P1158
BioGPU: P1163 & C28.



W802 - Deniz Akdemir - Locally Epistatic Genomic Relationship Matrices for Genomic Association and Prediction

Semi-parametric mixed model - SPMM

Multiple Kernel Models - use multiple kernels by combining them int a single one via a combination function.

Genetic data comes in natural hierarchies. At the bottom level, you have the SNPs, which you can block together. 

Breeding value is distinct from commercial value - commercial is what THAT generation does, and breeding is how much you can pass on. So if you have a genome-wide effect that can be lost due to recombination, that's not of much interest to breeders.

So, working more fine-grained, here. Locally epistatic values.

Only a small fraction of genomic regions are used in the final model and the importance scores for these regions are readily available as a model output.
Dimension reduction via feature extraction, block interactions.
Tests for regression coefficients, or hierarchical test variable
Robust for a few missing markers in a region.



W803 - Shizhong Xu - A Tutorial of Partial Least Squares Analysis

There are many methods for prediction, and some may work better in some populations than others. This is one such method.

PLS is a prediction of multiple response variable form multiple predictor (independent) variables.
It's a predictive model, not for variable selection. Don't use it for QT mapping or GWAS.

Related to:
1. Multiple linear regression
2. Principal component regression

You're trying to maximize the covariation of the scores of the Xs and the Ys

(Most of the rest of the tutorial is outside of my depth)

Determining the number of factors: k-fold cross-validation. That gives you a vector of observed and a vector of prediction, and then you calculate r squared between them.

He's used this method to predict hybrid yield and grain weight in rice.



###################################



Degraded DNA and Paleogenomics



W234 - Ian Barnes - How Do You Know When You Don't Have Any Ancient DNA?

Meridiungulates - extinct super-order. Variously identified as mono or polyphyletic.

Two end Pleistocene survivors. Toxodon plantensis - like a hippo-rhino.

Macrauchenia patachonica. Like a…camel-tapir.

Material tested for collagen preservation (since that sticks around more stably than DNA, I think he said)

NGS is nice for ancient DNA - no primer sequences, can work with small fragments, should be able to detect contamination and measure damage.
But it also has problems - not much good unless there’s a fairly close genome available. Also you tend to pick up a lot of non-endogenous soil bacteria. Also, it costs a lot.

Their BEST sample was still only soil biota. How do they know there’s no ancient DNA?
1) de-novo assembled into contigs, threw out the short ones, BLASTed to a local nucleotide database.
b) (didn’t catch this part)

But the samples they got were actually pretty far north, and in the southern hemisphere, (South America), that means you wouldn’t expect the DNA to last too long. Anything below 40-45N latitude is likely to have poor DNA recovery for the pleistocene (unless high altitude).

When is there not enough endogenous DNA to proceed? Depends on read length and proportion of endogenous DNA. 
Read length depends on burial environment, time, extraction method, library build
Endogenous DNA content is dependent on skeletal element, burial conditions, other things we don’t yet know.

Anyway, what they decided to do what look at the amino-acid sequence of type I collagen, and put that into a phylogenetic tree. Collagen is hard to sequence (repetitive), BUT it lasts about 10x as long as DNA.

The MS-MS data give a collagen phylogeny with high support for placement with Perissodactyla. Also, they're monophyletic. Share common ancestor with tapirs, horses.



W235 - Daniel G. Bradley - The Best Bones, Imputation and Ancient Genomics

Inspired by the Denisovian sequence - which was complete & came from a small finger bone - they ask what are the best bones for these studies?

They say is't the petrous bone. It's the bone that houses the inner ear, and is the hardest bone in the human body. Survives cremation. Human petrous is attached, sheep and goat are much easier to snap off.

Other bone elements get 10% endogenous reads if you're lucky. Petrous can be over 70% (though those data points are from more recent bones, but it still tends to out-perform other bones when older, too)

Imputation: Take phased reference haplotypes & your experimental partial haplotypes, and use that to call imputed haplotypes.
Might not work in aDNA, though, since ancient DNA can have different haplotypes.
Turns out they were able to bootstrap their data for imputation to get haplotypes out.



W236 - Jake Enk - Time, Temperature, and Tiling Density When Capturing Ancient DNA

R&D scientist at MYcroarray.

Enrichment through hybridization.
65 degrees C is sort of industry standard for modern DNAs, but there's also lower temperatures, touchdown, etc.
Lots of different times & tiling densities used, as well.

They simulated ancient DNAs (short fragment length - mode of 50 bp - which are shorter than the bait length of 80 bp). Embedded those in a soil bacteria background, such that the mix was about 6% endogenous.

Temperature: Bait length usually determines optimal temp, but since target is shorter here, it's the TARGET that dictates that. 
Hypothesis is that an increase in time will raise specificity and lower sensitivity.

For mito:
Specificity DOES go up - to a point. Looks like above 65 degrees it drops off (at 70 C)
Sensitivity looks to slightly peak around 50-55 degrees, then goes down. That's as measured by % of X raw reads that are unique sequences.
Optimal sensitivity looks to be at 55 degrees.

For nuclear:
Similar.

Time: Target aDNAs tend to be rare in the library. Hypothesis is that more time means more sensitivity (with caveats), and specificity MIGHT go down.

For mito:
Specificity not significantly different.
Sensitivity does go up with time, but not by much.
Probably want to avoid really short times, but anything beyond 16 hours is probably unnecessary.

Tiling density:
Hypothesis is that more tiling density means more sensitivity, and lower specificity (more likely to capture background).
Higher density doesn't change specificity in mito, and doesn't affect sensitivity much, either (except for a outlier at the low end).
So a little redundancy (2x) is good.

Conclusions:
Temp - if want highest % on-target, do 55 C. If want to capture as much as possible, go lower.
Time - avoid 4-6 hr, but no need to go longer than overnight.
Density - 2x is good, but no need to go beyond 4x.



W237 - Ludovic Orlando - Ancient DNA: From Genomes to Epigenomes

(There's now a 781 kya horse genome they published last year)

How they look at ancient epigenomes - nucleosome protection after death. By looking at depth of coverage, that should tell you where the nucleosomes were.
As for methylation, CpG should degrade to UpG, and mCpG will turn into TpGs. Old Illumina library building process couldn't extend through UpG, and CpG to TpG conversion should indicate there had been methylated.

That was the hypothesis, anyway. They looked at nucleosome arrays to see how well they matched up.

Nucleosomes were spaced about 193 bp, which is what was expected. They're also where you would expect based on exon boundaries.

ancient nucleosomal phasing and methylation, as well as ratio of methylation between gene body and promoter, can be used to predict ancient gene expression levels.

Methylation disappears pretty fast from ancient genomes.

Pulling down methylated DNA from ancient genomes may help you call SNPs in those regions.



W238 - Eleftheria Palkopoulou - Genomic Diversity and Population Dynamics of the Woolly Mammoth Prior to Its Extinction

There's an island north of Siberia (Wrangel island) that wasn't an island back in the last ice age. When seas rose, it became an island, the population on it was isolated.

Compared to Oimyakon sample - mainland siberian wooly mammoth.

Population history inferred by PSMC. Siberian and Wranger are a bit offset. Siberian is ~ 45,000 yBP, Wrangel is ~4,300 yBP (so substantially younger).
If you shift the curves so they line up, you can see how many mutations occurred in the Wrangel genome since the death of the mainland sample. Lets you calculate mutation rate. 

Both samples were males, meaning all their X sequence is phased. Used that to see that the population split was about 48,000 years ago, or about the time of the death of the mainland siberian individual.

Genome-wide diversity: Wrangel had lower het. per 1000 bp (1.00 vs. 1.25). It's not SUPER low (like snow leopards), so it's not THAT big a bottleneck.

So was there inbreeding?  Look at runs of homozygosity to find out. Higher number and longer ROHs for the Wrangel individual indicative breeding between distant relatives, rather than recent inbreeding (like between siblings, etc.)

Summary:
* Two severe population declines in the history of wooly mammoth
* Recent pop split between the ancestor of of the two individuals
* Decline in the genome-wide diversity of the Wrangel genome
* Increase in runs of homozygosity in the Wrangel genome



W239 - Peter D. Heintzman - Resolving the Evolutionary Relationships of Two Enigmatic Ungulates from Pleistocene North America

Camelops - extinct North American camel

Camelids actually originated in north america. Those that went to the old world became dromedary and bactrian camels, and those in south america became llamas and alpacas.

Camelids stuck in what is now Western US for most of pleistocene - but when the ice sheet wasn't there, they're found as far north as the Yukon. Better fossil samples up there, of course. Suspected to be ~125 kya.

Why care? Want to resolve phylogenetic tree, for one thing. Camelops currently thought to split from SA camels after they had split from old world camels - but not everyone trusts that model.

Extracted DNA in a way specifically designed to enrich for short fragments & did illumina shotgun sequencing. That worked.

Mitogenomic reconstruction - had to use iterative assembly methods, aligning to FOUR different extant camelid genomes. Required multiple rounds of assembly. Reads from each sample were aligned to the final draft with BWA and visually inspected for accuracy.

Made phylo trees in various ways, but the mito data always puts the camelops next to the OLD world camels, not the new world camels.

Phylo reconstruction inferred from low-coverage genomes: Mapped to Alpaca and wild Bactiran camel genomes, then Alpaca mapped to wold Bactrian camel and vice versa.
Again, camelops is on the old world branch. Evidence of a DNA damage artifact, too - gives you a bias toward mapping the more conserved reads.

Had they just relied on PCR, they would have found nothing, and excluded interesting specimens.



W240 - Christina Warinner - Direct Evidence of Milk Consumption from Ancient Human Dental Calculus

Milk is very nutritious, but it's hard for adults to consume because of lactose. Lactose is hard to break down, and requires special enzymes (lactase) to break down. If you CAN'T break it down, it gets all the way to your colon - where your bacteria then have a field day and use it to produce 8 liters of hydrogen gas.

Europeans have a super-high rate of lactase prevalence. That variant is the one that's been studied the best.

Buuuuut, it's not entirely clear how that variant got to northern Europe (and Spain) in the first place, since that allele doesn't show up when you'd expect it would in neolithic populations.
She points out that the way the milk is processed (think cheese) might make a difference, here.

So - they started looking at dental calculus. Shotgun metaproteomics.

Found BLG, a milk protein.
1. Humans don't make it, so it must be from some other animal.
2. It's ONLY in milk, not blood, etc.
3. It's hard for bacteria to break down
4. BLG lacks close orthologs in bacteria, so easy to identify
5. Many polymorphisms among species, so you can tell what animal it came from (except camels)
6. It's very abundant
7. Shows up a lot in products with high lactose content - so not so much in butters and aged cheeses, but really high in milk and whey.
8. Since it's sequenced based, it's a much more direct way from tying milk consumption to a specific individual.

On average, 20% of individuals tested positive for BLG from the skeletons from the milk-drinking regions.

Found goat milk in Italy, sheep and goat in Great Britain 



W241 - Beth Shapiro - The Rise and Fall of the Passenger Pigeon

There used to be a LOT of passenger pigeons - but they were tasty and cheap, too, and easy to trap.

So what happened? 

Stuff we (think) we know:

1. Pigeons are old
2. Ectopises is an old pigeon lineage.
3. Closest living relative is band-tailed pigeon.

When did they become so abundant in the first place? An open question. 
Hypotheses:
1. Post-glacial warming and forest expansion
2. Agriculture

So they tried out ten different models, and got 41 mitochondrial genomes.
Rapid, massive, star-like expansion. So none of their ten models fit.

They got scooped - another group showed that the carrier pigeon population already fluctuated a lot to begin with.
But they couldn't recapitulate it. Why not?

Well, the other group used the default settings on the analysis, which wasn't a good idea. The most important one is the C50 setting, since you don't want to use too strict a mappability criteria, since you're not able to use the ACTUAL genome you want to map to.

But, the big fluctuations don't tell you what happened in the last 20,000 years anyway, since you have no power there.

So, what do we know?
There's little diversity in the mitochondrial genome.
The other group made another mistake, which was making consensus sequences, which squashes heterozygosity.
So what her group did was create artificial haplotype sequence by picking a random base (from good reads) at each site, and then integrate.

Rather than nuclear heterozygosity of 0.1%, it's 1% - that's more diverse than chickens, which are incredibly diverse.

Don't know what's going on, but a few guesses:
Demographic history - could be (relatively recent) hybridization from something distantly related - but the bantail doesn't live anywhere near where the passenger pigeon.



###################################



Sequencing Complex Genomes



W712 - Thiru Ramaraj - De Novo Assembly of Plant Genomes: Hybrid Approaches for Heterogeneous NGS Data

National Center for Genome Resources (Santa Fe, New Mexico)

They use Illumina, PacBio, Oxford Nano, and (for mapping) BioNano Genomics.

Medicago truncatula - Want 25 genomes.
22 get just Illumina and ALLPATHS-LG.
For 3 of them, they want higher quality, so all of the above technologies are thrown at it.

Just with Illumina, it's decent - 80% proper pairs, 84% of reads map to assembly. Brought in PacBio to improve it.

P0756 - their Illumina/PacBio Hybrid workflow

Bottom line is that the hybrid approach is better in just about every measure. Helps prevents gene fragmentation in the annotation, too.

BioNano whole genome map - convert the images to molecules, and create consensus genome map. Can align against reference to look for structural variations, can use it for scaffolding, etc.

Cotton - a work in progress. They'd like to add in PacBio and BioNano stuff.



###################################



G001 - Philip Bourne - NIH as a Digital Enterprise - Implications for PAG

Relevant for many funders, not just NIH.

So what is a digital enterprise? Before that, a look at how far we've come in one researcher's career (his).
Back when he started working with the PDB, they used to ship tape to Australia via ship, and it took three months. We've clearly come quite a way since then.

Biomedical data is becoming more FAIR: Finding, Accessing, Integrating, and Reusing digital research objects.
But the idea here is to make things more efficient so we can do these things easier and better.

The move from an observational science to a more analytical science (though still observational) is being driven by ever-increasing amounts of digital data.

Cites "The Second Machine Age" - things like Google car, 3D printers, Waze, and Robotics as things that all showed up sooner than expected. All involve ingesting a very large amount of data.

Further perturbation: The Story of Meredith )http://fora.tv/2012/04/20/Congress_Unplugged_Phil_Bourne
As editor of PLOS comp bio, got a paper on pandemic modeling. Written by a single author. Sent the paper to Simon Levine, who also liked it. Meredith was 15 years old at the time - and was able to leverage a lot of public data, open literature, etc.

Four weeks ago, they had the gaming community workshop, to see what they might contribute to data analysis. It was great & productive - and they're going forward with a funding call - but then got stopped by a congressman writing an op-ed piece.

More perturbations - Begley and Ellis published in Nature that 47 of 53 "landmark" cancer publications could not be replicated.

ADDS mission statement - similar to NIH mission statement, but specifically about research conducted as a digital enterprise.

Some goals of the digital enterprise:
* Cost savings through sharing of best practices
* Sustainability
* Collaboration
* Reproducibility
* Integration of data types

Some of Today's Observations -
The good news:
* Genuine willingness to address the problem
* Global communities are emerging
* Efficiencies can be achieved
* BD2K is the beginnings of a plan
* We are beginning to quantify the issues

Bad news:
* Still no sustainability plan
* Global policies define the WHY but not the HOW (in terms of accessibility)
* We do not know how all the data we currently have are used - funding opportunities to look into this are upcoming. Gives the example of a netflix- or amazon-like predictive system that suggest data to look at based on the usage of others
* We can't estimate future supply and demand
* We need to ramp up training programs in data science

Sustainability 101 - NIH budget is effectively flat, but biological databases keep growing. This sort of thing does not scale.
Need to think about how much we spend on getting new data vs. keeping the data we already have.

What is the NIH doing to fulfill the promise of increasing the utility of biomedical research data?
He sees it as a three-legged stool: community, policy, and infrastructure.
Says this has to come from the community, but needs to meet in the middle with top-down policy.
Virtuous research cycle - what you do in your research is the motivating factor, not the infrastructure. But, the three legs of that stool can be of benefit to you.

Policies - now & forthcoming
Data sharing plans on all research awards, along with enforcement (machine readable plan, repository requirements to include grant numbers).

Data citation - legitimize data itself as a form of scholarship.

Infrastructure - BD2K has funded 12 centers of excellence, all of which are working with data in various forms.
Also funding DDICC - data discovery index consortium. That needs to exist so that data can be found (can't google data very well), so you need an index.
Software needs to be given credit the way data needs to be given credit - and also findable.
Standards are also being funded.

All of that - along with bringing in extramural labs - comprise The Commons.

The Commons - idea is that it'll be system agnostic, so it can be in the cloud, PHC platforms, or any other kind of platform (like in house compute solutions). Just need to have things be commons compliant.

The business model - switching things up. There's currently not a good sense of supply and demand, so they're trying something as a trial. Rather than awarding money, they give you credit to spend in commons-compliant resources. That should drive supply and demand.

How might PAG participate?
Consider contributing objects to the commons, regardless of who funded it (data, software, standard, narrative, course materials...). Working toward a more open framework.

Training goals - 
1. Build an open digital framework for data science training
2. Develop short-term training opportunities
3. Develop the disciple of biomedical data science and support cross-training - OPEN courseware


Questions from the audience:
Is there incentive to USE deposited data?
Answer: He's not a big fan of sticking data somewhere and just leaving it without tracking usage. Need at least some level of light curation.

What about SBIRs for "smart kids from California"?
Answer: Clearly useful, but nothing in BD2K. The institutes have various things that are applicable, but we need to do more.

Carl Schmidt - Research & scholarship is not business, and business models have been destructive to collaboration. Also: senior faculty need to be educated about how important data sharing, etc., are, so young faculty aren't hurt or denied tenure for doing the right thing.
Answer: There's a partnership that needs to take place here, as we're in a very dire situation. Some things are going to have to change to solve these problems. 

Data curation costs money, and some are forced to charge users. Is that sustainable?
Answer: Subscription is a possibility, certainly. Services that are on top of the data...well, the question is if there's a market for that, and that sort of thing should be explored. Maybe we can do better.



###################################



G002 - Beth Shapiro - The evolutionary consequences of interspecies hybridization: insights from ancient and modern bear genomes

Talking about when we find out things that we thought were distinct species actually hybridize - or, in general, when we find surprising things in the data we collect.

So maybe, by now, we shouldn’t think of this hybridization as a surprise. After all, as two populations diverge into separate species, there IS a point where they can no longer reproduce - but there’s gene flow before that, and sometimes it’s only an environmental barrier that’s preventing reproduction.
When that barrier disappears, you can get gene flow again. So, what does that mean evolutionarily?

There have been an increasing number of polar/brown bear hybrids accidentally killed. Finding these in the wild is new - there are polar/grizzly hybrids in zoos, but that's it.

Polar bears are larger than brown, adapted to swimming; brown adapted to running and climbing. Brown bears are very diverse, and are all over the world. Polar bears are pretty similar to each other, and brown pretty different from each other.

So - 1) is hybridization a new phenomenon?
2) What are the evolutionary consequences in admixture?

For 1) -
Initial work was in mitochondrial DNA. If you look at just this, everything is very geographically clear - clades clump together very nicely. All polar bears are in clade 2, which is otherwise just made up of brown bears.
Back then, the hypothesis was that polar bears were recently diverged from brown bears (like, 20,000 years ago, which is not very long all).

But then polar bear fossils much older than that were found.

So then a group published on nuclear DNA in polar bears, and things got messy. Another paper interpreted things differently, and it was all very confusing.

So they decided to sequence more bear genomes.
Polar bears are nearly identical to each other - even less than diversity within humans. Brown bears have more than polar, and the ABC island brown bears are ever so slightly closer to ABC polar bears than to brown grizzly bears. So something weird is going on.

They did a D statistic test - detects incomplete lineage sorting or admixture. Looking for an excess of shared, derived alleles different from what you'd expect based on the species tree.

Is there a difference among polar bears with how close they are to mainland brown bears? Nah. Same with ABC brown bears.
But - comparing mainland brown, polar, and ABC brown, they do detect gene flow - but not the direction.

To determine the direction, see if they can "hide" polar bear DNA in brown bears via simulation. They can, so that's the direction.

The Dx/Da ratio is more indicative that brown bear males moved to the ABC islands, where there were female polar bears, than vice versa. (Cahill et al. 2013)
But it's still not QUITE right.

So they sequenced MORE bears. This time, they got an extremely pure brown bear from Sweden. That revealed that the mainland brown from Denali they were looking at actually IS admixed.

So hybridization is not "new", but is it common?

Mitochondrial clade now has SOME of the now-extinct Ireland brown bears in the same clade as polar bears & other browns (though Ireland browns also show up in another clade). These ancient Irish bears ARE also hybrids of polar/brown bears.

2) What are the evolutionary consequences?

There's quite a lot of admixture, yet effectively no brown bear DNA in polar bears. So maybe they can't tolerate brown bear DNA to survive in their environment. When hybrids survive, they survive as brown bears.

How much of the ABC genome is polar bear?
Well, there's WAY more polar bear-polar bear homozygotes than you'd expect under HWE, so something is selecting for this polar bear DNA. That's despite these multiple rounds of back-crossing

Most of what was retained looks to be olfactory receptors. Huh. They don't really know why.

One final point - this is a weird way to think about what constitutes a species. You get gene flow from polar bears to brown, but not really in the other direction.


Questions from the audience:

Is this maybe a pheromone thing?
A: Maybe! A good idea.

What is the maximum diversity genome that still allows hybridization?
A: Don't know - other that, as long as things can mate, they do.

Did they actually find a phenotype for the hybrid?
A: The F1's look like the ones the hunters killed, and all the others just look like brown bears (and the Ireland ones are extinct). So they don't really know.

Are F1s sterile?
A: No.

Any evidence of imprinting?
A: Female polar bears are induced ovulators, female brown are seasonal ovulators. Also, male polar tend to kill female brown, while male brown are more likely to run in to 

Implications for polar bear conservation?
A: Don't think so. Polar bears aren't threatened by this hybridization.



G003 - Trey Ideker - Interpreting genome variants and mutations using a unified human genetic network

What if, instead of going from gene to phenotype (in GWAS), you add a middle layer to integrate genes with networks, and then go from network to phenotype? That's the network knowledge base.
Today, he's focusing on methods to use networks to translate genotype to phenotype.

Hahn and Weinberg, Nat Rev Cancer 2002 - cancer as a subway map. What it's trying to get across, though, is that we need a much more complete map.

Everything he's showing is visualized via Cyctoscape.

He works on cancer. Why is it hard to cluster/stratify cancer genomes? No two patient tumors look alike. About 100 somatic mutations per exome per patient, including about 10 drivers of cancer.
These cancer exomes are extremely sparse, so you can't cluster them.

So that's where they help prior knowledge of molecular biology in the cell might be helpful. Want to go through protein interaction databases to get robust clusters.

An intuition for network smoothing - put the genotype for two patients on a network, you'll see they mostly don't overlap. What you then do is have each point "bleed out" in a decreasing function the further it goes. That's network propagation, and it lets you see the things that are NEAR mutations in network space. THAT's what lets you do the clustering.
The idea is what seems random is not random if you have prior knowledge of cell biology.

Association of ovarian tumor subtypes with patient survival (since that's the output data that was available) - They divided survival into three subtypes, and saw clear relationship. Hypothesis was that most severe group, subtype 1, had platinum drug resistance - and it looks like that's the case.

Hofree et al Nat Methods 2013 - can focus on the part of the network most responsible for the clustering of the aggressive subtype being called as such.

Validation - ICGC Ovarian cancer cohort found basically the same kind of thing.

Same sort of thing applied to Corticospinal Mortor Neuron disease, a complex Mendelian disease (usually just one gene, but not the same gene as in other pedigrees)

Networks aren't just modular, but also hierarchical and multi-scale. Shouldn't really be represented as flat. Nested modules.

For example, a "hairball" that incorporates physical interaction, high co-expression, and genetic interaction (Carvunis and Ideker, Cell 2014). But it's hard to see the sort of interactions you can see by looking at the physical structure. Network models do not visually resemble the contents of cells.

nexontology.org - takes advantage of nested ontology of gene ontology. Problem with GO is that it's hard to scale.
This slide is constructed not by literature, but by database protein interaction information.

Looks like it's aiming to make the visualization of functional components more obvious. Again, supposed to be a nice middle link between hairballs and protein structure.


Also key to all this is a network-enabled candidate gene approach based on prior knowledge of "seed genes"


Questions:
How complete does a PPI network need to be to be useful?
A: They're working on it. In a recent paper, they removed edges to see how robust it is, and it's pretty robust. Likely due to lots of redundancy.



###################################



Thermo Fisher Scientific (Formerly Life Technologies&trade;) - Genetic analysis approaches in agriculture biotechnology



Jenny Xiao - Solutions for plant and animal research and breeding

Works out of Carlsbad

Where GBS fits into marker development workflow: discovery -> trait mapping -> screening. ThermoFisher has things for all three (Discovery Ion, Mapping Ion, and Validation/TaqMan).


Dr. Anoop - AmpliSeq workflow

Business unit that manages library products. Representing R&D here.

AmpliSeq is for target enrichment, and is the start of a workflow with stuff like the ion proton downstream.

ampliseq.com - has genomes preloaded, and you can upload your own genome.

After that is automated library prep.

Then Ion Chef: libraries in, loaded chips out. No hands-on time needed.

Ion Proton - fastest sequencer. 384 samples in 3 hours.



Lex Flagel - The genetic architecture of western corn rootworm resistance to the Cry3Bb1 Bt protein

Adults live in the pollen-containing parts. The larvae spend the winter as eggs, but then they eat corn roots as larvae. In aggregate, they can do a lot of damage.

Tough to control - resistant to crop rotation, several chemical insecticides.

Cry3Bb1 Bt toxin - developed by Monsanto to kill the WCR. Builds a pore that destabilizes the gut membrane, thus killing them.
In 2011, first reported resistance to this (in an area with low crop rotation).
Resistance appears to be persistent.

His goal was to map the resistance - understand the genetics & identify markers to track the trait in the field.
Need to know if it's heritable, and develop a genetic toolbox for the WCR.

H^2=R/S (survival of offspring divided by survival of parents).
Here, H^2 = 0.69, so high heritability. Good news it's mappable, bad news is it builds up fast in the field.

No difference between reciprocal crosses, so probably not sex-linked. Resistance allele predominantly recessive. 50-fold fitness advantage.

WCR is 2.5 Billion BP, with many young repeats. Challenging to assemble, so they did transcriptome instead.

But they just want to genotype.

PoolSeq - populations for SNP discovery. Combined individuals in a library, sequenced to 25x.
Picked the rare SNPs & made a genotyping platform from that.

AmpliSeq basically does amplicon PCR over your SNPs of choice.
Cross parents, then sib-cross F1s to get F2s.

Randomly partition to F2s, put some on Cry3Bb1 corn, and others on a similar plant with no transgene. Basically just a chi-squared.

Resistance clearly maps to LG8. They looked at many families to confirm this.

Are these markers relevant in the field? Yup.

Conclusions:
* This is a fast way to find markers - use pooled population sequencing, and GBS with the Ion machines.

This is also in Flagel et al. 2015, G3.



James C. Schnable - Fast turn-around genotyping without a reference genome using the Hi-Q™ sequencing kit on the Ion Proton™ System

Data2Bio:
* tunable Genotyping By Sequencing (tGBS)
Testing Hi-Q in Maise; reference-free tGBS in Hi-Q Maize.

Conventional GBS - kind of scattershot, so sometimes you don't hit the sites you want.
tGBS - more stringent, you pick the sites & have an easier time dealing with heterozygosity.

Instead of a dsDNA adapter, they add an ss oligo.

Hi-Q upgrade for the ion proton: intended to reduce deletion errors (and they do). Also allows wet lab time to be used much more efficiently. Also increases sequencing quality.

Assessing per base error rates: For each individual select only sites covered by >5 reads; identify SNPs and InDels present in only individual reads.

Required one and only one alignment to avoid problems with paralogs.

In his univserity job, looks at proso millet.
tGBS discovers many extremely rare alleles (even if the SNP had to show up in at least 3 accessions)

Combining tGBS with Hi-Q gives you a really fast turnaround genotyping.



Jason Boone - 300 Strong: Leveraging Ion AmpliSeq™ and Ion Torrent™ sequencing for targeted genotyping studies

From Floragenex, which does RAD-Seq. They discovery SNPs, genotype them, or help with the analytics.

Some of the people they work with don't need 10,000-100,000 SNPs, only like 100-1,000, but they want to pick them specifically.
That's why they use the AmpliSeq - not the only solution, but a really good solution, he says. Targets between 12-24,000 targets.

AmpliSeq integrated workflow:
Oligo design
Make primer pool
Prepare template with Ion Chef
Run sequence on Ion PGM sequencer
Analyze with Torrent Suite analysis software (with native variant caller, or Floragenix's own genotyper)

So they wanted to test the accuracy and reliability of the AmpliSeq.
Test population of OrNe Barley introgression population

340 targets, mostly 140 bp long (for the insert; total is 187 bp).

Performance: Reads per sample around 40,000, coefficient of variation 0.57 (meaning all targets get a reasonable amount of reads per sample); modal seq depth is ~45x coverage (median 102x).

You hit the same regions whenever you run this (or 86% of them or something)

Concordance between this and other platforms is about 99.4%, so extremely accurate.

Early access service for this begins now.



Li Song - Application of digital PCR in analyzing transgenic soybeans

Using QuantStudio 3D Digital PCR System

Some parts of the world are more strict on GMOs than others, so we need to detect them in seed lots and food stocks.

Roundup Ready Soybeans - from Monsanto. Resistant to the pesticide, and the 2nd generation of these also have a higher yield potential.

TaqMan assay - specific for detecting RR1 and RR2 soybeans using dPCR. Nothing else rises above background.
Using sheared DNA leads to similar results. Lower copy number per microliter, but similar specificity, at least based on the bar height in the graph.

Good accuracy and sensitivity in detecting/quantifying the RR1 soybeans by dPCR. That means you can find RR even when it's a small fraction (1%?) of the sample.

The designed soybean RR TaqMan assays could not detect RR maize DNA using dPCR.

So, dPCR is similar in accuracy/sensitivity to qPRC.


Q3D PCR method can effectively identify the positive T0 transgenic events.
dPCR gives a negative result for chimeric plants (they're not REAL transgenics).

T1 generation DNA analysis - Find Gene/Lec ratios of 0.5, 1, 1.5.

If ration is around 1, probably homozygous. 1.5 is het.

dPCR can identify homo or het plants in T1, while the regular method can't distinguish - so dPCR lets you save time, space, greenhouse labor.



###################################



It's not just the data, it's the analysis; - Why a Biology Cyberinfrastructure is the Best Way to Meet the Challenges of Large Datasets



W483 - Kapeel Chougule - Flash Demo: Atmosphere Cloud Computing - Applied to Visualizing RNA-Seq Data

Definitions for cloud computing:
Image - an alternative to copying the files. Here, you take a snapshot of the current system and load it onto the new one. That way, you can clone it completely, which helps you keep standard tools.
Instance - Disk, CPU, memory, plus the image.

Atmosphere features:
One-click access to hundreds of machines, customizable.

Can choose an existing image or customize. Instances up to 16 cores/128 GB of RAM.
Can access via shell or VNC
Images are sharable

Things to remember:
Images don't have auto access to your iPlant Data store (can use iDrop or iCommands to get it there)
Users have monthly allocation limits
All dat on terminated instances are destroyed.

Demo:
Log in, it launches an image. It loads instances you've been running, resources you've been running.
Can search for other images - say, if you want one for RNA-seq.

Visualization of RNAseq data using IGV - load in the BAM, just as you'd expect.



W484 - Donovan Bailey - Using iPlant Tools and Plastome Sequencing As a Springboard into Comparative Genomics: Plastome Organization and Sequence for the Mimosoid Legume Leucaena trichandra

"I have a confession to make - compared to most of the other speakers here, I don't know much bioinformatics." But that's one of the target groups of iPlant.

Working on legumes and plastomes.

Variation among legume plastomes - more variation than other angiosperm families. However, all 11 genomes they had so far were all closely related.

So they dove in, using Illumina data to assemble and annotate the species they were looking at.

Sources of plastome size variation - 
Mononucleotide repeats: small percentage.
Dispersed repeats: up to 30-fold variation, but it's not responsible for the Mimosoid large size.
Tandem repeats: These expansions look to be a significant part of the size of Leucaena and the other two.

Conclusions:
iPlant resources and training can represent an important stand-alone and/or springboard into serious bioinformatics analysis



W485 - Jeremy DeBarry - Flash Demo: Secure, Collaborative, Management of Your Life Sciences Data with the Data Store

This is the backbone of the iPlant infrastructure, and contains metadata capabilities.

Can view through a browser (or, I believe, the command line), and can load from local files or URLs.

Sharing: If with iPlant users, can share within the system. Otherwise, can make public links.

Fairly powerful file search integrated, and only searches file you have permission to access. You can create smart folders that update based on your search criteria.

iDrop Desktop - parallelizes file transfer.



W486 - Andrew D. Nelson - Utilizing iPlant to Unearth Long Non-Coding RNAs and Characterize Their Evolution in the Plant Family Brassicaceae

For his purposes, lincRNAs are the 99% (of your genome). Difficult to distinguish between background noise and "functional" long non-coding RNAs.

So he wants to find those that are functionally significant, those conserved across a family.
Mining genomic and transcriptomic data (where possible).

What are the mechanisms by which lincRNAs are gained or lost in a genome?

Do lincRNAs evolve in a manner similar to protein-coding genes following an polyploidy event?

How he found them in the first place: in iPlant, used public reads. TopHat, Cufflinks, Cuffmerge to group transcripts form multiple experiments, filter transcript for linkRNAs, then moved to CoGe for lincRNA genomic comparison.

lincRNAs are lost from genomes at a higher rate than protein-coding genes (what you'd expect). Faster rate of loss than in vertebrates.

lincRNAs are much more likey to be single copy, even in polyploid genomes.

lincRNA loci are more likey to be lost from polyploidy genomes. Supports polyploidy -> faster evolution.


Genomic conservation of a lincRNA locus across a family strongly correlates with expression. They'd like to know if function is also conserved.



Nirav - Image Analysis in BisQue

Can handle 300+ image file formats - including Geotiff, which has gps data integrated in the images (like drone photography of fields)

It does look pretty neat, especially for something in a browser. You can rotate z-stacks in 3D, for one thing.



W488 - Daniel B. Taylor - Life Sciences Cyberinfrastructure at Your Fingertips: Using Internet2 to Power Scientific Collaboration

Best-in-class national 100G optical network with 8.8 Tbps capacity.
Pushes the limits of networking.

Full range of networking options:
Layer 1 - point to point wavelength networking
Advanced layer 2 services - open virtual network with connecting spreads up to 100 Gbp
Advanced Layer 3 services - advanced IP connectivity to the world

Life sciences challenges - among other things, lack of metadata.

GridFTP is much faster than ftp

Cyberinfrastructure on demand -
Made a link from China to UC Davis. The mail would be 2 days, internet would be 26 hours, but the internet2 link took 30 seconds.



###################################



G004 - Xuemei Chen - Plant microRNAs: biogenesis, degradation, and modes of action

UC Riverside

Small RNAs are central players in PTGS and TGS.
miRNA and siena associated with members of the Argonaut clade, and piRNA is associated with a member of the Piwi clade.

Transposable elements are both the source and target of 24 nt siena to silence themselves (in the case of AGO4).

Small RNAs impact nearly all biological processes.

miR172 has a role in flower development (Chen, Science 2004). miR172 inhibits AP2.

Steady-state levels of miRNA is a combo of biogenesis and degradation. Little is known about the mechanism of degradation.

Hen1 deposits a methyl group on miRNAs and siRNAs - in plants, anyway. Animal miRNAs are not methylated, but siRNAs and piRNAs are (by Hen1 homologs).

What’s the function of this methylation?
Methylation protects miRNAs from 3’ truncation and 3’ tailing. Completely protects from necleotidyl transferases, and partially protects from exonucleases (SDN proteins).

The SDN family of exonucleases participates in miRNA degradation in vivo.

Are SDN proteins responsible for miRNA 3' truncation in hen1? Checked truncation via HTS. At least for some miRNAs, yes. Based on measuring length of truncation & length of tailing in various mutants.

HESO1 tails miRNAs in hen1. HESO1 and UTR1 act coordinately to tail miRNAs in hen1.
Looks like HESO1 does some of the longer tails, but even when knocked out, you still get some short tailing. That's what URT1 is responsible for.

The two have different substrate preferences. URT1 wants a substrate ending in A, HESO1 wants one ending in U - but both of them add on a U, which is why HESO1 is the one that adds longer tails.

Are 3' tailed species functional, despite being bound? She has an answer for one such tailed species.
The 22 nt miRNAs, rather than 21, can be functional & trigger the production of secondary miRNAs.
But, URT1 can turn an 21mer into a 22mer (as is the case with miR171), and then that can make functional secondary miRNAs.

De-methlylation may not be necessary for degradation if an exonuclease like SDN can just chew through it.

Target RNA incudes miRNA degradation.

3' truncation and 3' uridylation are conserved processes in small RNA metabolism - that is, conserved between plants and animals.

But it's different for miRNAs. Plant miRNAs are very similar to their target along their whole length. Animal miRNAs just use nucleoties 2-7 to do "seed match". That means the 3' end of the miRNA is still hidden by the PAZ domain - so it's still protected, and doesn't need methylation for protection.
BUT, if you make animal miRNA with high complementarity, then yes, you get 3' truncation and 3' tailing.
Animal piRNAs, on the other hand, already have high complementarity targets, hence the need for protection.

Different modes of action for plant and animal miRNAs - Plants guide the precise cleavage, while animals cause deadenylation and degradation.

Nucleotidyl transferases associate with AGO1 (maybe why they need to be protected in plants). Animal miRNAs don't associate like that, but she best piRNAs do.

ONe last bit - RNA uridylation is increasingly recognized as a signal for degradation in general.


Questions:
Why uridylation before degradation?
A: She doesn't know, but maybe some nucleases prefer them that way.

URT1 prefers truncated targets - is that maybe because it's seeking out erroneously-cleaved targets?
A: Yeah, probably.



G005 - Michael Goddard - The Evolution and Genetic Architecture of Complex Traits

How does genetic architecture we see today come about?

It's tricky, since we see heritability everywhere, but most of the models incorrectly predict that these will be due to rare alleles, and that's not the case.

New data may help - lots of GWAS out there, and cattle offer another perspective. As human diversity increases, cattle have decreased, so they're about the same as humans now in terms of diversity.

There's a lot of long-distance linkage disequilibria in cattle, so you get a LOT of SNPs associated with a given trait.

Can't really test one SNP at a time.
A better model is to fit all SNPs simultaneously and treat them as random effects. "Genomic selection" (Meuwissen et al 2001).
BayesR for mixing normal distributions to get whatever distribution you chose.

Applying BayesR does a much better job of positioning the genes related to the trait (in this case, weaning weight)

Most SNPs have no effect, but a very few come from distributions that explain as much as 1% of the variation (or more - about a third of those go beyond 1%). The more complex the trait, the fewer large effects and the more small effect SNPs.

The largest proportion of genetic variance are those between 10^-4 to 10^-3 variance each - since there are so many of them, that sums to about 52% of variance explained.
On average, anyway. Again, some diseases/traits are more complex than others, and those less complex have more large effects.

So, could they create a simulation that could match the experimental results?
Components of simulation:
Constant Ne
Mutation U per gamete (infinite sites model)
Effect ~ + or - Gamma (shape = 0.1)
Recombination
Fertilization
Selection

Ne = 20,000
Assumed 12 million sites that could effect a trait. A huge number, he thinks, but apparently you need to assume that many.
(More that went by too fast)

Tuned such that distribution of variance across effect sizes is what was seen experimentally - lots of small effects, very few large, mostly around-ish the middle.

Combining the variance across variance explained by individual QTL - again, 10^-4 to 10^-3 of variance explained is the biggest class.

On commercial chip, small effects are underestimated, because the minor allele frequency of all the SNPs on that chip are above 10%, and you can't have that in LD with something that's less than 10%.

But lack of large effects doesn't quite fit, so need to modify the model.
As you decrease the number of millions of mutation targets, you fix that, but then you shift the central tendency incorrectly.
He can't make the model fit, probably because in neither humans nor cattle is there actually an equilibrium situation. Things either go to fixation or are lost.

He says that the reason for the different shifts of rare vs. common alleles has to do with different "target size" - fewer sites that COULD affect those traits. That's how you get diseases/traits with  more or less complexity than others.

In conclusion, the model (with some quite striking features, he says) reasonably approximates what we see in the genome.
Selection against the mutations increases more than linearly as the size of the mutation goes up.
But real data has more small and big effects than predicted.


Questions:
Can we overcome sample size restriction?
A: You don't need genome-wide significance. Don't even need to find the QTLs. If you're using a block method of genomic selection. The relevant number is the number of genomic effects segregating in the population. So - answer is different for genomic selection vs. trying to prove you have the right QTL.

What about epistasis?
A: There's certainly some there, but it doesn't explain a large proportion of the total variance. The additive part looks to be the more important part. But it's hard to estimate epistatic variance.

Are the QTLs young, or rare & recently selected?
A: When we talk about the age, it's the age of the MRCA, not necessarily where it originated.

What about coming at this from the phenotype direction?
A: If we had less complex traits, there'd be less complex architecture. But, in the past, trying to break phenotype down into sub-traits tends to not be very successful.


