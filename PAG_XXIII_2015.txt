1/10/15

W786 - C. Titus Brown - How Well Can We Reconstruct Metagenome Content? Thoughts and Data on Metagenome Assembly

Reconstructing metagenomes from shotgun data.

Not talking about 16S. Shotgun is collecting samples, extracting DNA, sequence fragments,  and then you analyze it. "Sequence it all and let the bioinformaticians sort it out." That is, don't try to fiddle too much with the molecular biology - much can be sorted computationally.

To assemble or not? If the goal is to reconstruct phylogenetic content and predict functional potential of ensemble, you can either analyze short reads directly, OR you can assemble and THEN analyze (with weighted contains). So which to do?

Assembly is a bad bioinformatics technique - uses a lot of heuristics, bad software.

But - it can be useful. For stuff like this, you want to assemble. The length of the alignment means that you're passing a better single to blast for homology searches. Specificity is much better than just short reads.

Few assembly benchmarks have been done. 
We usually take unknown data and run it with new software & publish on it - which is a terrible way to do software.

Shakya et al 2013 - built a mock community so they KNEW what it was, then did real sequencing of the mock community. 101 bp PE reads, as well as other comparison sequencing.
What they found:
* Metagenome seq outperformed rRNA primer sets
* Illumina is good if you already know what you're looking for and you're just counting
* 454 with MG-RAST doesn't work very well at all with default parameters. So don't do that. Can change the perimeters.

What about assembly? His lab has been making a tutorial (since otherwise, there aren't really standards). Kalamazoo Metagenome Assembly protocol.

They used that to benchmark that mock community dataset. Tried three different assemblers (Velvet, IDBA, SPAdes). Also applied three different filterings. Since they have a known answer, they can compare with Quast.
Note - if something works on mock communities, it might not work on real ones (and probably won't).

IDBA and SPAdes outperform Velvet for genome fraction recovered (90-91%). Those two are pretty similar overall.

Computational cost - Quality takes a lot of RAM, which is why they do Diginorm and Partition.

Project is still in progress.
What is NOT being assembled, and why? Low coverage? Strain variation? Something else?
The mock community also gets 10 MB of sequence that isn't supposed to be there - probably contamination.
MEGAHIT is a good assembler.

90% recovery is pretty good - relatively few misassembles, too.

* You don't misassemble sequences (not systematically)
* You DO need big computers (for now)
* It's still technically tricky, but there's hope.

Everything they did here is reproducible and open. Search for "khmer protocols"

During questions, again stressed the importance of having better known standards, so we can actually trust our protocols.



W038 - Alexandre R. Caetano - De Novo Genome Assembly of the South American Freshwater Fish Tambaqui (Colossoma macropomum)

(Talk started before I arrived, so I don't have the beginning)

First round of assembly - one lane with shotgun and one lane with mate-pair
Partial assemblies using ~25% of each MP library size class and all 400+800 shotgun data available (if you throw out a lot of mate pair entires, the N50 drops)
That's only the case for the 10 kbp library - it's better than 15 kb, and much better than 5 kb.

Second round - additional reads. One lane with 400 bp SG, one with 800, and one lane with mate pair.
# of N's in the assembly was initially 33% of reads, but after gap filling, it dropped to 10%.

Quality check:
Detection of "universal" genes using CEGMA (run prior to gap filling)
85% of CEGMA core genes found complete
9% found partially
So pretty good.

Automated annotation: used MAKER2. Used the Mexican tetra to serve as the source of ESTs, since they don't have them for the actual species they're sequencing.

MAKER2 found 23,632 gene models found, ~20,000 shared with the tetra.

Variant calling - After filtering for Q20, have 3.6 million high-quality SNPs, and I think about 300,000 indels.

Future:
Data from other assemblers
RNA-seq to improve gene prediction
Detection of markers within and between related species

Used Nextera, so it doesn't go through PCR.



W788 - Mick Watson - The Microbial Proteome Associated with High Methane Emissions in Cattle Determined By Shotgun Metagenomic Sequencing

Food security - Everyone in the world having access to the food they need to survive. We don't do this well - despite all the obese people, there are a LOT of hungry people. Lots of challenges coming down the pipe, too - population increase, changes in consumer demand, globalization, climate change, competition for resources (like water), and Westerners who want to eat "happy chickens".

Three gorges damn - just to show that humans change the environment. It shifted so much mass that the day is now 0.06 microseconds longer.

Similarly, humans have changed our livestock, even over just ~45 years. That's all just through breeding. See van der Steen, Prall and Plastow, 2005 J. Anim Sci.
We need to keep this up, since we need these improvements to feed our species.

He studies gut rumen metagenomics. Why?
* Energy from food - microbiome could perhaps be used to get MORE energy from the food by manipulating microbiome.
* Novel enzyme discovery - Scientifically, it's a novel environment
* Methane emissions - entirely microbial in origin. And the cows burp it out, not from the other end.

No relationship between methane emissions and metaGENOMIC abundance of methanogenic archaea, but there is with metaTRANSCRIPTOMIC abundance. That was in a GR paper, anyway. Mick's data doesn't support this. (The Shi et al. paper looked at sheep, though, and Mick looked at cattle)

Matched steers based on breed and diet, looked at methane emissions. Also separated high and low methane emitters.
High methane emitters DO correlate with high archaeal abundance.

Enzyme abundance - matched for breed and diet, the abundance of several enzymes is associated with methane production.

What's in there? Assembled all 8 metagenomes from the cattle using MetaVelvet, predicted genes with Prokka, annotated using Pfam domains. 1.5 million gene/protein predictions, less than half have any known domain (and that includes domains of unknown function). Even in 8 samples, thousands of novel enzymes to look at.

Figuring out what all those enzymes DO is hard - as biologists, we don't really have a good system for tackling this (at scale?).

Metagenomic assembly - Working on a cattle metagenomics dataset, tried various assemblers

Mick: "There's regular velvet, and there's version specifically for metagenomics."
Titus: "But you're the only person who uses it now."
Mick: "...Yeah."

But you can't do the whole dataset - as Titus warned, you'll run out of RAM. 

So they did a smaller dataset. When you map back to the assembly with BWA, IDBA-UD works reasonably well.

Need to use complete genes in these studies, which is tricky in metagenomics, as orthologs often look like repeats, so a LOT of contigs start or end in the middle of a gene.
IDBA-UD did well relative to the others, BUT it still mostly found fragmented genes

Metagenomic assembly is an unsolved problem, need lots of RAM, and assemblies fail in the middle of genes.
IDBA-UC looks to be the best assembler so far.



W789 - Timothy P.L. Smith - Characterization of the Bovine Nasopharyngeal Microbiome in the Context of Respiratory Disease and Treatment

Dealing with Bovine Respiratory Disease Complex (BRDC) - 25-30% morbidity after feed lot.

Multi-factorial, stress-related disease. Mortality associated with bacterial pleurpneumonia.

Control measures - vaccination against the disease, vaccination against specific pathogens. Limited protective ability, though, and antibiotics not always effective.

First experiment - stress the cattle so they'll get sick (since they need to study sick cattle). Mixing and highway transport are big sterssors, make them "high risk" cattle.

Some were metaphylactically injected, and the rest weren't. Three sets of these treatments, from cattle from separate sources.

Studies nasophyryngial swabs, rectal samples, blood phenotypes (cytokines and immune repertoires)

Profiling the microbial community with 16S. The problem, though, is that most of the DNA you get is cow, not microbial. Bustamante's selection technique gets rid of too much DNA, and it costs them too much to just sequence everything, so they're punting on that and just doing 16S.

Can now also do full-length sequencing of 16S with PacBio.

Analysis: WebMGA vs Greengenes give somewhat different answers. Lots of mycoplasma, though.

Substantial animal-to-animal variation, even within the same sale barn.

All animals that received metaphylactic macrolide treatment had increased mycoplasma (N=7)

Summary:
* Full-length 16S does achieve species-level discrimination
* Nasopharynx is relatively low-diversity compared to, say, rumen.
* (And there were more, but the slide changed)



###################################



Plant Genome Engineering

(Note: this session had a small, low screen in a crowded room, so I was only able to see about the top half of each slide.)



W597 - Dan Voytas & Bob ??? - Strategies for the Targeted Modification of Plant Genomes

Research dedicated to developing SITE-DIRECTED DNA sequence modification metrologies and applications. Modifications often stimulated by DSBs.

Desired outcomes: mutagenesis, gene replacement or edition, targeted gene insertion (like from the transgene world, creating molecular stacks), targeted structural changes.

Example: targeting soybean Dcl1a and 1b with ZFNs (dicer family). Used one ZFN with two paralogous targets, and recovered two T0 plants with multiple mutations. Able to "stack" double mutants.

CRISPRs deleted a 76.7 kb region in Medicago. That's 4 CRISPRs, not just one.

Dan Voytas -

He's going to talk about using a "homologous template" to serve in repair, so you're able to put in your desired modification.

Using Agrobacterium to achieve gene targeting: the bacterial T-DNA goes from the bacterium to the plant nucleus, and has the machinery to make the cut. It ALSO contains, on the same construct, the modified template you'll use for HR repair.

Harnessing plant DNA viruses (geminiviruses) to get efficient targeting. Similar to before, but now the tDNA has the viral replication machinery. That lets it circularize in the plant nuclease, AND replicate so you have more copies to work with.

Replicons enhance gene targeting frequencies 10 to 100-fold. Partially due to increase in donor molecules, AND by pushing the host cell into S-phase, making HR more likely.

Introduced a promoter to drive Ant1. Since this is plants, you can take the part that was modified (explants) and grow it back into a whole plant. 

Questions: larger donor plasmids have less efficient rolling-circle replication, so keep that in mind.

Have they changed GC content of the replicons (lowering it) do geminivirus has an easier time b/c it'll be less methylated? They haven't done that yet.



W598 - Caixia Gao - Developing Genome Editing Technologies for Crop Improvement

Working in rice and wheat. 

SSNs enable efficient genome engineering. Their research focuses on ZFN and CRISPR/Cas9.

4-5 months from TALEN or CRISPR design to produce knock-out mutants.

Showed example of a ~1.3 kb deletion from TALEN

CRISPR: Rice uses OsU3 promoter, wheat uses TaU6 promoter

Homology-directed repair (HDR) in rice: Just inserting some restriction sites, but it works.

Creation of fragrant rice by genome editing. Done by making deletions in Badh2. TALEN-induced mutations in OsBADH2 gene, which were transmitted to T1 and T2 generations. Making these deletions increase 2AP content, the fragrant compound.

Interestingly, new indel mutations were detected in subsequent generations. Looks like 1 bp deletions were expanding in T1 and T2.

They also used CRISPR for OsBADH2, and found similar results.

Powdery mildew - a very destructive disease in wheat. They combated this by targeting three TaML homoeoalleles with TALENs. Knockout of all siz alleles of TaMLO in wheat confers resistance to powdery mildew. (She later noted that this does run into regulatory challenges in the US)

NHEJ-mediated GFP insertion into wheat protoplasts.



W599 - Puchta Holger - The CRISPR/Cas System can be used as Nuclease for in planta Gene Targeting and as Paired Nickases for Directed Mutagenesis in Arabidopsis Resulting in Heritable Progeny

Nuclease-induced genome engineering

Background: years ago we were using I-SceI to introduce DSBs - at least to show what happens when you make that break. You could put T-DNA in the break, and it needn't be homologous for it to integrate (why sometimes you get the CRISPR transgenes integrating therein). Lower frequency than homologous, of course.

In planta gene targeting technique - gene targeting that takes place during the life cycle of the plant. Idea is to do the targeting in all cells, and then going to the seeds to get what you want. The donor is integrated into the donor via transformation. The way they do this still involves I-SceI in his example - your donor has the same homology arms as whatever flanks your I-SceI site of interest, and that's how you get HR.

The idea now is to do the same sort of thing with targeted nucleases (like the CRISPR system). Turns out it works efficiently. Verified with Southern blots.

Used the agrobacterium approach on ADH1, TT4, etc. They work.

Germline transmission in their hands is 15-20%.

Off-target effects: To deal with this, they converted CRISPR/Cas into a nickase. Did that by knocking out one of the nuclease motifs, so it only cuts on one side. Make two nicks and you get the DSB you needed, but with twice the specificity.
(I am surprised that this solution also works in plants, with with multiple genome duplications, etc.)

Mentioned at the end - we now have the tools to make a synthetic plant genome.



W600 - Feng Zhang - Highly Efficient Genome Editing in Crops

From Celectis. Idea is to create better food products through genome editing.

Efficient delivery methods are key to success. There are a couple way to do it: agrobacterium, particle bombardment, PEG or electroporation, and RNA or DNA viruses.

Protoplast transformation: what people did before agrobacterium.

Targeted gene editing in N. benthamiana. They do TALEN transformation, regeneration, and mutant screening. No need to select the plants on selective media because the transformation efficiency is very high.

Multiplex gene knockout in one generation: Trying to knock out four genes at once; 17% of screened plants did.
Results in a significant reduction of N-glycosilation in mutant plants.

Potato: trying protoplast transformation in commercial varieties. 
Cold-induced sweetening reduces potato storage quality, so they're trying to tackle that.
TALENs to knock out the gene responsible for the sweetening. 
Transgene-free mutant plants were identified, and did have reduced reducing sugar (not a typo) and acrylamide levels.
This substantially reduced the darkening of the color after CIS.



W601 - Sandeep Kumar - A Modular Gene Targeting System for Sequential Transgene Stacking in Plants

SmartStax has 8 transgenes stacked into one product. Crops with multiple traits. Typically, the transgenes reside on different chromosomes, so it's tricky to put them together and KEEP them together. 

The solution is the molecular stack since transgenes can be stacked into a single locus.

They're heavily invested in ZFN, so that's what they're using.

They use a modular and sequential gene targeting and stacking strategy. Involves switching back and forth between selectable markers every other generation. Can keep using the same ZFN to do this.

Particle bombardment used to insert donor and ZFN. Done on a T1 homo target (T0 is selfed to get homo). Looks like about 30% mutational efficiency overall.

Targeting confirmed via Southern.



###################################



C01 - Gary Saunders - The European Variation Archive at EMBL-EBI

Many ways to GET data (generate it, get from a paper, the web, colleagues), BUT - it’s not accessioned, no metadata, and no links to core data.

That’s where Eva comes in. Centra repository for VCFs. Any variation is represented as a change against a reference sequence.

www.ebi.ac.uk/eva

Study browser - can filter by free text search, checkboxes for organism, or experiment type. 
There’s only one curation study - maize - and that’s where they picked what parts of the genome to analyze.

Clicking the left box on an entry expands the metadata.

Going to the study view has an FTP link at the bottom.

EVA variant browser - select organism/assembly, and can put the region in which you’re interested in looking.
SNP IDs are from dbSNP. The “view” column links out to various external ways to look at it, like ensembl and dbSPN.

There's a way that parses the VCF genotype info to make it a little more human-readable.

Can submit in any file format, though VCF is preferred. It's all described in their online documentation. 
You can contact them via email to get their various submission templates, etc.
Data submitted to EVA is also brokered to NCBI, so no need to submit again. Once NCBI accessions it, it's sent to Ensembl.



C02 - Nathan A. Dunn (LBNL) - WebApollo 2.0: A Scalable, Distributed Sequence Annotation Editor

Designed for collaborations across large groups

His example: log in, search "Group1.3", which takes you into the annotation editor. Can add annotations locally or via URL.

Can use BLAT to search the whole genome OR just the scaffold you're currently on.

Check boxes on the right can be used to add various other pre-loaded tracks.

Drag any feature up to the yellow area to see the annotation that you can start modifying. When you modify something, you're not REALLY changing the reference sequence, you're adding metadata to show that something has been added or removed.

If you make a change, the ORF is recalculated.

Option-click opens the information editor.

Can add in RNAseq data, and I believe dragging exon boundaries will snap to those supported by RNA-seq.

New UI adds sidebar to help with sharing to specific individuals, among other things.

He'll be at P1154.f



C03 - Jodi L. Humann - GenSAS: A Web-Based Platform for Automated and Manual Curation of Genomic Sequence

Wrapped a bunch of command-line tools into a GUI. It's for non-bioinformaticians. Integrates with JBrowse and WebApollo for visualization and editing.

genesas.bioinfo.wsu.edu

Log in, click "use GenSAS" tab. All steps therein have the directions/instructions in a tab. This is in lieu of external documentation.

All projects have a 120 expiration date. You can always reset the expiration, but if you don't, it'll disappear to free up space.

Uploads must all be in FASTA format. If you put all the sequences in a multi fasta file, then ALL those sequences will have to get the same analysis.

Can also upload GFF3 files, apparently.

After upload, you filter repeats. If you go back and give it a different name, you can run it with different settings and pick the one you want.

After that is masking, then gene prediction (Transcript alignment is via BLAST or BLAT, I believe).

Don't need to stay logged in after your jobs are submitted.

Consensus: Pick the gene finding jobs you want to use, and assign them a weight.

"Refine" and "Functional" aren't available yet, but will be in the future.

Annotate

Publish - GeneSAS exports in GFF3 and FASTA formats (sequence of the features, not the genomic sequence).



C04 - Kim D. Pruitt - Sequence Viewer and Genome Workbench - Sequence Display and Analysis Tools

Sequence Viewer is web-based, Genome Workbench is what you download.
sviewer designed to look at one accession number at a time. Also exports high-quality PDFs.

sviewer - basically a genome browser. For example, when you go to NCBI Gene, sviewer is what appears on that page.

Can display custom data, including BLAST RIDs.

If you select two annotation objects, the system with automatically highlight regions of difference. Good way to compare, say, ensembl and ncbi.

gbench - learning curve, but does have documentation and videos. It's used by RefSeq curators and the GRC.

GCF - refseq; GCA - genbank



C05 - Emily Perry - Analysing SNP Consequences on Genes and Proteins with Ensembl's Variant Effect Predictor

www.ebi.ac.uk/~emily/Workshiops/2015/PAG - everything from this talk, including the demo files.

SIFT scores - based on protein conservation and the chemical change.

Two ways to use the VEP: web interface and standalone perl script.

If they DON'T have a species in Ensembl, you can make a cache.

Her example vcf doesn't have a header, but should still work.

Web-based: click on the VEP link on the main Ensembl page, then Launch Vep. If you have a bunch of variants, probably better to upload file or provide URL, rather than paste.

Output: Summarizes output.
Can then filter by, for example "consequence" and "missense".
Can download the data, or send them to BioMart and get them that way.

Perl script version: Just follow the instructions. Pretty much just a matter of installing and then specifying your vcf on the command line. You can also download a cache of your genome of interest.

You get a local vcf output, and an html file that should give you the summary images.



C06 - Hugues Roest Crollius - Genomicus: Fast and Intuitive Comparative Genomics in Plant and Animal Genome

239 species in their database.

Search for a gene, and the default view pops up, called PhyloView.
Click arrow in upper left to select which genomes it's compared to.

Can change color scheme to view protein similarity by clicking near the top of the screen. The similarity scores come from ensembl.

View -> AlignView lets you view alignments of the other genomes, such that if there isn't a homologous gene in the same locus, a gap appears.

There's also a view (karyotype?) to see the orthologous chromosome segments between two genomes - just mouse over.



C07 - Justin T. Page - BamBam: Genome Sequence Analysis Tools for Biologists

sourceforge.net/projects/bambam. Recently published in BMC Research Notes.

For manipulating and summarizing BAM files. Only going over some of the tools today.

Bam2Consensus: returns a consensus sequence for every contig in the reference. If you give an annotation via a BED file, you can pull just those regions.

Counter - Breaks it down by BED region, counts up how many things were there.

InterSNP - straightforward SNP caller. Just looks at a single basepair and counts up how many A, C, G, and T in one BAM vs. another BAM. Default output is a genotype table.

SubBam - Takes a subset of a sorted BAM file. SAMtools does something similar. Lets you make a smaller BAM if you're not interested in most of what's covered.

HapHunt - uses k-means clustering. Needs continuous coverage for it to work. 

MetHead - summarizes bisulfite sequencing data



###################################


	
Next Generation Genome Annotation and Analysis

W522 - Shin-Han Shiu - Nuances in Plant Genome Annotation: Those Duplicated and Dead Genes

Recent whole genome triplication of Brassica rapa hasn't resulted as three times as many genes, since the rate of pseudogenes goes way, way up.

But are pesudogenes really "dead"? They're expressed in certain conditions, and when they ARE, they're expressed around the level of canonical genes. About 20% are transcribed.

Some likely are translated - about 20%, depending on the dataset. They checked via ribosome pull-down.

Some of the pseudogenes are under surprisingly high evolutionary constraint, and evovle as slow as housekeeping genes.

Comparing the 5' and 3' parts of pseudogenes, before and after the first stop/frameshift. 5' is under slightly stronger constraint than 3' meaning it's maintaining a bit of function.

Though duplicates tend to die, quite a few duplicates remain. That's after 25 million years, which is quite a long time, even evolutionarily.

How did they stick around? Might be sub-functionalization.

What they see if asymmetric partition of function. It's not half and half, but one-or-nothing. 

At the expression level, they found that one of the duplicated genes would be the same as the ancestor, and the other is on but lower. One seems to be decaying.

For example, DDF2 has lost function over time. Doesn't bind as well anymore, either.

Of duplicates that are still around, one copy seems much like the ancestor, and the other tends to decay (rather than becomes neo-functionalized).



W523 - Joshua Puzey - Transposon Density Affects Homeolog Specific Gene Expression in Allopolyploid Monkeyflower (Mimulus)

Adaptation to polyploidy. It CAN have a lot of benefits, but polyploidy doesn't happen that often - many rapidly go extinct.

Major problems:
* Chromosome pairing at meiosis
* Control of gene expression - especially during hybridization, where things from genome A may try to regulate things from genome B.
* Competition with diploid progenitors

Autopolyploidization - within a given species
Allopolyploidization - two species make a hybrid

He's focusing on gene expression.

Number of proximal TEs inversely correlated with gene expression in arabadopsis. Another paper found that when genes had methylated TEs upstream, it's not expressed, but if they're not methylated you're fine.

Homeologs - sister genes from different parental genomes.

He predicted that expression differences in homeologs may be explained by local TE density.
Also, this expression bias should be less in early generations than later.

M. guttatus has undergone multiple polyploidizations, some of which have happened multiple times independently in the field.

Mimulus peregrinus - a hybrid that formed in the UK, as botanists brought it two other plants back from California and Chile.

Gene expression in M. peregrinus - called SNPs to give confident mappings, since they needed to know which parental genome a given transcript came from. Demanded 100% identity.

But first - looking at how TEs affect expression in the parental species. Again, local transposon load is negatively correlated with expression, and that hold for both parents.

Homeologs with more TE bases UPSTREAM are less expressed than the other homeolog.

The skew of the Homeolog Expression Bias (HEB) toward the luteus homeolog increases with more generations. The bias comes from hybridization, not the initial duplication.



W524 - Daniel D. Ence - Transposable Element Islands Facilitate Adaptation to Novel Environments in an Invasive Species

He's from Yandell's lab.

TEs can be effected by effective population size.
Looking at Cardiocondyla obscurior, and invasive ant species. 

The smaller the effective population size, the more impact of genetic drift relative to natural selection. Any new TEs that arise in the "new" population can go to fixation much faster than they otherwise would in the previous, larger population.

C. obscurior - originated in SE asia, and has spread throughout the globe (though looks to be mostly warm climes). 

Made a reference genome with 454 and Illumina, annotated with RNAseq from all castes and developmental stages, and filtered for prokaryotic scaffolds (important, since it has symbiotic bacteria).
It's the smallest genome assembly of any ant.

Repeat annotation - C. obscurior has about the same proportion of TEs as other ants.

TEs are organized into TE islands. 

TEs in this species are organized differently than in other ants. It's more similar to Nasonia (a wasp) than other ants (that's another inbreeding species). 

TE island genes are overexpressed in adult queens compared to queen larvae.

Brazilian vs. Japanese colonies - Japanese larger, same number of workers, more queens. 
B aggressive to B, J, and external ant; J is only aggressive to external ant. Due to different cuticular compounds.

How do they differ genomically? 

TE island regions are enriched for SNVs. Since the individuals were pooled HAPLOID genomes, that means they're polymorphic at those loci. Since odor receptor genes are there, that affects their behavior.

Other points:
Genes important for development are excluded from TE islands.
TE islands preferentially expressed in adult queens



W525 - Ingo Braasch - The Spotted Gar Provides Connectivity Among Vertebrate Genomes and Insights into Evolution By Genome Duplication in Fish

Begins by reminding us of teleost genome duplication (after the split from the branch leading to tetrapods, so we don't have this). This complicates assignments of orthology between, say, zebrafish and humans.
Coelacanth is a lobefin fish, and doesn't have that teleost gene duplication (since it's from after the split, on the same branch as tetrapods).
Tetrapods have lost over 50 genes with known function in zebrafish.
Coelacanth is highly endangered, though, so can't do much work on them.

Duplicate retention and loss after the TGD (teleost genome duplication event): Majority of orthologs lost right after the TGD.

So, we need something on the rayfin branch from BEFORE the TGD. Hence, spotted gar. There are other species that could have been used, but spotted gar has the fewest problems.

GARNOMICS: genomic tools for the spotted gar:
* Fosmid/BAC libraries
* RNA-seq
* miRNA-seq
* High density genetic map (RAD-tags) - shows that it has an unduplicated genome relative to the human genome.

Broad Institute made an ALLPATHS-LG assembly.

Genome is available on ensembl. Repetitive elements only 20% (Zebrafish is 51%, medaka is 16%)
Probably around 21,000 genes.

Reconstructing ancestral vertebrate genome: use synteny database to compare species. You can see rearrangements between fish and humans (though not so much between the gar and chickens - so humans are the weird ones, here).
Further analysis shoes there is a history of microchromosomes in the vertebrate genome.

Resolving unclear gene family relationships: Like Prrx genes. Tetrapods have Prrx1 and 2, zebrafish have prrx1a and 1b. Prxx2 is in coelocanth - AND in the gar. Predates the divergence and was lost by teleosts, probably before the TGD.
Well, not QUITE - it's present in the basal teleosts. One copy lost right away, and then the other copy lost at a later split.

Connecting vertebrate cis-regulatory elements: LastZ -> pairwise -> phastCons to identify conserved noncoding elements. Can find CNEs specific to teleosts this way, for example.
Many CNEs appear to be lineage-specific - can't necessarily align CNEs between fish and humans, for example.
You can't align the things from human and pufferfish, but if you align both to the gar, THEN you can see that things line up.
The evolutionary rate in teleosts is very high, but gar has a much slower rate of evolution - so it's closer to humans than zebrafish.
Because of that, you can align zebrafish to gar and human to gar, and THEN you can see 24,926 CNEs that you wouldn't see otherwise - about 40% of which are orthologs.

How did gene expression evolve after the TGD? 
gpr22 is in brain and heart in gar. In teleosts, one ortholog is in brain, and the other ortholog is in heart. Sub-functionalization.
Neo-functionalization: slc1a3 is in brain in gar. Teleosts have it in brain AND liver.



###################################



1/11/15

W243 - Beat Keller - Lessons Learnt from Transgenic Gene Pyramiding for Classical Resistance Breeding in Wheat

Standing-room only, so I wasn't able to type up notes.



W245 - Jay Shockey - Sanding the Rough Edges: In Search of Enhanced Performance of Engineered Enzymes in Transgenic Systems

Novel oil plants - the oil-producing plants we’ve bred are for eating, not for industrial use. For that, we turn to novel oil-producing plants.

The downside is that those plants have poor agronomic traits, so that’s where they come in.

Putting transgenes from the exotic plant into target plants like arabadopsis doesn’t work too well. Fatty acid production goes from about 80% in exotic plant to around 20% in the target, which is unacceptably low.

Their goal is to find ALL the genes required for the biosynthesis of an oil with high amounts of an exotic fatty acid. This also involves promoter optimization, etc.

Castor oil - makes a very good lubricant that stands up to high temperatures, but you can’t cultivate the host plant at industrial scale because those plants produce ricin.

Diacylglycerol acyltransferase (DGAT) - catalyzes the committed step in TAG formation.

Castor bean DGAT2 strongly prefers dircinolein in vitro.

RcDGAT2 coexpression drives dramatic increases in seed ricinoleate levels.

Tung tree - Oil has conjugated double bonds, and contains trans double bonds. Unusual - and makes it easily oxidized.

FADX involved in biosynthesis of tung oil.

Design of a flexible cloning and expression system - what he's been up to. Hopes to share with the community soon.
Also adapted for CRISPR/Cas9.

Testing it - promoter optimization of tung FADX expression.

DGAT2 prefers eleostearate-containing substrates.

As in yeast, VfDGAT2 outperforms VfDGAT1 in planta.

Summary:
* DGAT, especially DGAT2, improves HFA and ESA production.
* Promoter choice suppression of competing processes, and the use of engineered enzymes all help.



W242 - Jian-Kang Zhu (not really, but someone presenting on his behalf) - Gene Editing in Plants using CRISPR/Cas and TALENs

Begins with explaining CRISPR, etc.

They use two sets of CRISPR vectors. One for arabadopsis, and one with a different promoter for rice.

Testing if CRISPR/Cas9 in Arabadopsis protplasts

Endogenous targets of CRISPR/Cas9 in Arabadopsis - testing three visible phenotypes. RFLP assay.

Successfully targeted two sites in the same gene simultaneously

Mutation is usally a single bp change - and drops off quickly in either direction.

Transmission:
T1: chimeric
T2: homozygous, bi-allelic or hetoerzygous
T3: homozygous or bi-allelic
They find that no new mutations or modifications arise as generations progress.

Did WGS to look for off-targets. No off-targets detected in T1 or T2. I believe he said there's a paper that contains this information (PNAS 2014? Might be http://www.ncbi.nlm.nih.gov/pubmed/24550464)
Ahhh, it's not quite what he said. They did WGS, yes, but they only looked for mutations in PREDICTED off-target sites. There's been other work published that has shown you can get off-target effects in sites you wouldn't predict based on sequence similarity.

CRISPR in rice: Similar RFLP results

Trying to knock out all PYLS in rice (and there are 13 of them).
Multiplex targeting or rice PYLS - one vector contains multiple sgRNAs.



W244 - Yinong Yang - Plant Genome Editing and Precision Breeding with CRISPR-Cas9 System

Nice figure indicating the 20mer is called the protospacer (which is why the PAM is called the protospacer-adjacent motif)

Made vectors for plant genome editing, some of which are already available through Addgene.

For off-target concern, made genome-wide prediction of specific gRNA spacers. Mol Plant 2014, http://www.ncbi.nlm.nih.gov/pubmed/24482433
That's the CRISPR-PLANT website.

Transgeneic rice lines with targeted mutation of OsMPK5. Got single base indels and frame-shifts, and you can cross out the transgene.

Developing advanced mulitples editin tools for functional geneomics and precision breeding. For example, expressing for more than one target for offset nicks, or just trying to hit multiple places with DSBs.

PTG technology - array gRNAs tandemly in a singe polycistronic gene and utilize endogenous tRNA processing system for precise cleaving and production of numerous gRNAs in vivo. Submitted to PNAS.
Didn't quite catch this, but sounds like connecting the guide RNAs to tRNAs.
Because it's a single transcript, you get identical expression of all the guides.

DNA fragment deletion at a frequency of 4-45%. Fragment deletion efficiency is negatively correlated to its size. (This is with multiplex targeting)

PTG:Cas9 has a higher mutational rate than sgRNA:Cas9.

For working in crops, you can do it by providing transient information, or using stable information and then crossing it out.

For example: regeneration of transgene-free potato plants from protoplasts after targeted mutagenesis of AS1.
Hopefully, he says, this sort of thing will alleviate concerns about genetically modified crops.



W246 - Vai Lor - Targeted Mutagenesis of the Tomato PROCERA Gene Using TALENs

Gibberellins (GAs) are phytohormones. Promote pant height, flowering, leaf size

You don't necessarily want TALLER plants - can lead to net yeild loss. Semi-dwarf GA mutants were therefore important during the "green revolution", increasing yeild.

GA relieves DELLA repression of the GA response. Combines with other proteins to ubiquitinate (and therefore degrade) DELLA.

The tomato DELLA gene is PROCERA (PRO), and the mutant is tall.
Two possibilities: Either the mutant isn't really a null, or there's a DELLA-independent function.

Designed TALENS to destroy a diagnostic restriction site to make mutant identification easier.

Seven pro alleles were recovered. proTALEN mutants are parthenocarpic (no seeds). They're male sterile.
So, if you cross WT male to proTALEN female, you do get fruit with seeds.

proTALEN mutants are not responsive to GA and Paclobutrazol (PAC). Suggests that proTALEN mutant is likely null.

GA signaling is fully activated in proTALEN mutants.



###################################



Statistical Genomics



W799 - Qishan Wang - An Empirical Bayes Method for Genome-Wide Association Studies

Most popular model is a linear mixed model. Combo fo fixed and random (random controls for the polygenic background). There are both exact methods (GEMMA, FaST) and approximate (EMMAX and P3D).

Two major issues:
1. Genomic background noise (marker density too high). Increase false positive.
2. Bonferroni correction is too conservative, leading to low power.

For example, if you had 2287 subjects with ~509,000 SNPs, you wouldn't be able to detect FGFR2 in breast cancer, despite actually being associated.

So they came up with Empirical Bayes (EB) method. Reduces noise and allows FGFR2 to show up as significant.

Their model has a different covariance matrix. Random marker effect approach.

Gamma k is the parameter of interest, and phi k squared is just prior variance.

Advantage 1: shrinkage nature.

Advantage 2: can calculate an "effective number of tests", and use THAT to perform Bonferroni correction to increase statistical power.
Note the effective number of tests only applies to the random model approach, not the fixed model approach. Testing in this way has a higher type-1 error (false positives) than using the usual Bonferroni correction.

Advantage 3: improving the computational speed



W800 - Lauren M. McIntyre - Using Path Analysis to Reveal Novel Components of the Sex Determination Hierarchy in Drosophila melanogaster

Example of retinitis pigmentosa - there are a LOT of SNPs in there that can cause it. But since they're all in the same gene with low frequency, they'd be really hard to find with GWAS.

Working on a "known" fly pathway - sex determination.

Building off the work of Sewall Wright. Supervised GRN Reconstruction.

Begin with baseline models, using two datasets (DSPR from King, and F1-Hybrid from Mackay).
DSPR shows no covarnaince, but F1-hybrids have some evidence of full or partial covariance.

Go through and add in all possible paths, asking if they improve model fit.
Found 12 putative new relationships.

Similarly, you can add new genes and see what happens.
Here, the datasets are VERY difference. 0 genes added to the DSPR model, but F1-Hybrids had 1,390 genes were added to the model.

Why? The number of alleles matters (allelic combinations). DSRP has fewer alleles, so the model becomes saturated sooner.

Of those 1,390 genes, they're enriched for sex-based splicing effects.

Now, corn.

Idea is to zoom to the relevant part of KEGG pathway, see what fits, and then see what else there is to improve the model.

Metabolomics - challenging and new. They have money for it that must be given to people outside of UFL.



W801 - Zhiwu Zhang - Getting Power Back from Population Structure and Kinship in Genome-Wide Association Studies

If you have linkage equilibrium, random mating will let you distinguish between the causative mutation and others that are just there not doing anything. 
If it's linkage disequalibrium - like if they're geographically isolated - then you can't necessarily do that.

Y = SNP + Q (or PCs) + e. Q or PCs captures population structure, and the other two terms is phenotype of individuals.

MLM for GWAS adds in kinship, to capture unequal relatedness. (Yu et al 2005, nature genetics)

GWAS does not have power for traits associated with structure.

PLINK is a GLM, but has 75% of the publications. Why don't human geneticists use MLM instead of PLINK? Well, in humans, you don't gain much power. You do in arabidopsis, though.

FarmCPU outperforms both GLM and MLM in terms of power, and that's for Arabidopsis AND human.

FarmCPU: P1158
BioGPU: P1163 & C28.



W802 - Deniz Akdemir - Locally Epistatic Genomic Relationship Matrices for Genomic Association and Prediction

Semi-parametric mixed model - SPMM

Multiple Kernel Models - use multiple kernels by combining them int a single one via a combination function.

Genetic data comes in natural hierarchies. At the bottom level, you have the SNPs, which you can block together. 

Breeding value is distinct from commercial value - commercial is what THAT generation does, and breeding is how much you can pass on. So if you have a genome-wide effect that can be lost due to recombination, that's not of much interest to breeders.

So, working more fine-grained, here. Locally epistatic values.

Only a small fraction of genomic regions are used in the final model and the importance scores for these regions are readily available as a model output.
Dimension reduction via feature extraction, block interactions.
Tests for regression coefficients, or hierarchical test variable
Robust for a few missing markers in a region.



W803 - Shizhong Xu - A Tutorial of Partial Least Squares Analysis

There are many methods for prediction, and some may work better in some populations than others. This is one such method.

PLS is a prediction of multiple response variable form multiple predictor (independent) variables.
It's a predictive model, not for variable selection. Don't use it for QT mapping or GWAS.

Related to:
1. Multiple linear regression
2. Principal component regression

You're trying to maximize the covariation of the scores of the Xs and the Ys

(Most of the rest of the tutorial is outside of my depth)

Determining the number of factors: k-fold cross-validation. That gives you a vector of observed and a vector of prediction, and then you calculate r squared between them.

He's used this method to predict hybrid yield and grain weight in rice.



###################################



Degraded DNA and Paleogenomics



W234 - Ian Barnes - How Do You Know When You Don't Have Any Ancient DNA?

Meridiungulates - extinct super-order. Variously identified as mono or polyphyletic.

Two end Pleistocene survivors. Toxodon plantensis - like a hippo-rhino.

Macrauchenia patachonica. Like a…camel-tapir.

Material tested for collagen preservation (since that sticks around more stably than DNA, I think he said)

NGS is nice for ancient DNA - no primer sequences, can work with small fragments, should be able to detect contamination and measure damage.
But it also has problems - not much good unless there’s a fairly close genome available. Also you tend to pick up a lot of non-endogenous soil bacteria. Also, it costs a lot.

Their BEST sample was still only soil biota. How do they know there’s no ancient DNA?
1) de-novo assembled into contigs, threw out the short ones, BLASTed to a local nucleotide database.
b) (didn’t catch this part)

But the samples they got were actually pretty far north, and in the southern hemisphere, (South America), that means you wouldn’t expect the DNA to last too long. Anything below 40-45N latitude is likely to have poor DNA recovery for the pleistocene (unless high altitude).

When is there not enough endogenous DNA to proceed? Depends on read length and proportion of endogenous DNA. 
Read length depends on burial environment, time, extraction method, library build
Endogenous DNA content is dependent on skeletal element, burial conditions, other things we don’t yet know.

Anyway, what they decided to do what look at the amino-acid sequence of type I collagen, and put that into a phylogenetic tree. Collagen is hard to sequence (repetitive), BUT it lasts about 10x as long as DNA.

The MS-MS data give a collagen phylogeny with high support for placement with Perissodactyla. Also, they're monophyletic. Share common ancestor with tapirs, horses.



W235 - Daniel G. Bradley - The Best Bones, Imputation and Ancient Genomics

Inspired by the Denisovian sequence - which was complete & came from a small finger bone - they ask what are the best bones for these studies?

They say is't the petrous bone. It's the bone that houses the inner ear, and is the hardest bone in the human body. Survives cremation. Human petrous is attached, sheep and goat are much easier to snap off.

Other bone elements get 10% endogenous reads if you're lucky. Petrous can be over 70% (though those data points are from more recent bones, but it still tends to out-perform other bones when older, too)

Imputation: Take phased reference haplotypes & your experimental partial haplotypes, and use that to call imputed haplotypes.
Might not work in aDNA, though, since ancient DNA can have different haplotypes.
Turns out they were able to bootstrap their data for imputation to get haplotypes out.



W236 - Jake Enk - Time, Temperature, and Tiling Density When Capturing Ancient DNA

R&D scientist at MYcroarray.

Enrichment through hybridization.
65 degrees C is sort of industry standard for modern DNAs, but there's also lower temperatures, touchdown, etc.
Lots of different times & tiling densities used, as well.

They simulated ancient DNAs (short fragment length - mode of 50 bp - which are shorter than the bait length of 80 bp). Embedded those in a soil bacteria background, such that the mix was about 6% endogenous.

Temperature: Bait length usually determines optimal temp, but since target is shorter here, it's the TARGET that dictates that. 
Hypothesis is that an increase in time will raise specificity and lower sensitivity.

For mito:
Specificity DOES go up - to a point. Looks like above 65 degrees it drops off (at 70 C)
Sensitivity looks to slightly peak around 50-55 degrees, then goes down. That's as measured by % of X raw reads that are unique sequences.
Optimal sensitivity looks to be at 55 degrees.

For nuclear:
Similar.

Time: Target aDNAs tend to be rare in the library. Hypothesis is that more time means more sensitivity (with caveats), and specificity MIGHT go down.

For mito:
Specificity not significantly different.
Sensitivity does go up with time, but not by much.
Probably want to avoid really short times, but anything beyond 16 hours is probably unnecessary.

Tiling density:
Hypothesis is that more tiling density means more sensitivity, and lower specificity (more likely to capture background).
Higher density doesn't change specificity in mito, and doesn't affect sensitivity much, either (except for a outlier at the low end).
So a little redundancy (2x) is good.

Conclusions:
Temp - if want highest % on-target, do 55 C. If want to capture as much as possible, go lower.
Time - avoid 4-6 hr, but no need to go longer than overnight.
Density - 2x is good, but no need to go beyond 4x.



W237 - Ludovic Orlando - Ancient DNA: From Genomes to Epigenomes

(There's now a 781 kya horse genome they published last year)

How they look at ancient epigenomes - nucleosome protection after death. By looking at depth of coverage, that should tell you where the nucleosomes were.
As for methylation, CpG should degrade to UpG, and mCpG will turn into TpGs. Old Illumina library building process couldn't extend through UpG, and CpG to TpG conversion should indicate there had been methylated.

That was the hypothesis, anyway. They looked at nucleosome arrays to see how well they matched up.

Nucleosomes were spaced about 193 bp, which is what was expected. They're also where you would expect based on exon boundaries.

ancient nucleosomal phasing and methylation, as well as ratio of methylation between gene body and promoter, can be used to predict ancient gene expression levels.

Methylation disappears pretty fast from ancient genomes.

Pulling down methylated DNA from ancient genomes may help you call SNPs in those regions.



W238 - Eleftheria Palkopoulou - Genomic Diversity and Population Dynamics of the Woolly Mammoth Prior to Its Extinction

There's an island north of Siberia (Wrangel island) that wasn't an island back in the last ice age. When seas rose, it became an island, the population on it was isolated.

Compared to Oimyakon sample - mainland siberian wooly mammoth.

Population history inferred by PSMC. Siberian and Wranger are a bit offset. Siberian is ~ 45,000 yBP, Wrangel is ~4,300 yBP (so substantially younger).
If you shift the curves so they line up, you can see how many mutations occurred in the Wrangel genome since the death of the mainland sample. Lets you calculate mutation rate. 

Both samples were males, meaning all their X sequence is phased. Used that to see that the population split was about 48,000 years ago, or about the time of the death of the mainland siberian individual.

Genome-wide diversity: Wrangel had lower het. per 1000 bp (1.00 vs. 1.25). It's not SUPER low (like snow leopards), so it's not THAT big a bottleneck.

So was there inbreeding?  Look at runs of homozygosity to find out. Higher number and longer ROHs for the Wrangel individual indicative breeding between distant relatives, rather than recent inbreeding (like between siblings, etc.)

Summary:
* Two severe population declines in the history of wooly mammoth
* Recent pop split between the ancestor of of the two individuals
* Decline in the genome-wide diversity of the Wrangel genome
* Increase in runs of homozygosity in the Wrangel genome



W239 - Peter D. Heintzman - Resolving the Evolutionary Relationships of Two Enigmatic Ungulates from Pleistocene North America

Camelops - extinct North American camel

Camelids actually originated in north america. Those that went to the old world became dromedary and bactrian camels, and those in south america became llamas and alpacas.

Camelids stuck in what is now Western US for most of pleistocene - but when the ice sheet wasn't there, they're found as far north as the Yukon. Better fossil samples up there, of course. Suspected to be ~125 kya.

Why care? Want to resolve phylogenetic tree, for one thing. Camelops currently thought to split from SA camels after they had split from old world camels - but not everyone trusts that model.

Extracted DNA in a way specifically designed to enrich for short fragments & did illumina shotgun sequencing. That worked.

Mitogenomic reconstruction - had to use iterative assembly methods, aligning to FOUR different extant camelid genomes. Required multiple rounds of assembly. Reads from each sample were aligned to the final draft with BWA and visually inspected for accuracy.

Made phylo trees in various ways, but the mito data always puts the camelops next to the OLD world camels, not the new world camels.

Phylo reconstruction inferred from low-coverage genomes: Mapped to Alpaca and wild Bactiran camel genomes, then Alpaca mapped to wold Bactrian camel and vice versa.
Again, camelops is on the old world branch. Evidence of a DNA damage artifact, too - gives you a bias toward mapping the more conserved reads.

Had they just relied on PCR, they would have found nothing, and excluded interesting specimens.



W240 - Christina Warinner - Direct Evidence of Milk Consumption from Ancient Human Dental Calculus

Milk is very nutritious, but it's hard for adults to consume because of lactose. Lactose is hard to break down, and requires special enzymes (lactase) to break down. If you CAN'T break it down, it gets all the way to your colon - where your bacteria then have a field day and use it to produce 8 liters of hydrogen gas.

Europeans have a super-high rate of lactase prevalence. That variant is the one that's been studied the best.

Buuuuut, it's not entirely clear how that variant got to northern Europe (and Spain) in the first place, since that allele doesn't show up when you'd expect it would in neolithic populations.
She points out that the way the milk is processed (think cheese) might make a difference, here.

So - they started looking at dental calculus. Shotgun metaproteomics.

Found BLG, a milk protein.
1. Humans don't make it, so it must be from some other animal.
2. It's ONLY in milk, not blood, etc.
3. It's hard for bacteria to break down
4. BLG lacks close orthologs in bacteria, so easy to identify
5. Many polymorphisms among species, so you can tell what animal it came from (except camels)
6. It's very abundant
7. Shows up a lot in products with high lactose content - so not so much in butters and aged cheeses, but really high in milk and whey.
8. Since it's sequenced based, it's a much more direct way from tying milk consumption to a specific individual.

On average, 20% of individuals tested positive for BLG from the skeletons from the milk-drinking regions.

Found goat milk in Italy, sheep and goat in Great Britain 



W241 - Beth Shapiro - The Rise and Fall of the Passenger Pigeon

There used to be a LOT of passenger pigeons - but they were tasty and cheap, too, and easy to trap.

So what happened? 

Stuff we (think) we know:

1. Pigeons are old
2. Ectopises is an old pigeon lineage.
3. Closest living relative is band-tailed pigeon.

When did they become so abundant in the first place? An open question. 
Hypotheses:
1. Post-glacial warming and forest expansion
2. Agriculture

So they tried out ten different models, and got 41 mitochondrial genomes.
Rapid, massive, star-like expansion. So none of their ten models fit.

They got scooped - another group showed that the carrier pigeon population already fluctuated a lot to begin with.
But they couldn't recapitulate it. Why not?

Well, the other group used the default settings on the analysis, which wasn't a good idea. The most important one is the C50 setting, since you don't want to use too strict a mappability criteria, since you're not able to use the ACTUAL genome you want to map to.

But, the big fluctuations don't tell you what happened in the last 20,000 years anyway, since you have no power there.

So, what do we know?
There's little diversity in the mitochondrial genome.
The other group made another mistake, which was making consensus sequences, which squashes heterozygosity.
So what her group did was create artificial haplotype sequence by picking a random base (from good reads) at each site, and then integrate.

Rather than nuclear heterozygosity of 0.1%, it's 1% - that's more diverse than chickens, which are incredibly diverse.

Don't know what's going on, but a few guesses:
Demographic history - could be (relatively recent) hybridization from something distantly related - but the bantail doesn't live anywhere near where the passenger pigeon.



###################################



Sequencing Complex Genomes



W712 - Thiru Ramaraj - De Novo Assembly of Plant Genomes: Hybrid Approaches for Heterogeneous NGS Data

National Center for Genome Resources (Santa Fe, New Mexico)

They use Illumina, PacBio, Oxford Nano, and (for mapping) BioNano Genomics.

Medicago truncatula - Want 25 genomes.
22 get just Illumina and ALLPATHS-LG.
For 3 of them, they want higher quality, so all of the above technologies are thrown at it.

Just with Illumina, it's decent - 80% proper pairs, 84% of reads map to assembly. Brought in PacBio to improve it.

P0756 - their Illumina/PacBio Hybrid workflow

Bottom line is that the hybrid approach is better in just about every measure. Helps prevents gene fragmentation in the annotation, too.

BioNano whole genome map - convert the images to molecules, and create consensus genome map. Can align against reference to look for structural variations, can use it for scaffolding, etc.

Cotton - a work in progress. They'd like to add in PacBio and BioNano stuff.



###################################



G001 - Philip Bourne - NIH as a Digital Enterprise - Implications for PAG

Relevant for many funders, not just NIH.

So what is a digital enterprise? Before that, a look at how far we've come in one researcher's career (his).
Back when he started working with the PDB, they used to ship tape to Australia via ship, and it took three months. We've clearly come quite a way since then.

Biomedical data is becoming more FAIR: Finding, Accessing, Integrating, and Reusing digital research objects.
But the idea here is to make things more efficient so we can do these things easier and better.

The move from an observational science to a more analytical science (though still observational) is being driven by ever-increasing amounts of digital data.

Cites "The Second Machine Age" - things like Google car, 3D printers, Waze, and Robotics as things that all showed up sooner than expected. All involve ingesting a very large amount of data.

Further perturbation: The Story of Meredith )http://fora.tv/2012/04/20/Congress_Unplugged_Phil_Bourne
As editor of PLOS comp bio, got a paper on pandemic modeling. Written by a single author. Sent the paper to Simon Levine, who also liked it. Meredith was 15 years old at the time - and was able to leverage a lot of public data, open literature, etc.

Four weeks ago, they had the gaming community workshop, to see what they might contribute to data analysis. It was great & productive - and they're going forward with a funding call - but then got stopped by a congressman writing an op-ed piece.

More perturbations - Begley and Ellis published in Nature that 47 of 53 "landmark" cancer publications could not be replicated.

ADDS mission statement - similar to NIH mission statement, but specifically about research conducted as a digital enterprise.

Some goals of the digital enterprise:
* Cost savings through sharing of best practices
* Sustainability
* Collaboration
* Reproducibility
* Integration of data types

Some of Today's Observations -
The good news:
* Genuine willingness to address the problem
* Global communities are emerging
* Efficiencies can be achieved
* BD2K is the beginnings of a plan
* We are beginning to quantify the issues

Bad news:
* Still no sustainability plan
* Global policies define the WHY but not the HOW (in terms of accessibility)
* We do not know how all the data we currently have are used - funding opportunities to look into this are upcoming. Gives the example of a netflix- or amazon-like predictive system that suggest data to look at based on the usage of others
* We can't estimate future supply and demand
* We need to ramp up training programs in data science

Sustainability 101 - NIH budget is effectively flat, but biological databases keep growing. This sort of thing does not scale.
Need to think about how much we spend on getting new data vs. keeping the data we already have.

What is the NIH doing to fulfill the promise of increasing the utility of biomedical research data?
He sees it as a three-legged stool: community, policy, and infrastructure.
Says this has to come from the community, but needs to meet in the middle with top-down policy.
Virtuous research cycle - what you do in your research is the motivating factor, not the infrastructure. But, the three legs of that stool can be of benefit to you.

Policies - now & forthcoming
Data sharing plans on all research awards, along with enforcement (machine readable plan, repository requirements to include grant numbers).

Data citation - legitimize data itself as a form of scholarship.

Infrastructure - BD2K has funded 12 centers of excellence, all of which are working with data in various forms.
Also funding DDICC - data discovery index consortium. That needs to exist so that data can be found (can't google data very well), so you need an index.
Software needs to be given credit the way data needs to be given credit - and also findable.
Standards are also being funded.

All of that - along with bringing in extramural labs - comprise The Commons.

The Commons - idea is that it'll be system agnostic, so it can be in the cloud, PHC platforms, or any other kind of platform (like in house compute solutions). Just need to have things be commons compliant.

The business model - switching things up. There's currently not a good sense of supply and demand, so they're trying something as a trial. Rather than awarding money, they give you credit to spend in commons-compliant resources. That should drive supply and demand.

How might PAG participate?
Consider contributing objects to the commons, regardless of who funded it (data, software, standard, narrative, course materials...). Working toward a more open framework.

Training goals - 
1. Build an open digital framework for data science training
2. Develop short-term training opportunities
3. Develop the disciple of biomedical data science and support cross-training - OPEN courseware


Questions from the audience:
Is there incentive to USE deposited data?
Answer: He's not a big fan of sticking data somewhere and just leaving it without tracking usage. Need at least some level of light curation.

What about SBIRs for "smart kids from California"?
Answer: Clearly useful, but nothing in BD2K. The institutes have various things that are applicable, but we need to do more.

Carl Schmidt - Research & scholarship is not business, and business models have been destructive to collaboration. Also: senior faculty need to be educated about how important data sharing, etc., are, so young faculty aren't hurt or denied tenure for doing the right thing.
Answer: There's a partnership that needs to take place here, as we're in a very dire situation. Some things are going to have to change to solve these problems. 

Data curation costs money, and some are forced to charge users. Is that sustainable?
Answer: Subscription is a possibility, certainly. Services that are on top of the data...well, the question is if there's a market for that, and that sort of thing should be explored. Maybe we can do better.


